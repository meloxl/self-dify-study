--

# 文件：README.md

# Daily Interview

## 背景

牛客网，知乎等众多网站上包含了数以百万计的面经，但往往大而散，面试者在准备面试时候去翻阅不但浪费时间，翻阅材料越多，越觉得自己很多知识点都没有掌握，造成心理上极大的压力，导致面试中不能发挥正常水平甚至面试失败。
其实，每一位求职者都应该有自己的一份面试笔记，记录笔试中常涉及到的知识点和项目中常被问到的问题。每次面试之前看一遍，做到举一反三，融会贯通，熟捻于心，方能在每次面试中汲取经验，最后从容应对。我个人就有自己的面试笔记，每次面试之前都会翻一遍，边看边想，但求好运。

## 宗旨

不需要大而全，涵盖所有内容，因为知识在不断更新迭代，我们也做不到涵盖所有。
不提供查漏补缺，因为每个人的短板不尽相同，需要面试者根据自己知识体系，多加思考，自己完善。
这是一份每一个面试者面试之前必看一遍的小面经。面试之前的半天时间，温故而知新。

## 内容

<div align=center>
<img src="https://github.com/datawhalechina/daily-interview/blob/master/content.png" width="400px">
</div>

## 使用指南

1. 目前大部分成员是做AI算法，所以主要精力在AI算法一块。若有对开发感兴趣的人员参与整理，十分欢迎。
2. 数据结构与算法本来属于计算机基础一部分，但是因为不管面试算法岗还是开发岗，都会问到，所以单独提出来。
3. 算法岗：重点是AI算法、数据结构与算法；了解数学、计算机基础。
   
   开发岗：重点是开发、数据结构与算法、计算机基础。

以面试岗位为梳理主线，整理面试之前必看的面试题目，给出高频的面试知识点和面试题。

## 招募

如果你也喜欢这个项目，想参与到面经项目中来，可以与我们联系 E-mail:xiongweinie@foxmail.com

## 关注我们

<div align=center>
<p>扫描下方二维码关注公众号：Datawhale</p>
<img src="https://raw.githubusercontent.com/datawhalechina/pumpkin-book/master/res/qrcode.jpeg" width = "180" height = "180">
</div>

## LICENSE

<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="知识共享许可协议" style="border-width:0" src="https://img.shields.io/badge/license-CC%20BY--NC--SA%204.0-lightgrey" /></a><br />本作品采用<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">知识共享署名-非商业性使用-相同方式共享 4.0 国际许可协议</a>进行许可。

---

# 文件：_coverpage.md

---

<!-- ![logo](https://docsify.js.org/_media/icon.svg) -->

# Datawhale面经

> Datawhale面经

[GitHub](https://github.com/datawhalechina/daily-interview.git)

<!-- [Get Started](#quick-start) -->

---

# 文件：_navbar.md

---

---

# 文件：_sidebar.md

---

<!-- docs/_sidebar.md -->

- 计算机基础
  
  - [操作系统](./计算机基础/操作系统.md)
  - [计算机网络](./计算机基础/计算机网络.md)
  - [数据库](./计算机基础/数据库.md)
- Big data
  
  - [MapReduce](./开发/大数据/mapreduce.md)
  - [Technology](./开发/大数据/Technology.md)
  - [Questions](./开发/大数据/questions.md)
- 前端开发
  
  - [基础知识](./开发/前端开发/README.md)
  - [JavaScript相关问题](./开发/前端开发/【1】javascript.md)
  - [HTML 相关问题](./开发/前端开发/【2】html.md)
  - [CSS 相关问题](./开发/前端开发/【3】css.md)
  - [网络及浏览器相关问题](./开发/前端开发/【4】网络及浏览器.md)
  - [前端框架及打包工具相关问题](./开发/前端开发/【5】前端框架及打包工具.md)
  - [NodeJS 相关问题](./开发/前端开发/【6】nodejs.md)
- Java 后端开发
  
  - [基础知识](./开发/Java后端开发.md)
- 数学
  
  - [logic题](./数学/统计学/logic.md)
  - [probability题](./数学/统计学/logic.md)
- 数据结构与算法
  
  - [数组](./数据结构与算法/Array.md)
  - [排序](./数据结构与算法/sort.md)
  - [贪心](./数据结构与算法/greedy.md)
  - [字符串](./数据结构与算法/string.md)
  - [链表](./数据结构与算法/linklist.md)
  - [二叉树](./数据结构与算法/binaryTree.md)
  - [图](./数据结构与算法/graph.md)
  - [搜索](./数据结构与算法/search.md)
  - [动态规划](./数据结构与算法/dp.md)
  - 其他
- 机器学习基础
  
  - [Metrics](./AI算法/machine-learning/metrics.md)
  - [过拟合与欠拟合](./AI算法/machine-learning/过拟合与欠拟合.md)
  - [梯度下降](./AI算法/machine-learning/梯度下降.md)
  - [ABTest](./AI算法/machine-learning/ABTest.md)
- 机器学习算法
  
  - [线性回归+逻辑回归](./AI算法/machine-learning/线性回归+逻辑回归.md)
  - [SVM](./AI算法/machine-learning/SVM.md)
  - [Decision Tree](./AI算法/machine-learning/DecisionTree.md)
  - [EnsembleLearning](./AI算法/machine-learning/EnsembleLearning.md)
  - [Adaboost](./AI算法/machine-learning/Adaboost.md)
  - [XGBoost](./AI算法/machine-learning/XGBoost.md)
  - [LightGBM](./AI算法/machine-learning/LightGBM.md)
  - [Catboost](./AI算法/machine-learning/Catboost.md)
  - [KMeans](./AI算法/machine-learning/KMeans.md)
  - [KNN](./AI算法/machine-learning/KNN.md)
  - [NaïveBayes](./AI算法/machine-learning/NaïveBayes.md)
  - [CRF](./AI算法/machine-learning/CRF.md)
  - [Apriori](./AI算法/machine-learning/Apriori.md)
  - [Prophet](./AI算法/machine-learning/Prophet.md)
- 图像处理算法
  
  - [CV基础](./AI算法/CV/CV基础.md)
- 自然语言处理算法
  
  - [文本结构理解](./AI算法/NLP/文本表示/文本结构理解.md)
  - [文本表征方式](./AI算法/NLP/文本表示/文本表征方式.md)
  - [特征挖掘-基于深度学习的模型](./AI算法/NLP/特征挖掘/基于深度学习的模型.md)
  - [特征挖掘-Bert](./AI算法/NLP/特征挖掘/Bert/Bert面试题.md)
  - [NLG应用场景](./AI算法/NLP/应用场景/NLG.md)

---

# 文件：AI算法\CV\CV基础.md

---

# CV基础知识

## 1. 为什么需要做特征归一化、标准化？

- 使不同量纲的特征处于同一数值量级，减少方差大的特征的影响，使模型更准确。
- 加快学习算法的收敛速度。
  **参考资料**:  [归一化 （Normalization）、标准化 （Standardization）和中心化/零均值化 （Zero-centered）](https://www.jianshu.com/p/95a8f035c86c)，《百面机器学习》

## 2. 常用常用的归一化和标准化的方法有哪些？

- 线性归一化（min-max标准化）
  x’ = (x-min(x)) / (max(x)-min(x))，其中max是样本数据的最大值，min是样本数据的最小值
  适用于数值比较集中的情况，可使用经验值常量来来代替max，min
- 标准差归一化（z-score 0均值标准化）
  x’=(x-μ) / σ，其中μ为所有样本的均值，σ为所有样本的标准差
  经过处理后符合标准正态分布，即均值为0，标准差为1
- 非线性归一化
  使用非线性函数log、指数、正切等，如y = 1-e^(-x)，在x∈[0, 6]变化较明显， 用在数据分化比较大的场景
  ![20210126230537122.png](./images/20210126230537122.png)

## 3. 介绍一下空洞卷积的原理和作用

空洞卷积(Atrous Convolution)也叫做膨胀卷积、扩张卷积，最初的提出是为了解决图像分割在用下采样（池化、卷积）增加感受野时带来的特征图缩小，后再上采样回去时造成的精度上的损失。空洞卷积通过引入了一个扩张率的超参数，该参数定义了卷积核处理数据时各值的间距。
![v2-a08645e392a6a5cb49e271e5310f0dd8_1440w.png](./images/v2-a08645e392a6a5cb49e271e5310f0dd8_1440w.png)
可以在增加感受野的同时保持特征图的尺寸不变,从而代替下采样和上采样，通过调整扩张率得到不同的感受野不大小：
a. 是普通的卷积过程(dilation rate = 1),卷积后的感受野为3
b. 是dilation rate = 2的空洞卷积,卷积后的感受野为5
c. 是dilation rate = 3的空洞卷积,卷积后的感受野为8
可以这么说,普通卷积是空洞卷积的一种特殊情况
**参考资料**: [吃透空洞卷积(Dilated Convolutions)](https://zhuanlan.zhihu.com/p/113285797)、[『计算机视觉』空洞卷积](https://www.cnblogs.com/hellcat/p/9687624.html)

## 4. 怎么判断模型是否过拟合，有哪些防止过拟合的策略？

在构建模型的过程中，通常会划分训练集、测试集。
当模型在训练集上精度很高，在测试集上精度很差时，模型过拟合；当模型在训练集和测试集上精度都很差时，模型欠拟合。**预防过拟合策略：**

- 增加训练数据：获取更多数据，也可以使用图像增强、增样等；
- 使用合适的模型：适当减少网络的层数、降低网络参数量；
- Dropout：随机抑制网络中一部分神经元，使的每次训练都有一批神经元不参与模型训练；
- L1、L2正则化：训练时限制权值的大小，增加惩罚机制，使得网络更稀疏；
- 数据清洗：去除问题数据、错误标签和噪声数据；
- 限制网络训练时间：在训练时将训练集和验证集损失分别输出，当训练集损失持续下降，而验证集损失不再下降时，网络就开始出现过拟合现象，此时就可以停止训练了；
- 在网络中使用BN层（Batch Normalization）也可以一定程度上防止过拟合。
  **参考资料**：[N，LN，IN，GN都是什么？不同归一化方法的比较](https://cloud.tencent.com/developer/article/1651655)、[深度学习中的五种归一化（BN、LN、IN、GN和SN）方法简介](https://blog.csdn.net/u013289254/article/details/99690730)、[层归一化，循环批归一化（2016）和批归一化RNN（2015）有什么区别？](https://qastack.cn/datascience/12956/paper-whats-the-difference-between-layer-normalization-recurrent-batch-normal)

## 5. 除了SGD和Adam之外，你还知道哪些优化算法？

主要有三大类：
a. 基本梯度下降法，包括 GD，BGD，SGD；
b. 动量优化法，包括 Momentum，NAG 等；
c. 自适应学习率优化法，包括 Adam，AdaGrad，RMSProp 等。
**参考资料**: [从SGD到NadaMax，十种优化算法原理及实现](https://mp.weixin.qq.com/s/L9jCK5rtyq3fJZEBpLvagg)

## 6. 阐述一下感受野的概念，并说一下在CNN中如何计算

- 感受野指的是卷积神经网络每一层输出的特征图上每个像素点映射回输入图像上的区域的大小，神经元感受野的范围越大表示其接触到的原始图像范围就越大，也就意味着它能学习更为全局，语义层次更高的特征信息，相反，范围越小则表示其所包含的特征越趋向局部和细节。因此感受野的范围可以用来大致判断每一层的抽象层次，并且我们可以很明显地知道网络越深，神经元的感受野越大。
- 卷积层的感受野大小与其之前层的卷积核尺寸和步长有关，与padding无关。
  [计算CNN的感受野](https://mp.weixin.qq.com/s?__biz=MzI4ODY2NjYzMQ%3D%3D&chksm=ec3ba8eddb4c21fbb3083e5508b7370d8885661a775bd6c95368fb2638b1b22e92f179b94f54&idx=1&mid=2247486853&scene=21&sn=f98b7038ec4ac67be69537a3efb93206#wechat_redirect)

## 7. 训练神经网络有哪些调参技巧

[深度学习调参技巧合集](https://mp.weixin.qq.com/s/Ml9MrdbgxRZAnLYxIrev4Q)
[22个神经网络训练技巧](https://mp.weixin.qq.com/s/lmh0J-to5V8jylPWeUymzQ)

## 8. 神经网络的深度和宽度分别指的是什么？

神经网络的深度决定了网络的表达能力，早期的backbone设计都是直接堆叠卷积层，它的深度指的是神经网络的层数；后来的backbone设计采用了更高效的module（或block）堆叠的方式，每个module是由多个卷积层组成，这时深度指的是module的个数。
神经网络的宽度决定了网络在某一层学习到的信息量，指的是卷积神经网络中最大的通道数，由卷积核数量最多的层决定。通常的结构设计中卷积核的数量随着层数越来越多的，直到最后一层feature map达到最大，这是因为越到深层，feature map的分辨率越小，所包含的信息越高级，所以需要更多的卷积核来进行学习。通道越多效果越好，但带来的计算量也会大大增加，所以具体设定也是一个调参的过程，并且各层通道数会按照8×的倍数来确定，这样有利于GPU的并行计算。

## 9. 上采样的原理和常用方式

在卷积神经网络中，由于输入图像通过卷积神经网络(CNN)提取特征后，输出的尺寸往往会变小，而有时我们需要将图像恢复到原来的尺寸以便进行进一步的计算(如图像的语义分割)，这个使图像由小分辨率映射到大分辨率的操作，叫做上采样，它的实现一般有三种方式：
a. 插值，一般使用的是双线性插值，因为效果最好，虽然计算上比其他插值方式复杂，但是相对于卷积计算可以说不值一提，其他插值方式还有最近邻插值、三线性插值等；
b. 转置卷积又或是说反卷积，通过对输入feature map间隔填充0，再进行标准的卷积计算，可以使得输出feature map的尺寸比输入更大；
c. Max Unpooling，在对称的max pooling位置记录最大值的索引位置，然后在unpooling阶段时将对应的值放置到原先最大值位置，其余位置补0；
**参考资料**：[深度卷积网络中如何进行上采样？](https://www.cnblogs.com/jiangkejie/p/12904304.html)、[三种上采样方法 | Three up sampling methods](https://zhuanlan.zhihu.com/p/344354520)、[上采样（upsampling）](https://www.malaoshi.top/show_1EF52HM7gu6g.html)

## 10. 下采样的作用是什么？通常有哪些方式？

下采样层有两个作用，一是减少计算量，防止过拟合；二是增大感受野，使得后面的卷积核能够学到更加全局的信息。
下采样的方式主要有两种：
a. 采用stride为2的池化层，如Max-pooling和Average-pooling，目前通常使用Max-pooling，因为他计算简单而且能够更好的保留纹理特征；
b. 采用stride为2的卷积层，下采样的过程是一个信息损失的过程，而池化层是不可学习的，用stride为2的可学习卷积层来代替pooling可以得到更好的效果，当然同时也增加了一定的计算量。
**参考资料**：[CNN详解（卷积层及下采样层）](https://blog.csdn.net/baidu_14831657/article/details/60570765)

## 11.  模型的参数量指的是什么？怎么计算？

参数量指的是网络中可学习变量的数量，包括卷积核的权重weights，批归一化（BN）的缩放系数γ，偏移系数β，有些没有BN的层可能有偏置bias，这些都是可学习的参数，即在模型训练开始前被赋予初值，在训练过程根据链式法则不断迭代更新，整个模型的参数量主要是由卷积核的权重weights的数量决定，参数量越大，则该结构对平台运行的内存要求越高。**参数量的计算方式：**

- Kh × Kw × Cin × Cout （Conv卷积网络）
- Cin × Cout （FC全连接网络）

## 12. 模型的FLOPs（计算量）指的是什么？怎么计算？

神经网络的前向推理过程基本上都是乘累加计算，所以它的计算量也是指的前向推理过程中乘加运算的次数，通常用FLOPs来表示，即floating point operations(浮点运算数)。计算量越大，在同一平台上模型运行延时越长，尤其是在移动端/嵌入式这种资源受限的平台上想要达到实时性的要求就必须要求模型的计算量尽可能地低，但这个不是严格成正比关系，也跟具体算子的计算密集程度(即计算时间与IO时间占比)和该算子底层优化的程度有关。**FLOPs的计算方式：**

- Kh × Kw × Cin × Cout × H × W = params × H × W （Conv卷积网络）
- Cin x Cout （FC全连接网络）
- 计算量 = 输出的feature map * 当前层filter 即（H × W × Cout） × （K × K × Cin）
  **参考资料**：[CNN 模型所需的计算力flops是什么？怎么计算？](https://zhuanlan.zhihu.com/p/137719986)、[pytorch: 计算网络模型的计算量(FLOPs)和参数量(Params)](https://blog.csdn.net/Caesar6666/article/details/109842379)、[神经网络的计算量和参数量估计总结](https://mp.weixin.qq.com/s/RoyqxxAWE1zSxlL-wqau9A)

## 13. 有哪些经典的卷积类型？

[经典的卷积类型](https://blog.csdn.net/weixin_37737254/article/details/102920408)

## 14. 深度可分离卷积的概念和作用

深度可分离卷积将传统的卷积分两步进行，分别是depthwise和pointwise。首先按照通道进行计算按位相乘的计算，深度可分离卷积中的卷积核都是单通道的，输出不能改变feature map的通道数，此时通道数不变；然后依然得到将第一步的结果，使用1*1的卷积核进行传统的卷积运算，此时通道数可以进行改变。
![20200314215428236.png](./images/20200314215428236.png)
计算量的前后对比：
Kh × Kw × Cin × Cout × H × W
变成了 Kh × Kw × Cin × H × W + 1 × 1 × Cin × Cout × H × W

## 15. 神经网络中Addition / Concatenate区别是什么？

Addition和Concatenate分支操作统称为shortcut，Addition是在ResNet中提出，两个相同维度的feature map相同位置点的值直接相加，得到新的相同维度feature map，这个操作可以融合之前的特征，增加信息的表达，Concatenate操作是在Inception中首次使用，被DenseNet发扬光大，和addition不同的是，它只要求两个feature map的HW相同，通道数可以不同，然后两个feature map在通道上直接拼接，得到一个更大的feature map，它保留了一些原始的特征，增加了特征的数量，使得有效的信息流继续向后传递。
![3e1f4862466b01fdfb642f6c1d6da19b.png](./images/3e1f4862466b01fdfb642f6c1d6da19b.png)
**参考资料**：[The difference and connection between contact and add operation (feature fusion)](https://www.programmersought.com/article/48464790207/)

## 16. 激活函数是什么？你知道哪些常用的激活函数？

激活函数（又叫激励函数，后面就全部统称为激活函数）是模型整个结构中的非线性扭曲力，神经网络的每层都会有一个激活函数。
常用的激活函数：Sigmoid函数、tanh函数、Relu函数、Leaky ReLU函数（PReLU）、ELU (Exponential Linear Units) 函数、MaxOut函数
**参考资料**：[常用激活函数的比较](https://zhuanlan.zhihu.com/p/32610035)、[深度学习领域最常用的10个激活函数，一文详解数学原理及优缺点](https://www.jiqizhixin.com/articles/2021-02-24-7)、[26种神经网络激活函数可视化](https://www.jiqizhixin.com/articles/2017-10-10-3)、[深度学习笔记——常用的激活（激励）函数](https://www.cnblogs.com/wj-1314/p/12015278.html)

## 17. 神经网络中1×1卷积有什么作用？

- 降维，减少计算量；在ResNet模块中，先通过1×1卷积对通道数进行降通道，再送入3×3的卷积中，能够有效的减少神经网络的参数量和计算量；
- 升维；用最少的参数拓宽网络通道，通常在轻量级的网络中会用到，经过深度可分离卷积后，使用1×1卷积核增加通道的数量，例如mobilenet、shufflenet等；
- 实现跨通道的交互和信息整合；增强通道层面上特征融合的信息，在feature map尺度不变的情况下，实现通道升维、降维操作其实就是通道间信息的线性组合变化，也就是通道的信息交互整合的过程；
- 1×1卷积核可以在保持feature map尺度（不损失分辨率）不变的情况下，大幅增加非线性特性（利用后接的非线性激活函数）。
  **参考资料**：[卷积神经网络中用1*1 卷积有什么作用或者好处呢？](https://www.zhihu.com/question/56024942)、[深度学习——1×1卷积核理解](https://www.cnblogs.com/czifan/p/9490565.html)

## 18. 随机梯度下降相比全局梯度下降好处是什么？

- 当处理大量数据时，比如SSD或者faster-rcnn等目标检测算法，每个样本都有大量候选框参与训练，这时使用随机梯度下降法能够加快梯度的计算；
- 每次只随机选取一个样本来更新模型参数，因此每次的学习是非常快速的，并且可以进行在线更新。

## 19. 如果在网络初始化时给网络赋予0的权重，这个网络能正常训练嘛？

不能，因为初始化权重是0，每次传入的不同数据得到的结果是相同的。网络无法更新

## 20. 为什么要对网络进行初始化，有哪些初始化的方法？

权重初始化的目的是在深度神经网络中前向传递时，阻止网络层的激活函数输出爆炸（无穷大）或者消失（0）。如果网络层的输出爆炸或者消失，损失函数的梯度也会变得很大或者很小，无法有效后向传递，使得神经网络需要更长的时间才能收敛甚至无法收敛。
**初始化方法有：** Xavier初始化、Kaiming初始化、随机初始化
**参考资料**：[神经网络中的权重初始化一览：从基础到Kaiming](https://zhuanlan.zhihu.com/p/62850258)、[深度学习中神经网络的几种权重初始化方法](https://blog.csdn.net/u012328159/article/details/80025785)、[神经网络参数初始化方法](https://blog.csdn.net/yyl424525/article/details/100823398)

## 21. 增大感受野的方法？

空洞卷积、池化操作、较大卷积核尺寸的卷积操作

## 22. 神经网络的正则化方法？过拟合的解决方法？

- 数据增强(镜像对称、随机裁剪、旋转图像、剪切图像、局部弯曲图像、色彩转换)
- early stopping(比较训练损失和验证损失曲线，验证损失最小即为最优迭代次数)
- L2正则化(权重参数的平方和)
- L1正则化(权重参数的绝对值之和)
- dropout 正则化(设置keep_pro参数随机让当前层神经元失活)

## 23. 梯度消失和梯度爆炸的原因是什么？

原因：激活函数的选择。
梯度消失：令bias=0，则神经网络的输出结果等于各层权重参数的积再与输入数据集相乘，若参数值较小时，则权重参数呈指数级减小。
梯度爆炸：令bias=0，则神经网络的输出结果等于各层权重参数的积再与输入数据集相乘，若参数值较大时，则权重参数呈指数级增长。
**参考资料**：[出现梯度消失与梯度爆炸的原因以及解决方案](https://www.cnblogs.com/XDU-Lakers/p/10553239.html)、[详解机器学习中的梯度消失、爆炸原因及其解决方法](https://blog.csdn.net/qq_25737169/article/details/78847691)

## 24. 深度学习为什么在计算机视觉领域这么好？

传统的计算机视觉方法需首先基于经验手动设计特征，然后使用分类器分类，这两个过程都是分开的。而深度学习里的卷积网络可实现对局部区域信息的提取，获得更高级的特征，当神经网络层数越多时，提取的特征会更抽象，将更有助于分类，同时神经网路将提取特征和分类融合在一个结构中。

## 25. 为什么神经网络种常用relu作为激活函数？

- 在前向传播和反向传播过程中，ReLU相比于Sigmoid等激活函数计算量小；
- 在反向传播过程中，Sigmoid函数存在饱和区，若激活值进入饱和区，则其梯度更新值非常小，导致出现梯度消失的现象。而ReLU没有饱和区，可避免此问题；
- ReLU可令部分神经元输出为0，造成网络的稀疏性，减少前后层参数对当前层参数的影响，提升了模型的泛化性能。

## 26. 卷积层和全连接层的区别是什么？

- 卷积层是局部连接，所以提取的是局部信息；全连接层是全局连接，所以提取的是全局信息；
- 当卷积层的局部连接是全局连接时，全连接层是卷积层的特例；

## 27. 什么是正则化？L1正则化和L2正则化有什么区别？

所谓的正则化，就是在原来 Loss Function 的基础上，加了一些正则化项，或者叫做模型复杂度惩罚项，正则化机器学习中一种常用的技术，其主要目的是控制模型复杂度，减小过拟合。
两者的区别：
L1：它的优良性质是能产生稀疏性，导致 W 中许多项变成零。 稀疏的解除了计算量上的好处之外，更重要的是更具有“可解释性”。
L2：使得模型的解偏向于范数较小的 W，通过限制 W 范数的大小实现了对模型空间的限制，从而在一定程度上避免了过拟合。不过岭回归并不具有产生稀疏解的能力，得到的系数仍然需要数据中的所有特征才能计算预测结果，从计算量上来说并没有得到改观。
L1范数更容易产生稀疏的权重，L2范数更容易产生分散的权重
**参考资料**：[l1正则与l2正则的特点是什么，各有什么优势？](https://www.zhihu.com/question/26485586)、[深入理解L1、L2正则化](https://www.cnblogs.com/zingp/p/10375691.html)

## 28. 常见的损失函数有哪些？你用过哪些？

平方损失（预测问题）、交叉熵（分类问题）、hinge损失（SVM支持向量机）、CART回归树的残差损失
[总结 | 深度学习损失函数大全](https://mp.weixin.qq.com/s/jYvTvA_LZjJrDQ2q6Crg3w)

## 29. dropout为什么能解决过拟合？

防止参数过分依赖训练数据，减少神经元之间复杂的共适应关系，增加参数对数据集的泛化能力。

## 30. 深度学习中的batch的大小对学习效果有何影响？

一般来说，在合理的范围之内，越大的 batch size 使下降方向越准确，震荡越小；batch size 如果过大，则可能会出现内存爆表和局部最优的情况。小的 bath size 引入的随机性更大，难以达到收敛，极少数情况下可能会效果变好。
**参考资料**：[深度学习中的batch的大小对学习效果有何影响？](https://www.zhihu.com/question/32673260)，[深度学习中Batch size对训练效果的影响](https://blog.csdn.net/xjp_xujiping/article/details/102235107)

## 31. PyTorch和TensorFlow的特点分别是什么？

- TensorFlow与PyTorch编程方式不同
  PyTorch：命令式编程；TensorFlow：符号式编程
- 图的创建及调试不同
  pytorch 图结构的创建是动态的，即图是运行时创建；更易调试pytorch代码，调试pytorch代码就像调试python代码一样，可以利用pdp在任何地方设置断点
- 灵活性、设备管理不同

**参考资料**：[pytorch 和 tensorflow的区别和选择](https://www.huaweicloud.com/articles/da8df2db34a0b4a101a8bfbd6fe04366.html)；[TensorFlow与PyTorch编程方式不同](https://blog.csdn.net/wuzhuoxi7116/article/details/103706390)

tensorflow 图结构的创建是静态的，即图首先被"编译"，然后在运行；不易调试要么从会话请求检查变量，要么学习使用tfdbg调试器

## 32. Pytorch 多卡并行的时候怎么实现参数共享，通信梯度是指平均梯度，还是最大梯度，还是梯度总和？

**参考资料**：[Pytorch 分布式训练](https://zhuanlan.zhihu.com/p/76638962)；[深度学习框架分布式训练总结](https://blog.kkyan.cn/posts/2019/05/07/distributed-train.html)

## 33. 数据不平衡的解决方法

- 欠采样 - 随机删除观测数量足够多的类，使得两个类别间的相对比例是显著的。虽然这种方法使用起来非常简单，但很有可能被我们删除了的数据包含着预测类的重要信息。
- 过采样 - 对于不平衡的类别，我们使用拷贝现有样本的方法随机增加观测数量。理想情况下这种方法给了我们足够的样本数，但过采样可能导致过拟合训练数据。
- 合成采样（ SMOTE ）-该技术要求我们用合成方法得到不平衡类别的观测，该技术与现有的使用最近邻分类方法很类似。问题在于当一个类别的观测数量极度稀少时该怎么做。比如说，我们想用图片分类问题确定一个稀有物种，但我们可能只有一幅这个稀有物种的图片。
- 在loss方面，采用focal loss等loss进行控制不平衡样本。

## 34. ReLU函数在0处不可导，为什么还能用？

反馈神经网络正常工作需要的条件就是每一个点提供一个方向，即导数；0值不可微，本质上来说是因为这个地方可画多条切线，但我们需要的只是一条；由于这出现的0值的概率极低，任意选择一个子梯度就OK了，在0处的次微分集合是【0，1】；即选择其中一个就OK了；一般默认是0；

## 35. Pooling层的作用以及如何进行反向传播

池化层没有可以训练的参数，因此在卷积神经网络的训练中，池化层只需要将误差传递到上一层，并不需要做梯度的计算。要追求一个原则，那就是梯度之和不变。

- average pooling: 前向传播是取某特征区域的平均值进行输出，这个区域的每一个神经元都是有参与前向传播了的，因此，在反向传播时，框架需要将梯度平均分配给每一个神经元再进行反向传播;
  ![KocaU1zbxnXYsyJ.jpg](./images/KocaU1zbxnXYsyJ.jpg)
- max pooling: 前向传播是取某特征区域的最大值进行输出，这个区域仅有最大值神经元参与了前向传播，因此，在反向传播时，框架仅需要将该区域的梯度直接分配到最大值神经元即可，其他神经元的梯度被分配为0且是被舍弃不参与反向传播的，但如何确认最大值神经元，这个还得框架在进行前向传播时记录下最大值神经元的Max ID位置。
  ![CZnUSwEcFy84JVL.jpg](./images/CZnUSwEcFy84JVL.jpg)

**参考资料**：[池化层（pooling）的反向传播是怎么实现的](https://blog.csdn.net/Jason_yyz/article/details/80003271)

## 36. 为什么max pooling 要更常用？什么场景下 average pooling 比 max pooling 更合适？

- 作用：对输入的特征图进行压缩，一方面使特征图变小，简化网络计算复杂度；一方面进行特征压缩，提取主要特征。
- 通常来讲，max-pooling的效果更好，虽然max-pooling和average-pooling都对数据做了下采样，但是max-pooling感觉更像是做了特征选择，选出了分类辨识度更好的特征，提供了非线性。 pooling的主要作用一方面是去掉冗余信息，一方面要保留feature map的特征信息，在分类问题中，我们需要知道的是这张图像有什么object，而不大关心这个object位置在哪，在这种情况下显然max pooling比average pooling更合适。在网络比较深的地方，特征已经稀疏了，从一块区域里选出最大的，比起这片区域的平均值来，更能把稀疏的特征传递下去。
- average-pooling更强调对整体特征信息进行一层下采样，在减少参数维度的贡献上更大一点，更多的体现在信息的完整传递这个维度上，在一个很大很有代表性的模型中，比如说DenseNet中的模块之间的连接大多采用average-pooling，在减少维度的同时，更有利信息传递到下一个模块进行特征提取。
- average-pooling在全局平均池化操作中应用也比较广，在ResNet和Inception结构中最后一层都使用了平均池化。有的时候在模型接近分类器的末端使用全局平均池化还可以代替Flatten操作，使输入数据变成一维向量。

**参考资料**：[计算机视觉面试常见问题](https://blog.nowcoder.net/n/9c67c993bf9842a68aaa7622228c2299)

## 37. 为什么要反向传播？手推反向传播公式展示一下

反向传播算法的motivation是期望通过在神经网络的训练过程中自适应的调整各神经元间的连接权值，以寻求最佳的输入输出间的映射函数，使得目标函数或损失函数达到最小，完成分类、回归等任务。
手推反向传播公式：[BP（反向传播算法）公式推导及例题解析](https://zhuanlan.zhihu.com/p/32819991)

## 38. CV中的卷积操作和数学上的严格定义的卷积的关系？

CNN的卷积运算并非数学定义的卷积，CNN中的运算是不需要翻转卷积核的。

## 39. 简述CNN分类网络的演变脉络及各自的贡献与特点

**参考答案**：[卷积神经网络发展历程](https://zhuanlan.zhihu.com/p/76275427)；[CNN模型演变](https://zhuanlan.zhihu.com/p/108522266)；[一文详解卷积神经网络的演变历程！](https://cloud.tencent.com/developer/article/1065373)

## 40. 神经网络的优缺点？

**优势:**

- 拟合复杂的函数：随着神经网络层数的加深，网络的非线性程度愈来愈高，从而可拟合更加复杂的函数；
- 结构灵活：神经网络的结构可根据具体的任务进行相应的调整，选择适合的网络结构；

**参考资料**：[深度学习 优缺点](https://www.cnblogs.com/emanlee/p/12404147.html)

## 41. Softmax+Cross Entropy如何反向求导？

![ce.png](./images/ce.png)

## 42. 有什么数据增强的方式？

- 单样本几何变换：翻转，旋转，裁剪，缩放
- 单样本像素内容变换：噪声，模糊，颜色扰动
- 多样本插值 Mixup：图像和标签都进行线性插值

**参考资料**：[计算机视觉中的数据增强](https://www.cnblogs.com/shuiyj/p/13185329.html)

## 43. 为什么在模型训练开始会有warm up？

warm up, 在刚刚开始训练时以很小的学习率进行训练，使得网络熟悉数据，随着训练的进行学习率慢慢变大，到了一定程度，以设置的初始学习率进行训练，接着过了一些inter后，学习率再慢慢变小；学习率变化：上升——平稳——下降。

- 有助于减缓模型在初始阶段对mini-batch的提前过拟合现象，保持分布的平稳；
- 有助于保持模型深层的稳定性。

**参考资料**：[深度学习 warmup 策略](https://blog.csdn.net/comway_Li/article/details/105016725)；[神经网络中 warmup 策略为什么有效；有什么理论解释么？](https://www.zhihu.com/question/338066667)

## 44. VGG使用3\*3卷积核的优势是什么?

2个3\*3的卷积核串联和5\*5的卷积核有相同的感知野,前者拥有更少的参数。多个3\*3的卷积核比一个较大尺寸的卷积核有更多层的非线性函数,增加了非线性表达,使判决函数更具有判决性。

## 45. 什么是Group Convolution

若卷积神将网络的上一层有N个卷积核,则对应的通道数也为N。设群数目为M,在进行卷积操作的时候,将通道分成M份,每个group对应N/M个通道,然后每个group卷积完成后输出叠在一起,作为当前层的输出通道。

## 46. 训练过程中,若一个模型不收敛,那么是否说明这个模型无效?导致模型不收敛的原因有哪些?

并不能说明这个模型无效,导致模型不收敛的原因可能有数据分类的标注不准确,样本的信息量太大导致模型不足以fit整个样本空间。学习率设置的太大容易产生震荡,太小会导致不收敛。可能复杂的分类任务用了简单的模型。数据没有进行归一化的操作。

## 47. Relu比Sigmoid的效果好在哪里?

Sigmoid的导数只有在0的附近时有较好的激活性,而在正负饱和区域的梯度趋向于0,从而产生梯度弥散的现象,而relu在大于0的部分梯度为常数,所以不会有梯度弥散现象。Relu的导数计算的更快。Relu在负半区的导数为0,所以神经元激活值为负时,梯度为0,此神经元不参与训练,具有稀疏性。

## 48. Batch Normalization的作用

神经网络在训练的时候随着网络层数的加深,激活函数的输入值的整体分布逐渐往激活函数的取值区间上下限靠近,从而导致在反向传播时低层的神经网络的梯度消失。而BatchNormalization的作用是通过规范化的手段,将越来越偏的分布拉回到标准化的分布,使得激活函数的输入值落在激活函数对输入比较敏感的区域,从而使梯度变大,加快学习收敛速度,避免梯度消失的问题。

## 49. GAN网络的思想

GAN用一个生成模型和一个判别模型,判别模型用于判断给定的图片是不是真实的图片,生成模型自己生成一张图片和想要的图片很像,开始时两个模型都没有训练,然后两个模型一起进行对抗训练,生成模型产生图片去欺骗判别模型,判别模型去判别真假,最终两个模型在训练过程中,能力越来越强最终达到稳态。

## 50. Attention机制的作用

减少处理高维输入数据的计算负担,结构化的选取输入的子集,从而降低数据的维度。让系统更加容易的找到输入的数据中与当前输出信息相关的有用信息,从而提高输出的质量。帮助类似于decoder这样的模型框架更好的学到多种内容模态之间的相互关系。

## 51. 怎么提升网络的泛化能力

从数据上提升性能:收集更多的数据,对数据做缩放和变换,特征组合和重新定义问题。
从算法调优上提升性能:用可靠的模型诊断工具对模型进行诊断,权重的初始化,用小的随机数初始化权重。对学习率进行调节,尝试选择合适的激活函数,调整网络的拓扑结构,调节batch和epoch的大小,添加正则化的方法,尝试使用其它的优化方法,使用early stopping。

## 52. CNN为什么比DNN在图像识别上更好

DNN的输入是向量形式，并未考虑到平面的结构信息，在图像和NLP领域这一结构信息尤为重要，例如识别图像中的数字，同一数字与所在位置无关（换句话说任一位置的权重都应相同），CNN的输入可以是tensor，例如二维矩阵，通过filter获得局部特征，较好的保留了平面结构信息。

## 53. DNN的梯度是如何更新的？

参考答案：[DNN的梯度更新方式](https://www.nowcoder.com/ta/review-ml/review?tpId=96&tqId=32614&query=&asc=true&order=&page=183)

## 54. Depthwise 卷积实际速度与理论速度差距较大，解释原因。

首先，caffe原先的gpu实现group convolution很糟糕，用for循环每次算一个卷积，速度极慢。第二，cudnn7.0及之后直接支持group convolution，但本人实测，速度比github上几个直接写cuda kernel计算的dw convolution速度慢。例如对于n=128, c=512, h=32, w=32, group=512的卷积跑100次，cudnn 7.0里的group convolution需要4秒多，而DepthwiseConvolution大概只需要1秒。
分析了一下dw convolution与普通convolution的理论计算复杂度，举例如下：
卷积1：普通卷积，输入为64\*64\*256，输出为64\*64\*256，卷积核大小为3\*3。参数为3\*3\*256\*256=590K，计算量为64\*64\*256\*3\*3\*256=2.42G，计算过程的工作集内存总量（输入输出数据+参数）为64\*64\*256\*2 + 3\*3\*256\*256 = 2.69M。
卷积2：dw卷积，输入为64\*64\*256，输出为64\*64\*256，卷积核大小为3\*3。参数为3\*3\*256=2.3K个，计算量为64\*64\*256\*3\*3=9.44M，计算过程的工作集内存总量为64\*64\*256\*2 + 3\*3\*256=2.10M。
卷积3：普通卷积，输入为64\*64\*16，输出为64\*64\*16，卷积核大小为3\*3。参数为3\*3\*16\*16=2.3K个，计算量为64\*64\*16\*3\*3\*16=9.44M，计算过程的工作集内存总量为64\*64\*16\*2 + 3\*3\*16\*16=133K。
可以看到卷积2肯定比卷积1快，因为计算量下降到1/256了，但卷积2实际上无法达到卷积1的256倍速度（我记得我测得结果大概是快10倍左右），因为工作集内存大小并没有显著降低。卷积2也无法达到卷积3的速度，因为虽然FLOPS相同，但工作集内存大小相差了很多倍，因此单位数据的计算密度小很多，很难充分利用GPU上的计算单元。

## 主要的参考文献：

- [深度学习CV岗位面试基础问题总结（基础篇）](https://blog.csdn.net/qq_39056987/article/details/112157031)
- [计算机视觉岗常见面试题](https://github.com/donnyyou/cv-interview)
- [CV算法面试题总结](https://www.nowcoder.com/discuss/119664?type=2)
- [CV_interviews_Q-A](https://github.com/GYee/CV_interviews_Q-A)
- [深度学习六十问！一位算法工程师经历 30+ 场 CV 面试后总结的常见问题合集下](https://bbs.cvmart.net/articles/4896)
- [Awesome-algorithm-interview](https://github.com/lcylmhlcy/Awesome-algorithm-interview/blob/master/README.md#1-%E9%9D%A2%E8%AF%95)
- [CV计算机机视觉基础/面试题](https://lingan1996.top/32471.html)
- [深度学习三十问！一位算法工程师经历30+场CV面试后总结的常见问题合集](http://www.python88.com/topic/115196)
- [深度学习面试100题](https://www.cnblogs.com/think90/p/11461424.html)
- [CV深度学习面试问题记录](https://szukevin.site/2020/06/30/CV%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E9%9D%A2%E8%AF%95%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95/)
- [DeepLearning-500-questions](https://github.com/scutan90/DeepLearning-500-questions/)

---

# 文件：AI算法\CV\目标检测.md

---

# 目标检测部分

## 一、目标检测背景知识

目标检测任务是找出图像或视频中人们感兴趣的物体，并同时检测出它们的位置和大小(Bouding box)。不同于图像分类任务，目标检测不仅要解决分类问题，还要解决定位问题，是属于Multi-Task的问题。
<img src="images/检测分类分割.jpg" alt="检测分类分割" />

目标检测的发展脉络可以划分为两个周期：

- 传统目标检测算法时期(1998年-2014年)
- 基于深度学习的目标检测算法时期(2014年-至今)

<img src="images/目标检测模型年限图.png" alt="目标检测模型年限" style="zoom: 50%;" />

### 传统目标检测算法

不同于现在的卷积神经网络可以自动提取高效特征进行图像表示，以往的传统目标检测算法主要基于手工提取特征。传统检测算法流程可概括如下：

1. 选取感兴趣区域，选取可能包含物体的区域
2. 对可能包含物体的区域进行特征提取
3. 对提取的特征进行检测分类

基于手工提取特征的传统目标检测算法主要有以下三个缺点：

1. 识别效果不够好，准确率不高
2. 计算量较大，运算速度慢
3. 可能产生多个正确识别的结果

经典传统目标检测算法：
Viola-Jones(VJ)  detector（2001），Histogram of Oriented Gradients (HOG) Detector（2005）， Deformable Part-based Model (DPM)（2008，2010）

尽管现在基于深度学习的检测器在精度方面已经远远超过了传统检测器，但在他们身上仍然能看到许多来自传统检测器的影响，例如：混合模型，困难样本挖掘，边界框回归等。

### 基于深度学习目标检测算法

这里主要针对最近几年发展比较快的基于深度学习的目标检测算法。经过近7年的发展，目标检测的算法抽象成三个惯用的组件：Backbone, Neck, Head(DenseHead, RoIHead)。

1. Backbone用于特征提取。
2. Neck进行高低层特征的传递与融合，此外还起到重要的“分而治之”作用。
3. Head进行最终的预测。

<img src="images/目标检测模型示意图(YOLOv4).png" alt="目标检测模型示意图(YOLOv4)" style="zoom:50%;" />

## 二、Questions

### 1、Faster-Rcnn网络

#### （1）请介绍一下faster R-CNN网络的原理（注: 需要能够详细画出网络结构图）<顺丰-一面(2018)、腾讯-一面(2018)、旷视-三面（2018）>

Faster R-CNN是一种两阶段（two-stage）方法,它提出的RPN网络取代了选择性搜索（Selective search）算法后使检测任务可以由神经网络端到端地完成。

在结构上，Faster RCNN将特征抽取(feature extraction)，候选区域提取（Region proposal提取），边框回归（bounding box regression），分类（classification）都整合在了一个网络中，使得综合性能有较大提高，在检测速度方面尤为明显。
<img src="images/faster-rcnn原理图.png" alt="faster-rcnn原理图" style="zoom: 80%;" />

#### （2）请简要叙述一下"R-CNN——faster R-CNN"

1. R-CNN。
   - 首先通过传统的 selective search 算法在图片上预取 2000 个左右 Region Proposal；
   - 接着将这些 Region Proposal 通过前处理统一尺寸输入到 CNN 中进行特征提取；
   - 然后把所提取的特征输入到 SVM 支持向量机中进行分类；
   - 最后对分类后的 Region Proposal 进行 bbox 回归。
     此时算法的整个过程较为繁琐，速度也较慢。
2. Fast R-CNN。
   - 首先通过传统的 selective search 算法在图片上预取 2000 个左右 Region Proposal；
   - 接着对整张图片进行特征提取；
   - 然后利用 Region Proposal 坐标在 CNN 的最后一个特征图上进去 RoI 特征图提取；
   - 最后将所有 RoI 特征输入到分类和回归模块中。
     此时算法的整个过程相比 R-CNN 得到极大的简化，但依然无法联合训练。
3. Faster R-CNN。
   - 首先通过可学习的 RPN 网络进行 Region Proposal 的预取；
   - 接着利用 Region Proposal 坐标在 CNN 的特征图上进行 RoI 特征图提取；
   - 然后利用 RoI Pooling 层进行空间池化使其所有特征图输出尺寸相同；
   - 最后将所有特征图输入到后续的 FC 层进行分类和回归。
     此时算法的整个过程一气呵成，实现了"端到端"训练。

#### （3）faster R-CNN是经典的two-stage 检测器，请简单说明其中two-stage的含义。

faster R-CNN之所以叫做 two-stage 检测器，原因是其包括一个区域提取网络 **RPN** 和 一个RoI Refine 网络 **R-CNN**(检测网络)，同时为了将 RPN 提取的不同大小的 RoI 特征图组成 batch 输入到后面的 R-CNN 中，在两者中间还插入了一个 RoI Pooling 层，可以保证任意大小特征图输入都可以变成指定大小输出。即在RPN提取ROI区域后，送入检测网络再进行分类和回归。
![faster-rcnn结构图示意图](images/faster-rcnn结构图示意图.jpg)

#### （4）请简要说明一下RPN（Region Proposal Network）网络的作用、实现细节<上交苏研院-二面（2020）、旷视-交叉面（2019）>

- **RPN网络的作用：**
  
  - RPN专门用来提取候选框，一方面RPN耗时少，另一方面RPN可以很容易结合到Fast RCNN中，成为一个整体。
- **RPN网络的实现细节：**
  
  - 1个特征图（Faster RCNN的公共Feature Map）经过sliding window处理，得到256维特征，对每个特征向量做两次全连接操作，一个得到2个分数，一个得到4个坐标{然后通过两次全连接得到结果2k个分数和4k个坐标[k指的是由锚点产生的K个框(K anchor boxes)]}
  - 2个分数，即每个结果都有2个分数，因为RPN是提候选框，还不用判断类别，所以只要求区分是不是物体就行，那么就有两个分数，前景（物体）的分数，和背景的分数；
  - 4个坐标，即每个结果都有4个坐标，这里是指对**原图**的坐标偏移，一定要记住是原图；
  - 9个anchor，faster R-CNN预先设定好的anchor共有9种组合，所以k等于9，最后我们的结果是针对这9种组合的，所以特征图上有H x W x 9个结果，每个特征点共有18个分数和36个坐标。
    ![RPN网络示意图](images/RPN网络示意图.png)
- **RPN的损失函数**(多任务损失:二分类损失+SmoothL1损失)
  训练RPN网络时，对于每个锚点(anchor)我们定义了一个二分类标签（是该物体或不是）。
  以下两种情况我们视锚点为了一个正样本标签时：
  
  1. 锚点和锚点们与标注之间的最高重叠矩形区域
  2. 或者锚点和标注的重叠区域指标（IOU）>0.7
     ![RPN_loss](images/RPN_loss.png)

$$
L({p_i},{t_i}) = \frac{1}{N_{cls}}\sum_{i}L_{cls}(p_i,p^*_i) + \lambda\frac{1}{N_{reg}}\sum_{i}p^*_iL_{reg}(t_i,t^*_i)
$$

- **RPN损失中的回归损失部分输入变量是怎么计算的？**(注意回归的不是坐标和宽高，而是由它们计算得到的偏移量)

$$
\begin{aligned}
smoothth_{L_1}(x) =\begin{cases}
0.5x^2, &if|x| < 1\\
|x| - 0.5,& otherwise \\
\end{cases}
  \end{aligned}
$$

其中 $ t_i $ 和$t^*_i$分别为网络的预测值和回归的目标

$$
\begin{aligned}
t_x &=x-x_a / w_a, & t_y=y-y_a / h_a \\
t_w &= w / w_a,  &t_m{h}= h / h_a \\
t_{x}^* &=x^*-x_a / w_a,  &t_{y}^*=y^*-y_a / h_a \\
t_{w}^* &= w^* / w_a, & t_{h}^*= h^* / h_a
\end{aligned}
$$

在训练RPN时需要准备好目标t*。它是通过ground-truth box（目标真实box）和anchor box（按一定规则生成的anchor box）计算得出的，代表的是ground-truth box与anchor box之间的转化关系。用这个来训练rpn，那么rpn最终学会输出一个良好的转化关系t。而这个t，是predicted box与anchor box之间的转化关系。通过这个t和anchor box，可以计算出预测框box的真实坐标

- **RPN中的anchor box是怎么选取的？**
  
  滑窗的中心在原像素空间的映射点称为anchor，以此anchor为中心，生成k(paper中default k=9, 3 scales and 3 aspect ratios/不同尺寸和不同长宽比)个proposals。三个面积尺寸$128^2，256^2，512^2）$，然后在每个面积尺寸下，取三种不同的长宽比例$（1:1,1:2,2:1）$
- **为什么提出anchor box？**<蘑菇街-一面（2018）>
  
  主要有两个原因：一个窗口只能检测一个目标、无法解决多尺度问题。
  
  目前anchor box尺寸的选择主要有三种方式：
  (1)人为经验选取
  (2)k-means聚类
  (3)作为超参数进行学习
- **为什么使用不同尺寸和不同长宽比？**
  为了得到更大的交并比(IOU)。

#### （5）说一下RoI Pooling是怎么做的？有什么缺陷？有什么作用<顺丰-一面(2018)>

RoI Pooling的过程就是将一个个大小不同的box矩形框，都映射成大小固定（w * h）的矩形框

- **具体操作：**
  （1）根据输入image，将ROI映射到feature map对应位置
  （2）将映射后的区域划分为相同大小的sections（sections数量与输出的维度相同）；
  （3）对每个sections进行max pooling操作；

这样可以从不同大小的方框得到固定大小的相应的feature maps。值得一提的是，输出的feature maps的大小不取决于ROI和卷积feature maps大小。ROI pooling 最大的好处就在于极大地提高了处理速度。
（*在Pooling的过程中需要计算Pooling后的结果对应到feature map上所占的范围，然后在那个范围中进行取max或者取average*）

- **优点：**
  （1）允许我们对CNN中的feature map进行reuse；
  （2）可以显著加速training和testing速度；
  （3）允许end-to-end的形式训练目标检测系统。
- **缺点**
  由于 RoIPooling 采用的是最近邻插值（即INTER_NEAREST） ，在resize时，对于缩放后坐标不能刚好为整数的情况，采用了粗暴的舍去小数，相当于选取离目标点最近的点，损失一定的空间精度。
- **两次整数化（量化）过程：**
  1.region proposal的xywh通常是小数，但是为了方便操作会把它整数化。
  2.将整数化后的边界区域平均分割成 k x k 个单元，对每一个单元边界进行整数化。
  （*经过上述两次整数化，此时的候选框已经和最开始回归出来的位置有一定的偏差，这个偏差会影响检测或者分割的准确度*）
- **怎么做的映射:**
  *映射规则:*

$$
\begin{aligned}
\frac{各点坐标}{输入图片大小/feature map大小}
\end{aligned}
$$

#### （6)ROI Pooling与ROI Align(Mask R-CNN)的区别 <字节跳动-二面（2019）>

- **ROI Align:**
  ROI Align的思路很简单：取消量化操作，使用双线性内插的方法获得坐标为浮点数的像素点上的图像数值,从而将整个特征聚集过程转化为一个连续的操作;
  1.遍历每一个候选区域，保持浮点数边界不做量化。
  2.将候选区域分割成$k x k$个单元，每个单元的边界也不做量化。
  3.在每个单元中计算固定四个坐标位置，用双线性内插的方法计算出这四个位置的值，然后进行最大池化操作。
- **区别:**
  ROI Align相比RoI Pooling舍去了近似像素取整数的量化方法，改用双线性插值的方法确定特征图坐标对应于原图中的像素位置.
  ROI Align很好地解决了ROI Pooling操作中两次量化造成的区域不匹配(mis-alignment)的问题。
  
  对于检测图片中大目标物体时，两种方案的差别不大，而如果是图片中有较多小目标物体需要检测，则优先选择RoiAlign，更精准些。
- **RoI Align中双线性插值计算像素值的具体方法**
  
  在数学上，双线性插值是有两个变量的插值函数的线性插值扩展，其核心思想是在两个方向分别进行一次线性插值。

<div align=center><img src =images/双线性插值.png#pic_center/></div>

假如我们想得到未知函数 $f$ 在点 $P = (x, y)$ 的值，假设我们已知函数$ f$ 在 $Q_{11} = (x_1, y_1)$、$Q_{12} = (x_1, y_2)$, $Q_{21} = (x_2, y_1)$ 以及$ Q_{22} = (x_2, y_2)$ 四个点的值。最常见的情况，f就是一个像素点的像素值。首先在 x 方向进行线性插值，得到

$$
\begin{aligned}
f(R_{1}) \approx \frac{x_{2}-x}{x_{2}-x_{1}} f(Q_{11})+\frac{x-x_{1}}{x_{2}-x_{1}} f(Q_{21}) \text { where } \quad R_{1}=(x, y_{1}) \\
f(R_{2}) \approx \frac{x_{2}-x}{x_{2}-x_{1}} f(Q_{12})+\frac{x-x_{1}}{x_{2}-x_{1}} f(Q_{22}) \quad \text { where } \quad R_{2}=(x, y_{2})
\end{aligned}
$$

然后在 y 方向进行线性插值，得到:

$$
f(P) \approx \frac{y_{2}-y}{y_{2}-y_{1}} f(R_{1})+\frac{y-y_{1}}{y_{2}-y_{1}} f(R_{2})
$$

综合起来就是双线性插值最后的结果：

$$
f(x, y) \approx \frac{f(Q_{11})}{(x_{2}-x_{1})(y_{2}-y_{1})}(x_{2}-x)(y_{2}-y)+\frac{f(Q_{21})}{(x_{2}-x_{1})(y_{2}-y_{1})}(x-x_{1})(y_{2}-y)\\
+\frac{f(Q_{12})}{(x_{2}-x_{1})(y_{2}-y_{1})}(x_{2}-x)(y-y_{1})+\frac{f(Q_{22})}{(x_{2}-x_{1})(y_{2}-y_{1})}(x-x_{1})(y-y_{1})
$$

由于图像双线性插值只会用相邻的4个点，因此上述公式的分母都是1。

每个采样点的特征值由其相邻的4个整型特征点的像素值通过双线性差值得到。

- **最近邻插值法(图像的内插):**
  在原图中最近得像素点赋值给新的像素点

#### （7）Faster R-CNN是如何解决正负样本不平衡的问题？

限制正负样本比例为1:1，如果正样本不足，就用负样本补充，这种方法后面研究工作用的不多。通常针对类别不平衡问题可以从调整样本数或修改loss weight两方面去解决，常用的方法有OHEM、OHNM、class balanced loss和Focal loss。

#### （8）Faster RCNN怎么筛选正负anchor

1. 给两种锚点分配一个正标签：
   （i）具有与实际边界框的重叠最高交并比（IoU）的锚点，
   （ii）具有与实际边界框的重叠超过0.7 IoU的锚点。
2. 负标签：IoU比率低于0.3。

#### （9）faster-rcnn中bbox回归用的是什么公式，说一下该网络是怎么回归bbox的？

$$
t_{x} =(x-x_{a}) / w_{a},  t_{y}=(y-y_{a}) / h_{a} \\
t_{w} =\log (w / w_{a}),  t_{h}=\log (h / h_{a}) \\
t_{x}^{*} =(x^{*}-x_{a}) / w_{a},  t_{y}^{*}=(y^{*}-y_{a}) / h_{a} \\
t_{w}^{*} =\log (w^{*} / w_{a}),  t_{h}^{*}=\log (h^{*} / h_{a})
$$

其中x,y,w,h分别为bbox的中心点坐标，宽与高。$x, x_{a}, x^{*}$分别是预测box、anchor box、真实box。

前两行是预测的box关于anchor的offset与scales，后两行是真实box与anchor的offset与scales。那回归的目的很明显，即使$t_{i}, t_{i}^{*}$得尽可能相近。回归损失函数利用的是Fast-RCNN中定义的smooth L1函数，对外点更不敏感：

$$
L_{reg}(t_{i}, t_{i}^{*})=\sum_{i \in\{x, y, w, h\}} {smooth}_{L1}(t_{i}-t_{i}^{*})
$$

$$
\text { smooth }_{L_{1}}(x)=\{\begin{array}{ll}
0.5 x^{2} & \text { if }|x|<1 \\
|x|-0.5 & \text { otherwise }
\end{array}
$$

损失函数优化权重W，使得测试时bbox经过W运算后可以得到一个较好的平移量（offsets）与尺度（scales），利用这个平移量（offsets）与尺度（scales）可在原预测bbox上微调，得到更好的预测结果。

- **为什么要做Bounding-box regression？**
  
  边框回归用来微调候选区域/框，使微调后的框更Ground Truth更接近。

#### （10）简述faster rcnn的前向计算过程并简述faster rcnn训练步骤

**前向计算过程：**

1. 输入一张待检测图片
2. vgg16网络(或者其他网络，例如：resnet50)conv layers提取整张图片的特征，输出feature map分别输入到RPN和Fast RCNN网络开头
3. RPN网络得出region proposal，将这些候选框信息送入到Fast RCNN网络开头
4. 利用候选框在之前送到的feature map提取特征，并通过ROI Pooling层得到规定大小的feature map
5. 将这些feature map送入Fast RCNN网络中进行分类和回归坐标，最终得到需检测物体的坐标。

**简述faster rcnn训练步骤：**

1. 训练RPN，该网络用ImageNet预训练的模型初始化，并端到端微调，用于生成region proposal；
2. 训练Fast R-CNN，由imageNet model初始化，利用第一步的RPN生成的region proposals作为输入数据，训练Fast R-CNN一个单独的检测网络，这时候两个网络还没有共享卷积层；
3. 调优RPN，用第二步的fast-rcnn model初始化RPN再次进行训练，但固定共享的卷积层，并且只微调RPN独有的层，现在两个网络共享卷积层了；
4. 调优Fast R-CNN,由第三步的RPN model初始化fast-RCNN网络，输入数据为第三步生成的proposals。保持共享的卷积层固定，微调Fast R-CNN的fc层。这样，两个网络共享相同的卷积层，构成一个统一的网络。

#### （11）介绍faster rcnn这个流程，faster rcnn有哪些缺点？如何改进？<商汤-一面（2019）>

- **改进：**
  1. 更好的特征网络ResNet等；
  2. 更精确的RPN：可以使用FPN网络架构来设计RPN网络
  3. 更好的ROI分类方法：比如ROI分别在conv4和conv5上做ROI-Pooling，合并后再进行分类，这样基本不增加计算量，又能利用更高分辨率的conv4；
  4. 使用softNMS代替NMS;

#### （12）阐述一下Mask RCNN网络，这个网络相比于Faster RCNN网络有哪些改进的地方

Mask rcnn网络是基于faster rcnn网络架构提出的新的目标检测网络。该网络可以在有效地完成目标检测的同时完成实例分割。
Mask RCNN主要的贡献在于如下：

1. 强化了基础网络：通过ResNeXt-101+FPN用作特征提取网络，达到state-of-the-art的效果。
2. ROIAlign替换之前faster rcnn中的ROI Pooling，解决错位（Misalignment）问题。
3. 使用新的Loss Function：Mask RCNN的损失函数是分类，回归再加上mask预测的损失之和。

#### （13） 比较FasterRCNN在RCNN系列中的改进点

改用可学习的 RPN 网络替换selective search 算法进行 Region Proposal 的预取；

总结来说，mask rcnn的主要贡献就是采用了ROI Align以及加了一个mask分支。

### 2、YOLO系列网络

#### （1）YOLOV1、YOLOV2、YOLOV3复述一遍  YOLOv1到v3的发展历程以及解决的问题。<云从科技-一面（2020）、旷视-三面（2018）>

YOLO系列算法是一类典型的one-stage目标检测算法，其利用anchor box将分类与目标定位的回归问题结合起来，从而做到了高效、灵活和泛化性能好。

1. **YOLOv1：** YOLOv1的核心思想就是利用整张图作为网络的输入，直接在输出层回归 bounding box（边界框） 的位置及其所属的类别。
   
   **YOLOv1的基本思想:** 把一副图片，首先reshape成448×448大小（由于网络中使用了全连接层，所以图片的尺寸需固定大小输入到CNN中），然后将划分成SxS个单元格（原文中S=7），以每个格子所在位置和对应内容为基础，来预测检测框和每个框的Confidence以及每个格子预测一共C个类别的概率分数。
   
   **创新点：**
   
   1. 将整张图作为网络的输入，直接在输出层回归bounding box的位置和所属的类别
   2. 速度快，one stage detection的开山之作
   
   损失函数设计细节：YOLOv1对位置坐标误差，IoU误差，分类误差均使用了均方差作为损失函数。激活函数（最后一层全连接层用线性激活函数，其余层采用leak RELU）
   
   **缺点：**
   
   1. 首先，每个单元格只预测2个bbox，然后每个单元格最后只取与gt_bbox的IOU高的那个最为最后的检测框，也只是说每个单元格最多只预测一个目标。
   2. 损失函数中，大物体 IOU 误差和小物体 IOU 误差对网络训练中 loss 贡献值接近（虽然采用求平方根方式，但没有根本解决问题）。因此，对于小物体，小的 IOU 误差也会对网络优化过程造成很大的影响，从而降低了物体检测的定位准确性。
   3. 由于输出层为全连接层，因此在检测时，YOLO 训练模型只支持与训练图像相同的输入分辨率的图片。
   4. 和two-stage方法相比，没有region proposal阶段，召回率较低
2. **YOLOv2：** YOLOv2又叫YOLO9000，其能检测超过9000种类别的物体。相比v1提高了训练图像的分辨率；引入了faster rcnn中**anchor box**的思想，对网络结构的设计进行了改进，输出层使用卷积层替代YOLO的全连接层，联合使用coco物体检测标注数据和imagenet物体分类标注数据训练物体检测模型。相比YOLO，YOLO9000在识别种类、精度、速度、和定位准确性等方面都有大大提升。
   
   **相比于v1的改进： **
   
   1. Anchor: 引入了Faster R-CNN中使用的Anchor，作者通过在所有训练图像的所有边界框上运行k-means聚类来选择锚的个数和形状(k = 5，因此它找到五个最常见的目标形状)
   2. 修改了网络结构，去掉了全连接层，改成了全卷积结构。
   3. 使用Batch Normalization可以从model中去掉Dropout，而不会产生过拟合。
   4. 训练时引入了世界树（WordTree）结构，将检测和分类问题做成了一个统一的框架，并且提出了一种层次性联合训练方法，将ImageNet分类数据集和COCO检测数据集同时对模型训练。
3. **YOLOv3：** YOLOv3总结了自己在YOLOv2的基础上做的一些尝试性改进，有的尝试取得了成功，而有的尝试并没有提升模型性能。其中有两个值得一提的亮点，一个是使用残差模型，进一步加深了网络结构；另一个是使用**FPN**架构实现多尺度检测。
   
   **改进点：**
   
   1. 多尺度预测 （类FPN）：每种尺度预测3个box, anchor的设计方式仍然使用聚类，得到9个聚类中心。
   2. 更好的基础分类网络（类ResNet）和分类器 darknet-53。
   3. 用逻辑回归替代softmax作为分类器。

#### （2）yolo的预测框是什么值:

**（x,y,w,h）**

#### （3）YOLOv2中如何通过K-Means得到anchor boxes？

由于卷积神经网络具有平移不变性，且anchor boxes的位置被每个栅格固定，因此我们只需要通过k-means计算出anchor boxes的**width**和**height**即可

#### （4）请叙述一下YOLOv3中k-means聚类获得anchor boxes过程详解

1. 将所有的bounding box坐标提取出来
2. 数据处理获得所有训练数据bounding boxes的宽高数据（长=右下角横坐标-左上角横坐标、宽=右下角纵坐标-左上角纵坐标）
3. 初始化k个anchor box（通过在所有的bounding boxes中随机选取k个值作为k个anchor boxes的初始值）。
4. 计算每个bounding box与每个anchor box的iou值，计算得到距离（传统的聚类方法是使用欧氏距离来衡量差异，YOLOv3作者发现传统方法在box尺寸比较大的时候，其误差也更大，改用iou距离）：
   $$
   d = 1 - iou
   $$
5. 分类操作。上一步得到每一个bounding box对于每个anchor box的误差$D_{n*k}$,对于第i个bounding box对于每个anchor box的误差为{d(i,1),d(i,2),...,d(i,k)}，选取误差最小的anchor box，分配给它。
6. anchor box更新。（经过上一步，我们就知道每一个anchor box都有哪些bounding box属于它，然后对于每个anchor box中的那些bounding box，我们再求这些bounding box的宽高的**中值大小**（这里参照github上作者qqwweee那个yolov3项目，也许也有使用**平均值**进行更新），将其作为该anchor box新的尺寸）
7. 重复操作第四步到第六步，直到在第五步中发现对于全部bounding box其所属的anchor box类与之前所属的anchor box类完全一样。（这里表示所有bounding box的分类已经不再更新）
8. 计算anchor boxes精确度。至第七步，其实已经通过k-means算法计算出anchor box。

#### （5）YOLO系列anchor的设计原理，kmeans的原理，anchor距离如何度量，如何改进k-means原理？

K-means算法是很典型的基于距离的聚类算法，采用距离作为相似性的评价指标，即认为两个对象的距离越近，其相似度就越大。该算法认为簇是由距离靠近的对象组成的，因此把得到紧凑且独立的簇作为最终目标。
由于从标记文件的width，height计算出的anchor boxes的width和height都是相对于整张图片的比例,特征金字塔上的长宽：

$$
w=\text { anchor宽 } * (\text { 输入图像宽 }/\text { downsamples } )\\
h=\text { anchor高 }* (\text { 输入图像高 }/\text { downsamples } )
$$

#### （6）YOLOv3框是怎么得到的？ YOLOv3有什么致命问题？<长虹AI lab-一面（2020）、阿里实习-一面（2018）、中兴-一面（2019）>

yolov3通过聚类的方式自定义anchor box的大小，在一定程度上，这可以提高定位的准确率。
**缺点：** 识别物体位置精准性差，召回率低（在每个网格中预测两个bbox这种约束方式减少了对同一目标的多次检测）

### 3. 其他检测网络

#### （1）简要阐述一下SSD网络，有哪些优点缺点？<商汤-一面（2019）、蘑菇街-一面（2018）>

SSD网络的特点是对不同尺度下的feature map中的每一个点都设置一些default box,这些default box有不同的大小和横纵比例，对这些default box进行分类和边框回归的操作。SSD的核心是对固定设置的default box（不同尺度feature map中每一个空间位置都设置一组default box，这里只考虑空间位置，不考虑feature的通道个数）计算属于各类物体的概率以及坐标调整的数值。这个计算方式是对每层的feature map做卷积操作，卷积核设定为3*3，卷积核的个数是与default box个数相关。

- **优点：** SSD的优点是运行速度超过yolo，精度在一定条件下超过faster rcnn。缺点是需要人工设置先验框（prior box）和min_size，max_size和长宽比（aspect_ratio）值，网络中default_box的基础大小和形状不能直接通过学习获得，而是需要手工设置，虽然使用了图像金字塔的思路，但是对小目标的recall（召回率）依然一般
- **SSD有什么致命缺点，如何改进？**<长虹AI lab-一面(2020)>
  
  SSD主要缺点：SSD对小目标的检测效果一般，作者认为小目标在高层没有足够的信息，
  对小目标检测的改进可以从下面几个方面考虑：
  
  1. 增大输入尺寸
  2. 使用更低的特征图做检测(比如S3FD中使用更低的conv3_3检测)
  3. 添加FPN(已经是检测网络的标配)

#### （2）简述SSD网络前向是如何计算的

1. 数据增强，获取训练样本，将训练数据统一resize到固定尺寸；
2. 通过卷积网络获取feature map：①使用的卷积网络，前半部分使用基础分类网络获取各层的feature map，这部分称为base network。②下一步计算的输入，就是上述的不同尺寸的feature map；
3. 通过卷积操作，从特征图中获取检测信息。①此处实现方式与yolo类似；②与Faster R-CNN类似，在特征图中每个点新建若干固定尺寸的anchor。检测信息包括每个anchor的信息。主要包括：confidence（代表这个anchor中是否存在物体）、分类信息以及bbox信息。

#### （3）简要阐述一下RetinaNet

RetinaNet的作者对one-stage检测器准确率不高的问题原因进行探究，发现主要问题在于正负类别不均衡，提出Focal Loss来解决类别不平衡问题。目的是通过减少易分类样本的权重，从而使得模型在训练时更注重于难分类的样本。
RetinaNet网络的结构可归纳为：

$$
RetinaNet = ResNet+FPN+Two sub-network(分类＋回归网络)+Focal Loss
$$

RetinaNet由backbone网络和两个子任务网络组成，backbone网络负责计算feature map，子任务网络一个负责目标分类，一个负责bbox回归，网络的loss使用Focal loss。
总的来说，RetinaNet 有两个大创新：

- Focal Loss
- RetinaNet 网络

#### （4）简单叙述一下RetinaNet中的FPN代码运行流程，P6、P7的作用是什么？

对代码运行流程描述为：

1. 将从骨干网络（ResNet）获得的 c3、c4 和 c5 三个特征图全部经过各自 1x1 卷积进行通道变换得到 m3~m5，输出通道统一为 256
2. 从 m5(特征图最小)开始，先进行 2 倍最近邻上采样，然后和 m4 进行 add 操作，得到新的 m4
3. 将新 m4 进行 2 倍最近邻上采样，然后和 m3 进行 add 操作，得到新的 m3
4. 对 m5 和新融合后的 m4、m3，都进行各自的 3x3 卷积，得到 3 个尺度的最终输出 P5～P3
5. 将 c5 进行 3x3 且 stride=2 的卷积操作，得到 P6
6. 将 P6 再一次进行 3x3 且 stride=2 的卷积操作，得到 P7

总结：FPN 模块接收 c3, c4, c5 三个特征图，输出 P2-P7 五个特征图，通道数都是 256, stride 为 (8,16,32,64,128)，其中大 stride (特征图小)用于检测大物体，小 stride (特征图大)用于检测小物体。

**P6 和 P7 目的:**
提供一个大感受野强语义的特征图，有利于大物体和超大物体检测。

#### （5） 简单介绍下cascade rcnn<字节跳动-三面（2019）>

Cascade R-CNN 主要针对 Faster R-CNN 中 R-CNN 部分 IoU 阈值选取对最终检测 bbox 质量有重大影响，而提出一种级联 R-CNN 结构，不同级采用不同 IoU 阈值来进行重新计算正负样本和采样策略来逐渐提高 bbox 质量。

#### （6）简要介绍一下FPN<百度实习-一面（2019）、字节跳动-二面（2019）、云从科技-二面(2019)>

<img src="images\FPN.png" style="zoom:75%;" />

FPN网络直接在Faster R-CNN单网络上做修改，每个分辨率的 feature map 引入后一分辨率缩放两倍的 feature map 做 element-wise 相加的操作。通过这样的连接，每一层预测所用的 feature map 都融合了不同分辨率、不同语义强度的特征，融合的不同分辨率的 feature map 分别做对应分辨率大小的物体检测。这样保证了每一层都有合适的分辨率以及强语义（rich semantic）特征。同时，由于此方法只是在原网络基础上加上了额外的跨层连接，在实际应用中几乎不增加额外的时间和计算量。

#### （7）FPN的特征融合为什么是相加操作呢？

假设两路输入来说，如果是通道数相同且后面带卷积的话，add等价于concat之后对应通道共享同一个卷积核。FPN里的金字塔，是希望把分辨率最小但语义最强的特征图增加分辨率，从性质上是可以用add的。如果用concat，因为分辨率小的特征通道数更多，计算量是一笔不小的开销。所以FPN里特征融合使用相加操作可以理解为是为了降低计算量。

#### （8）阐述一下FPN为什么能提升小目标的准确率?

低层的特征语义信息比较少，但是目标位置准确；高层的特征语义信息比较丰富，但是目标位置比较粗略。原来多数的object detection算法都是只采用顶层特征做预测。FPN同时利用低层特征高分辨率和高层特征的高语义信息，通过融合这些不同特征层的特征达到预测的效果。并且预测是在每个融合后的特征层上单独进行的。所以可以提升小目标的准确率。

#### （9）基于FPN的RPN是怎么训练的？

在FPN的每个预测层上都接一个RPN子网，确定RPN子网的正负anchor box样本，再计算各预测层上RPN的anchor box分类和回归损失，利用BP将梯度回传更新权值

### 4、综合

#### （1）阐述一下ssd和retinanet的区别

SSD的基础网络是VGG，且SSD在使用多层feature map时只是简单的在不同层的feature map上放default box，并没有真正将低维度特征和高维度特征进行融合。且SSD网络中使用的控制正负样本数量比的方法是难样本挖掘方法，loss是分类+回归的loss。而RetinaNet网络的基础网络是resnet+FPN，是真正将低维度的特征和高维度的特征进行了特征融合后再来做检测的。且控制正负样本的方法是使用Focal Loss。

#### （2）faster rcnn和yolo，ssd之间的区别和联系<上交苏研院-二面（2020）、云从科技-二面（2020）、旷视-一面（2018）、阿里实习-一面（2018）>

1. 针对之前RCNN系列selective search的方法导致算法没有实时性，所以faster rcnn提出RPN网络来取代之前的方法，可以理解为fasterrcnn=fast rcnn+rpn网络，且rpn网络和fast rcnn的分类，回归网络共用特征提取层，这样使得引入RPN网络不会增加太多计算量。整体流程为先使用RPN网络找出可能存在object的区域，再将这些区域送入fast rcnn中进一步定位和分类。所以faster rcnn是典型的Two stage算法。因为faster rcnn中包含了两次定位，所以其精度一般高于YOLO和SSD算法，所以速度一般慢于YOLO和SSD。
2. YOLO算法的特点是将检测问题转换成回归问题，即YOLO直接通过回归一次既产生坐标，又产生每种类别的概率。YOLO中将每张图分成7*7的网格，每个网格默认可能属于2个object，即在一张图片上提取98个region proposal，相比于faster rcnn使用Anchor机制提取20k个anchor再从中提取最终的300个region proposal，所以faster rcnn的精度比YOLO要高，但是由于需要处理更多region proposal，所以faster rcnn的速度要比YOLO慢。
3. SSD相比于faster rcnn使用了多层网络特征，而不仅仅使用最后一层feature map。SSD还借鉴了YOLO算法中将检测任务转换为回归任务的思想，且SSD也借鉴了faster rcnn中的anchor机制，只是SSD的anchor不是每个位置的精调，而是类似于YOLO那样在feature map上分割出网格，在网格上产生anchor。但是SSD和YOLO不需要selective search步骤，所以SSD和YOLO同属于One-Stage算法。

#### （3）分析一下SSD,YOLO,Faster rcnn等常用检测网络对小目标检测效果不好的原因

**SSD，YOLO**等单阶段多尺度算法，小目标检测需要较高的分辨率，SSD对于高分辨的低层特征没有再利用，而这些层对于检测小目标很重要。按SSD的设计思想，其实SSD对小目标应该有比较好的效果，但是需要重新精细设计SSD中的default box，比如重新设计min_sizes参数，扩大小default box的数量来cover住小目标。但是随着default box数量的增加，网络速度也会降低。YOLO网络可以理解为是强行把图片分割成7*7个网格，每个网格预测2个目标，相当于只有98个anchor，所以不管是小目标，还是大目标，YOLO的表现都不是很理想，但是由于只需处理少量的anchor，所以YOLO的速度上有很大优势。

**Faster rcnn**系列对小目标检测效果不好的原因是faster rcnn只用卷积网络的最后一层，但是卷积网络的最后一层往往feature map太小，导致之后的检测和回归无法满足要求。甚至一些小目标在最后的卷积层上直接没有特征点了。所以导致faster rcnn对小目标检测表现较差。

### 5. Loss

#### （1）请列举目标检测中常见的Loss（回归的，分类的）<海康-一面（2020）>

目标检测中的损失函数通常由两部分组成：classification loss 和 bounding box regression loss。

- 分类损失：
  1. 交叉熵损失函数（这里举例：softmax+交叉熵）
     对于二分类的交叉熵损失函数形式如下：

$$
L = -ylog(p)-(1-y)log(1-p) = \{\begin{array}{ll}log(p) & y=1 \\log(1-p) & y=0\end{array}.
$$

2. focal loss出于论文Focal Loss for Dense Object Detection，主要是为了解决one-stage目标检测算法中正负样本比例严重失衡的问题，降低了大量简单负样本在训练中所占的比重，可理解为是一种困难样本挖掘。focal loss是在交叉熵损失函数上修改的。具体改进：

$$
FL(p, y)=-\alpha y (1-p)^{\gamma}log(p)-(1-\alpha)(1-y)p^{\gamma}log(1-p)
\\= \{\begin{array}{ll}-\alpha (1-p)^{\gamma}log(p) & y=1 \\-(1-\alpha)p^{\gamma}log(1-p) & y=0\end{array}.
$$

ps: focal loss出来之后，不少loss基于focal loss延伸

- 回归损失：
  
  1. L1（MAE）
  
  $$
  L1 = |x|
  $$
  
  2. L2（MSE）
  
  $$
  L2 = x^2
  $$
  
  3. smooth L1损失函数（smooth L1损失函数是在Fast R-CNN中提出）
  
  $$
  \text { smooth }_{L_{1}}(x)=\{\begin{array}{ll}0.5 x^{2} & \text { if }|x|<1 \\|x|-0.5 & \text { otherwise }\end{array}.
  $$
  
  4. IoU Loss(iou计算见上)
     IoU loss是基于预测框和真实框之间的IoU（交并比）的，IoU定义如下：
  
  $$
  L_{iou} = 1 - IoU
  $$
  
  5. GIoU loss
     GIoU loss在IoU loss的基础上考虑了两个框没有重叠区域时产生的损失。具体定义如下：
  
  $$
  L_{giou} = 1 - IoU + R(P,G) = 1- IoU + \frac {|C - P \cup G|}{|C|}
  $$
  
  6. DIoU loss
     DIoU在IoU loss的基础上考虑了两个框的中心点距离，具体定义如下：
  
  $$
  L_{diou} = 1 - IoU + R(P,G) = 1- IoU + \frac {\rho(p,g)}{c^2}
  $$
  
  其中，ρ表示预测框和标注框中心端的距离，p和g是两个框的中心点。c表示两个框的最小包围矩形框的对角线长度。当两个框距离无限远时，中心点距离和外接矩形框对角线长度无限逼近，R→1
  
  7. CIoU loss
     DIoU loss考虑了两个框中心点的距离，而CIoU loss在DIoU loss的基础上做了更详细的度量，具体包括：
     - 重叠面积
     - 中心点距离
     - 长宽比
     - 具体定义如下：
  
  $$
  L_{diou} = 1 - IoU + R(P,G) = 1- IoU + \frac {\rho(p,g)}{c^2} + \alpha v
    \\ \text{其中:}， v = \frac{4}{\pi^2}(arctan\frac{w^g}{h^g}-arctan\frac{w^p}{h^p})^2 , \alpha = \frac{v}{(1 - Iou)+v}
  $$

#### （2）focal loss解决什么问题，如何写，每个参数有什么作用?<云从科技-二面（2019）>

Focal loss主要是为了解决**one-stage**目标检测中**正负样本比例严重失衡**的问题。该损失函数降低了大量简单负样本在训练中所占的权重，也可理解为一种困难样本挖掘

$$
FL(p, y)=- y (1-p)^{\gamma}log(p)-(1-y)p^{\gamma}log(1-p)
\\= \{\begin{array}{ll}- (1-p)^{\gamma}log(p) & y=1 \\-p^{\gamma}log(1-p) & y=0\end{array}
$$

添加参数γ，当γ大于0时，对于易分的正样本或负样本，权重小，而对于难区分的样本则权重大，避免让简单样本主导loss，γ越大，困难样本的权重越大

$$
FL(p, y)=-\alpha y (1-p)^{\gamma}log(p)-(1-\alpha)(1-y)p^{\gamma}log(1-p)
\\= \{\begin{array}{ll}-\alpha (1-p)^{\gamma}log(p) & y=1 \\-(1-\alpha)p^{\gamma}log(1-p) & y=0\end{array}
$$

添加参数α，用来平衡正负样本本身的比例不均

其中，文中参数是**α=0.25，γ=2**

#### （3） 训练过程中loss一致无法收敛，可能的原因<虹软-一面（2019）>

1. **数据和标签**
   数据分类标注是否准确？数据是否干净？数据库太小一般不会带来不收敛的问题，只要你一直在train总会收敛（rp问题跑飞了不算）。反而不收敛一般是由于样本的信息量太大导致网络不足以fit住整个样本空间。样本少只可能带来过拟合的问题
2. **学习率设定不合理**
   在自己训练新网络时，可以从0.1开始尝试，如果loss不下降的意思，那就降低，除以10，用0.01尝试，一般来说0.01会收敛，不行的话就用0.001. 学习率设置过大，很容易震荡。不过刚刚开始不建议把学习率设置过小，尤其是在训练的开始阶段。在开始阶段我们不能把学习率设置的太低否则loss不会收敛。我的做法是逐渐尝试，从0.1,0.08,0.06,0.05 ......逐渐减小直到正常为止，
3. **网络设定不合理**
   如果做很复杂的分类任务，却只用了很浅的网络，可能会导致训练难以收敛，换网络换网络换网络，重要的事情说三遍，或者也可以尝试加深当前网络。
4. **数据集label的设置**
   检查lable是否有错，有的时候图像类别的label设置成1，2，3正确设置应该为0,1,2。
5. **改变图片大小**
   改变图片大小有时可以解决收敛问题
6. **数据归一化**
   神经网络中对数据进行归一化是不可忽略的步骤，网络能不能正常工作，还得看你有没有做归一化，一般来讲，归一化就是减去数据平均值除以标准差，通常是针对每个输入和输出特征进行归一化

### 6、基础知识

#### （1）手撕IOU<华为-一面（2020）、作业帮-一面（2019）、腾讯-一面(2018)>

<img src="images\IOU.png" style="zoom:60%;" />

计算IOU非常简单，有两种方法：

1. 两个矩形的宽之和减去组合后的矩形的宽就是重叠矩形的宽，重叠矩形的高计算同宽
2. 右下角的$min(x)$减去左上角的$max(x)$就是重叠矩形的宽，重叠矩形的高计算同宽

然后 IOU = 重叠面积 / （两矩形面积之和 —  重叠面积）

**参考代码（第二种方法）：**

```python
# IOU计算
    # 假设box1维度为[N,4]   box2维度为[M,4]
    def iou(self, box1, box2):
        N = box1.size(0)
        M = box2.size(0)

        lt = torch.max(  # 左上角的点
            box1[:, :2].unsqueeze(1).expand(N, M, 2),   # [N,2]->[N,1,2]->[N,M,2]
            box2[:, :2].unsqueeze(0).expand(N, M, 2),   # [M,2]->[1,M,2]->[N,M,2]
        )

        rb = torch.min(
            box1[:, 2:].unsqueeze(1).expand(N, M, 2),
            box2[:, 2:].unsqueeze(0).expand(N, M, 2),
        )

        wh = rb - lt  # [N,M,2]
        wh[wh < 0] = 0   # 两个box没有重叠区域
        inter = wh[:,:,0] * wh[:,:,1]   # [N,M]

        area1 = (box1[:,2]-box1[:,0]) * (box1[:,3]-box1[:,1])  # (N,)
        area2 = (box2[:,2]-box2[:,0]) * (box2[:,3]-box2[:,1])  # (M,)
        area1 = area1.unsqueeze(1).expand(N,M)  # (N,M)
        area2 = area2.unsqueeze(0).expand(N,M)  # (N,M)

        iou = inter / (area1+area2-inter)
        return iou
```

#### （2）anchor设置的意义

其实就是多尺度的滑动窗口

#### （3）如何理解concat和add这两种常见的feature map特征融合方式

两者都可以理解为整合特征图信息。**concat**是通道数的增加;**add**是特征图相加，通道数不变。
add是描述图像的特征下的信息量增多了，但是描述图像的维度本身并没有增加，只是每一维下的信息量在增加，这显然是对最终的图像的分类是有益的。而concat是通道数的合并，也就是说描述图像本身的特征数（通道数）增加了，而每一特征下的信息是没有增加。
concat每个通道对应着对应的卷积核。 而add形式则将对应的特征图相加，再进行下一步卷积操作，相当于加了一个先验：对应通道的特征图语义类似，从而对应的特征图共享一个卷积核（对于两路输入来说，如果是通道数相同且后面带卷积的话，add等价于concat之后对应通道共享同一个卷积核）。因此add可以认为是特殊的concat形式。但是add的计算量要比concat的计算量小得多。

#### （4）介绍一下目标检测的主要评测指标

- 目标检测问题，一般的常用评价指标有：

1. map（平均准确度均值，精度评价）
2. 速度指标:FPS, (即每秒处理的图片数量或者处理每张图片所需的时间，当然必须在同一硬件条件下进行比较）

- **mAP定义及相关概念**

1. **map计算方法**
   首先计算某一类别的AP值。不同数据集的某类别的AP计算方法大同小异，主要分为三种：
   （1）在VOC2010以前，只需要选取当Recall >= 0, 0.1, 0.2, ..., 1共11个点时的Precision最大值，然后AP就是这11个Precision的平均值，map就是所有类别AP值的平均。
   （2）在VOC2010及以后，需要针对每一个不同的Recall值（包括0和1），选取其大于等于这些Recall值时的Precision最大值，然后计算PR曲线下面积作为AP值，map就是所有类别AP值的平均。
   （3）COCO数据集，设定多个IOU阈值（0.5-0.95,0.05为步长），在每一个IOU阈值下都有某一类别的AP值，然后求不同IOU阈值下的AP平均，就是所求的最终的某类别的AP值。
   **最后，mAP值即：所有类的AP值平均值**。
2. **速度指标**
   一般来说目标检测中的速度评价指标有：
   （1）FPS，检测器每秒能处理图片的张数
   （2）检测器处理每张图片所需要的时间

#### （5） FLOPs计算

- FLOPs的定义：
  **FLOPS(全大写)：** 是floating point operations per second的缩写，意指每秒浮点运算次数，理解为计算速度，是一个衡量硬件性能的指标。
  **FLOPs(s小写)：** 是floating point operations的缩写（s表复数），意指浮点运算数，理解为计算量，可以用来衡量算法/模型的复杂度
  同一硬件情况下，它的最大FLOPS是相同的，不同网络，处理每张图片所需的FLOPs(浮点运算次数)是不同的，所以同一硬件处理相同图片所需的FLOPs越小，相同时间内，就能处理更多的图片，速度也就越快。
  
  处理每张图片所需的FLOPs与许多因素有关，比如你的网络层数，参数量，选用的激活函数等等，这里仅谈一下网络的参数量对其的影响，一般来说参数量越低的网络，FLOPs会越小，保存模型所需的内存小，对硬件内存要求比较低，因此比较对嵌入式端较友好。
- FLOPs计算（以下计算FLOPs不考虑激活函数的运算）
  
  1. 标准卷积层的FLOPs
     $FLOPs=(2*C_{in}*k^2 - 1）*H*W*C_{out}$（不考虑bias）
     $FLOPs=(2*C_{in}*k^2）*H*W*C_{out}$（考虑bias）
     *$C_{in}$为输入特征图通道数，$K$为卷积核边长，$H,W,C_{out}$分别为输出特征图的高，宽和通道数。*
  2. 深度可分离卷积的FLOPs
     深度可分离卷积分成两部分，一部分是分通道卷积，另一部分是1*1卷积。（可大大减少计算量的卷积方法）
     讨论以考虑bias为准：
     
     - 第一部分： $FLOPs_1=(2*k^2）*H*W*C_{in}$
     - 第二部分： $FLOPs_2=(2*C_{in}）*H*W*C_{out}$
     
     最终结果： $FLOPs = FLOPs_1 + FLOPs_2$
  3. 池化层的FLOPS
     这里又分为全局池化和一般池化两种情况：(注意：池化层的$C_{in}=C_{out}$)
     **全局池化:**
     针对输入所有值进行一次池化操作，不论是max、sum还是avg，都可以简单地看做是只需要对每个值算一次。
     所以结果为:$FLOPs = H_{in}*W_{in}*C_{in}$
     **一般池化：**
     结果为$FLOPs=(k^2）*H_{out}*W_{out}*C_{out}$
  4. 全连接层的FLOPs
     考虑bias：$(2*C_{in})*C_{out}$
     不考虑bias：$(2*C_{in} - 1)*C_{out}$
  5. 激活层的FLOPs
     **ReLU:**
     ReLU一般都是跟在卷积层的后面，这里假设卷积层的输出为$H_{out}*W_{out}*C_{out}$因为ReLU函数的计算只涉及到一个判断，因此计算量就是:$H_{out}*W_{out}*C_{out}$
     **Sigmoid**
     根据sigmoid的公式可以知道，每个输入都需要经历4次运算，因此计算量是:$H_{out}*W_{out}*C_{out}*4$

#### （6） 参数量paras计算 （*参数与FLOPs中一致*）

1. 标准卷积层的参数量
   不考虑bias: $（C_{in}*k^2）*C_{out}$
   考虑bias:  $（C_{in}*k^2+1）*C_{out}$
2. 深度可分离卷积的参数量
   深度可分离卷积分成两部分，一部分是分通道卷积，另一部分是1*1卷积。（可大大减少计算量的卷积方法）
   这里不考虑bias为准：
   
   - 第一部分： $参数量_1=k^2*C_{in}$
   - 第二部分： $参数量_2=(1*1*C_{in}）*C_{out}$
     最终参数量 = 参数量1 + 参数量2
3. 池化层的参数量
   池化层没有需要学习的参数，所以参数量为**0**。
4. 全连接层的参数量
   考虑bias：$C_{in}*C_{out} + 1$

#### （7）简单介绍一下kmeans算法<小米-一面（2020）、腾讯-一面(2018)>

**简介：** k-means算法是一种聚类算法，所谓聚类，即根据相似性原则，将具有较高相似度的数据对象划分至同一类簇，将具有较高相异度的数据对象划分至不同类簇。聚类与分类最大的区别在于，聚类过程为无监督过程，即待处理数据对象没有任何先验知识，而分类过程为有监督过程，即存在有先验知识的训练数据集。
**K-means算法原理:** K-means算法中的k代表类簇个数，means代表类簇内数据对象的均值（这种均值是一种对类簇中心的描述），因此，k-means算法又称为k-均值算法。k-means算法是一种基于划分的聚类算法，以距离作为数据对象间相似性度量的标准，即数据对象间的距离越小，则它们的相似性越高，则它们越有可能在同一个类簇。数据对象间距离的计算有很多种，k-means算法通常采用欧氏距离来计算数据对象间的距离。算法详细的流程描述如下：
![](images/kmeans算法.jpg)

**k-means算法优缺点分析:**

- 优点： 算法简单易实现；
- 缺点： 需要用户事先指定类簇个数； 聚类结果对初始类簇中心的选取较为敏感； 容易陷入局部最优； 只能发现球形类簇；

#### （8）说一下非极大值抑制（NMS）（non maximum suppression） NMS实现细节，手撕NMS<小米-二面（2020）、<百度实习-一面（2019）、商汤-一面（2019）、腾讯-一面(2018)>

- **用处:** 本质是搜索局部极大值，抑制非极大值元素。
- **原理:** NMS为非极大值抑制，用来抑制检测时冗余的框。
- **大致算法流程为：**
  1.对所有预测框的置信度降序排序
  2.选出置信度最高的预测框，确认其为正确预测，并计算他与其他预测框的IOU
  3.根据2中计算的IOU去除重叠度高的，IOU>threshold阈值就删除
  4.剩下的预测框返回第1步，直到没有剩下的为止
  
  （*需要注意的是：Non-Maximum Suppression一次处理一个类别，如果有N个类别，Non-Maximum Suppression就需要执行N次*）
- **假设两个目标靠的很近，则会识别成一个bbox，会有什么问题，怎么解决？**
  
  当两个目标靠的非常近时，置信度低的会被置信度高的框抑制掉，从而两个目标靠的非常近时会被识别成一个bbox。为了解决这个问题，可以使用soft-NMS（基本思想：用稍低一点的分数来代替原有的分数，而不是直接置零）
  
  **参考代码：**
  
  - NMS算法
  
  ```python
  # bboxes维度为[N,4]，scores维度为[N,], 均为tensor
    def nms(self, bboxes, scores, threshold=0.5):
        x1 = bboxes[:,0]
        y1 = bboxes[:,1]
        x2 = bboxes[:,2]
        y2 = bboxes[:,3]
        areas = (x2-x1)*(y2-y1)   # [N,] 每个bbox的面积
        _, order = scores.sort(0, descending=True)    # 降序排列
  
        keep = []
        while order.numel() > 0:       # torch.numel()返回张量元素个数
            if order.numel() == 1:     # 保留框只剩一个
                i = order.item()
                keep.append(i)
                break
            else:
                i = order[0].item()    # 保留scores最大的那个框box[i]
                keep.append(i)
  
            # 计算box[i]与其余各框的IOU(思路很好)
            xx1 = x1[order[1:]].clamp(min=x1[i])   # [N-1,]
            yy1 = y1[order[1:]].clamp(min=y1[i])
            xx2 = x2[order[1:]].clamp(max=x2[i])
            yy2 = y2[order[1:]].clamp(max=y2[i])
            inter = (xx2-xx1).clamp(min=0) * (yy2-yy1).clamp(min=0)   # [N-1,]
  
            iou = inter / (areas[i]+areas[order[1:]]-inter)  # [N-1,]
            idx = (iou <= threshold).nonzero().squeeze() # 注意此时idx为[N-1,] 而order为[N,]
            if idx.numel() == 0:
                break
            order = order[idx+1]  # 修补索引之间的差值
        return torch.LongTensor(keep)   # Pytorch的索引值为LongTensor
  ```

**拓展：手撕soft-nms**

### 7、延伸问题

#### （1）讲一下目标检测优化的方向<百度实习-一面（2019）>

1. 可以从数据集下手，提升特征表征强度，
2. 从backbone下手，加深、加宽、换卷积方式、添加注意力机制等，最近transformer在cv领域有了很好的进展，例如：swin-transformer，
3. 从RPN下手（级联，FPN，IOU NET），
4. LOSS（行人检测领域有些问题，如重叠，可以靠修改loss提升准确度），
5. FPN（例如：改进特征融合方式）

#### （2）如果只能修改RPN网络的话，怎么修改可以提升网络小目标检出率

1. 修改RPN网络的结构，比如引入FPN结构，利用多层feature map融合来提高小目标检测的精度和召回
2. 针对小目标重新精细设计Anchor的尺寸和形状，从而更好地对小目标进行检测

#### （3）阐述一下如何检测小物体

**小目标难以检测的原因：** 分辨率低，图像模糊，携带的信息少。

1. 借鉴FPN的思想，在FPN之前目标检测的大多数方法都是和分类一样，使用顶层的特征来进行处理。虽然这种方法只是用到了高层的语义信息，但是位置信息却没有得到，尤其在检测目标的过程中，位置信息是特别重要的，而位置信息又是主要在网络的低层。因此FPN采用了多尺度特征融合的方式，采用不同特征层特征融合之后的结果来做预测。
2. 要让输入的分布尽可能地接近模型预训练的分布。先用ImageNet做预训练，之后使用原图上采样得到的图像来做微调，使用微调的模型来预测原图经过上采样的图像。该方法提升效果比较显著。
3. 采用多尺度输入训练方式来训练网络；
4. 借鉴Cascade R-CNN的设计思路，优化目标检测中Two-Stage方法中的IOU阈值。检测中的IOU阈值对于样本的选取是至关重要的，如果IOU阈值过高，会导致正样本质量很高，但是数量会很少，会出现样本比例不平衡的影响；如果IOU阈值较低，样本数量就会增加，但是样本的质量也会下降。如何选取好的IOU，对于检测结果来说很重要。
5. 采用分割代替检测方法，先分割，后回归bbox来检测微小目标。

#### （4）阐述一下目标检测任务中的多尺度

输入图片的尺寸对检测模型的性能影响相当明显，事实上，多尺度是提升精度最明显的技巧之一。在基础网络部分常常会生成比原图小数十倍的特征图，导致小物体的特征描述不容易被检测网络捕捉。通过输入更大、更多尺寸的图片进行训练，能够在一定程度上提高检测模型对物体大小的鲁棒性，仅在测试阶段引入多尺度，也可享受大尺寸和多尺寸带来的增益。

检测网络SSD中最后一层是由多个尺度的feature map一起组成的。FPN网络中采用多尺度feature map分层融合，分层预测的方法可以提升小目标的检测效果。

- **阐述一下如何进行多尺度训练**
  **多尺度训练可以分为两个方面:** 1. 图像金字塔 2. 特征金字塔

**图像金字塔：** 人脸检测的MTCNN就是图像金字塔，使用多种分辨率的图像送到网络中识别，时间复杂度高，因为每幅图都要用多种scale去检测。
**特征金字塔：** FPN网络属于采用了特征金字塔的网络，一次特征提取产生多个feature map即一次图像输入完成，所以时间复杂度并不会增加多少

**anchors：** faster rcnn多个anchor带来的多种尺寸的roi可以算muti scale思想的应用。

#### （5）如果有很长，很小，或者很宽的目标，应该如何处理目标检测中如何解决目标尺度大小不一的情况  小目标不好检测，有试过其他的方法吗？比如裁剪图像进行重叠

**小目标不好检测的两大原因：**

1. 数据集中包含小目标的图片比较少，导致模型在训练的时候会偏向medium和large的目标。
2. 小目标的面积太小了，导致包含目标的anchor比较少，这也意味着小目标被检测出的概率变小。

**改进方法：**

1. 对于数据集中含有小目标图片较少的情况，使用过度采样（oversample）的方式，即多次训练这类样本。
2. 对于第二类问题，则是对于那些包含小物体的图像，将小物体在图片中复制多分，在保证不影响其他物体的基础上，人工增加小物体在图片中出现的次数，提升被anchor包含的概率。
3. 使用FPN（低层检测小目标）；
4. RPN中anchor size的设置一定要合适，这样可提高proposal的准确率。
5. 对于分辨率很低的小目标，我们可以对其所在的proposal进行超分辨率，提升小目标的特征质量，更有利于小目标的检测。

#### （6）检测的框角度偏移了45度，这种情况怎么处理?<海康-一面（2020）>

RRPN也是基于Faster R-CNN，引入RPN，它对比CTPN加入了旋转信息(即：通过生成更加密集的旋转Anchor 来适应旋转框检测的任务)。CTPN只能检测水平文本，而RRPN可以检测任意方向的文本，因为CTPN的提议框是水平的，而RRPN的提议框带有旋转角度。

**为什么提出旋转的提议框呢？**
因为水平提议框在检测倾斜文本的时候会带有一些冗余（非文本部分）

### 8、思考一下

#### （1）讲一下two-stage和one-stage的异同<百度实习-一面（2019）>

#### （2）讲一下Faster RCNN的两阶段训练和END TO END训练的不一样？<百度实习-一面（2019）>

#### （3）讲一下你所知道的插值方式<百度实习-一面（2019）>

#### （4）一阶段SSD，YOLO之间的区别是什么？<Tencent AI lab-一面（2020）>

#### （5）R-CNN系列和SSD本质有啥不一样吗？<Tencent AI lab-一面（2020）>

#### （6）讲一下目标检测的发展历程，从传统到深度<百度实习-一面（2019）>

#### （7）近年来，一些新的目标检测的backbone有哪些，各有什么特点？<小米-一面（2020）>

#### （8） 为选择用YOLOv3，这个网络真的好吗？<长虹AI lab-一面（2020）>

## 三、参考链接

> 1.https://mp.weixin.qq.com/s/b40PdGB7o95jmPMEIICBUQ
> 
> 2.https://zhuanlan.zhihu.com/p/349807581
> 
> 3.https://zhuanlan.zhihu.com/p/7030601
> 
> 4.https://zhuanlan.zhihu.com/p/346198300
> 
> 5.https://blog.csdn.net/gaoyi135/article/details/105912701
> 
> 6.https://zhuanlan.zhihu.com/p/51680715
> 
> 7.https://www.nowcoder.com/discuss/515113
> 
> 8.https://www.nowcoder.com/discuss/181413
> 
> 9.https://zhuanlan.zhihu.com/p/89450552
> 
> 10.https://blog.csdn.net/liuxiao214/article/details/83043197
> 
> 11.https://www.nowcoder.com/discuss/267852
> 
> 12.https://blog.csdn.net/qq_33638791/article/details/79802934
> 
> 13.https://www.nowcoder.com/discuss/245668
> 
> 13.https://www.nowcoder.com/discuss/250532
> 
> 14.https://www.nowcoder.com/discuss/238556
> 
> 15.https://blog.csdn.net/qq_42109740/article/details/105740704
> 
> 16.https://zhuanlan.zhihu.com/p/360952172
> 
> 17.https://zhuanlan.zhihu.com/p/346198300
> 
> 18.https://zhuanlan.zhihu.com/p/54709759
> 
> 19.https://blog.csdn.net/weixin_43750248/article/details/116656242
> 
> 20.https://blog.csdn.net/qq_24502469/article/details/105121529
> 
> 21.https://baijiahao.baidu.com/s?id=1622412414004300046&wfr=spider&for=pc

---

# 文件：AI算法\machine-learning\ABTest.md

---

# AB测试面试题

## 1. 介绍一下ABTest的步骤

ABtest就是为了测试和验证模型/项目的效果，在app/pc端设计出多个版本，在同一时间维度下，分别用组成相同/相似的群组去随机访问这些版本，记录下群组的用户体验数据和业务数据，最后评估出最好的版本给予采用。

步骤：

```python
1. 基于现状和期望，分析并提出假设
2. 设定目标制定方案
3. 设计与开发
4. 分配流量进行测试
5. 埋点采集数据
6. 实验后分析数据
7. 发布新版本/改进设计方案/调整流量继续测试
```

## 2. ABtest背后的理论支撑是什么？

**中心极限定理**：在样本量足够大的时候，可以认为样本的均值近似服从正态分布。

**假设检验**：假设检验是研究如何根据抽样后获得的样本来检查抽样前所作假设是否合理，**A/B Test 从本质上来说是一个基于统计的假设检验过程**，它首先对实验组和对照组的关系提出了某种假设，然后计算这两组数据的差异和确定该差异是否存在统计上的显著性，最后根据上述结果对假设做出判断。

假设检验的核心是**证伪**，所以原假设是统计者想要拒绝的假设，无显著差异我们也可以理解为：实验组和对照组的统计差异是由抽样误差引起的（误差服从正态分布）。

## 3. 如何分组才能更好地避免混淆呢？

1. 利用用户的唯一标识的尾号或者其他标识进行分类，如按照尾号的奇数或者偶数将其分为两组。
2. 用一个hash函数将用户的唯一标识进行hash取模，分桶。可以将用户均匀地分到若干个桶中，如分到100个或者1000个桶中，这样的好处就是可以进一步将用户打散，提高分组的效果。

当然，如果有多个分组并行进行的情况的话，要考虑独占域和分享域问题。（不同域之间的用户相互独立，交集为空）对于共享域，我们要进行分层。但是在分层中，下一层要将上一层的用户打散，确保下一层用户的随机性。

## 4. 样本量大小如何？

```
理论上，我们想要样本量越多的越好，因为这样可以避免第二类错误。随着样本量增加，power=1-β也在增大，一般到80%，这里我们可以算出一个最小样本量，但理论上样本量还是越大越好。
```

实际上，样本量越少越好，这是因为

1. 流量有限：小公司就这么点流量，还要精打细算做各种测试，开发各种产品。在保证样本分组不重叠的基础上，产品开发速度会大大降低。
2. 试错成本大：如果拿50%的用户做实验，一周以后发现总收入下降了20%，这样一周时间的实验给公司造成了10%的损失，这样损失未免有点大。

## 5. 两类错误是什么？

1. **弃真**：实验组和对照组没有显著差异，但我们接受了方案推了全量。减少这种错误的方法就是提高显著性水平，比如 p 值小于 0.05 才算显著，而不是小于 0.1，显著性水平是人为给定的犯一类错误的可以接受的上限（$p$值为犯 I 类错误的概率$\alpha$ ）。
2. **存伪**：实验组和对照组有显著差异，但我们没有接受方案。
   
   II 类错误和**统计功效 (power)** 有关，统计功效可以简单理解为真理能被发现的可能性。统计功效 为:$1-\beta$ ，而$\beta$为犯第二类错误的概率。影响统计功效的因素有很多，主要的有三个：统计量、样本量和 I 类错误的概率$\alpha$  。

## 6. 埋点&暗中观察

```
当我们确定了需要分析的具体指标之后，就需要我们进行埋点设计，把相关的用户行为收集起来，供后续的流程进行数据分析，从而得出实验结论。
```

```
对于 ABTest我们需要知道当前用户是处于对照组还是实验组，所以埋点中这些参数必须要有。埋点完了就是收集实验数据了（暗中观察），主要看以下两个方面：
```

1. 观察样本量是否符合预期，比如实验组和对照组分流的流量是否均匀，正常情况下，分流的数据不会相差太大，如果相差太大，就要分析哪里出现了问题。
2. 观察用户的行为埋点是否埋的正确，很多次实验之后，我们发现埋点埋错了。

## 7. 如果一个人有多个账号，分别做不同用途，abtest的时候怎么分组才最合理呢？

我们对这类人的分类是，看的不是他是谁，而是他做了什么。按照我们对行业的分类，行为不同的话就是两类人，和身份证是不是同一个无关。我们要聚合的是有相同行为特征的账户，而不是人。

## 参考资料

https://zhuanlan.zhihu.com/p/165406531

---

# 文件：AI算法\machine-learning\Adaboost.md

---

![Adboost框架图](img/Adaboost/Adboost框架图.png)

# Adaboost 算法介绍

## 1. 集成学习

集成学习（ensemble learning）通过构建并结合多个学习器（learner）来完成学习任务，通常可获得比单一学习器更良好的泛化性能（特别是在集成弱学习器（weak learner）时）。

目前集成学习主要分为2大类：

一类是以bagging、Random Forest等算法为代表的，**各个学习器之间相互独立、可同时生成**的并行化方法；

一类是以boosting、Adaboost等算法为代表的，**个体学习器是串行序列化生成的、具有依赖关系**，它试图不断增强单个学习器的学习能力。

## 2. Adaboost 算法详解

### 2.1 Adaboost 步骤概览

1. 初始化训练样本的权值分布，每个训练样本的权值应该相等（如果一共有$N$个样本，则每个样本的权值为$\frac{1}{N}$)
2. 依次构造训练集并训练弱分类器。如果一个样本被准确分类，那么它的权值在下一个训练集中就会降低；相反，如果它被分类错误，那么它在下个训练集中的权值就会提高。权值更新过后的训练集会用于训练下一个分类器。
3. 将训练好的弱分类器集成为一个强分类器，误差率小的弱分类器会在最终的强分类器里占据更大的权重，否则较小。

### 2.2 Adaboost 算法流程

给定一个样本数量为$m$的数据集

$$
T= \left \{\left(x_{1}, y_{1}\right), \ldots,\left(x_{m}, y_{m}\right)  \right \}
$$

$y_i$ 属于标记集合$\{-1,+1\}$。

训练集的在第$k$个弱学习器的输出权重为

$$
D(k)=\left(w_{k 1}, w_{k 2}, \ldots w_{k m}\right) ; \quad w_{1 i}=\frac{1}{m} ; i=1,2 \ldots m
$$

- 初始化训练样本的权值分布，每个训练样本的权值相同：

$$
D(1)=\left(w_{1 1}, w_{1 2}, \ldots w_{1 m}\right) ; \quad w_{1 i}=\frac{1}{m} ; i=1,2 \ldots m
$$

- 进行多轮迭代，产生$T$个弱分类器。
  - 使用权值分布 $D(t) $的训练集进行训练，得到一个弱分类器

$$
G_{t}(x) : \quad \chi \rightarrow\{-1,+1\}
$$

- 计算 $G_t(x)$ 在训练数据集上的分类误差率（其实就是被 $G_t(x) $误分类样本的权值之和）:

$$
e_{t}=P\left(G_{t}\left(x_{i}\right) \neq y_{i}\right)=\sum_{i=1}^{m} w_{t i} I\left(G_{t}\left(x_{i}\right) \neq y_{i}\right)
$$

- 计算弱分类器 Gt(x) 在最终分类器中的系数(即所占权重)

$$
\alpha_{t}=\frac{1}{2} \ln \frac{1-e_{t}}{e_{t}}
$$

- 更新训练数据集的权值分布，用于下一轮（t+1）迭代

$$
D(t+1)=\left(w_{t+1,1} ,w_{t+1,2} ,\cdots w_{t+1, i} \cdots, w_{t+1, m}\right)
$$

$$
w_{t+1,i}=\frac{w_{t,i}}{Z_{t}} \times \left\{\begin{array}{ll}{e^{-\alpha_{t}}} & {\text （{ if } G_{t}\left(x_{i}\right)=y_{i}}） \\ {e^{\alpha_{t}}} & {\text （{ if } G_{t}\left(x_{i}\right) \neq y_{i}}）\end{array}\right.= \frac{w_{t,i}}{Z_{t}} \exp \left(-\alpha_{t} y_{i} G_{t}\left(x_{i}\right)\right)
$$

```
其中
```

$Z_t$是规范化因子，使得$D(t+1)$成为一个概率分布（和为1）：

$$
Z_{t}=\sum_{j=1}^{m} w_{t,i} \exp \left(-\alpha_{t} y_{i} G_{t}\left(x_{i}\right)\right)
$$

* 集成$T$个弱分类器为1个最终的强分类器：

$$
G(x)=\operatorname{sign}\left(\sum_{t=1}^{T} \alpha_{t} G_{t}(x)\right)
$$

## 3. 算法面试题

### 3.1 Adaboost分类模型的学习器的权重系数$\alpha$怎么计算的？

Adaboost是前向分步加法算法的特例，分类问题的时候认为损失函数指数函数。

1. 当基函数是分类器时，Adaboost的最终分类器是：
   
   $$
   f(x)=\sum_{m-1}^{M}{\alpha_mG_m(x)}=f_{m-1}(x)+{\alpha_mG_m(x)}
   $$
2. 目标是使前向分步算法得到的$\alpha$和$G_m(x)$使$f_m(x)$在训练数据集T上的指数损失函数最小，即
   
   $$
   (\alpha, G_m(x))=arg min_{\alpha, G}\sum_{i=1}^{N}exp[-y_i(f_{m-1}(x_i)+\alpha G(x_i))]
   $$
   
   其中，$\hat{w}_{mi}=exp[-y_i f_{m-1}(x_i)].$为了求上式的最小化，首先计算$G_m^*(x)$,对于任意的$\alpha >0$,可以转化为下式：
   
   $$
   G_{m}^*=argmin_{G}\sum_{i=1}^{N}\hat{w}_{mi}I(y_i \neq G(x_i))
   $$
   
   之后求$\alpha_m^*$,将上述式子化简，得到

$$
\sum_{i=1}^{N}\hat{w}_{mi}exp[-y_i \alpha G(x_i)]
= \sum_{y_i =G_m(x_i)}\hat{w}_{mi}e^{-\alpha}+\sum_{y_i \neq G_m(x_i)}{\hat{w}_{mi}e^{\alpha}} = (e^{\alpha} - e^{- \alpha})\sum_{i=1}^{N}\hat{w}_{mi}I(y_i \neq G(x_i)) + e^{- \alpha}\sum_{i=1}^{N}\hat{w}_{mi}
$$

将已经求得的$G_m^*(x)$带入上式面，对$\alpha$求导并等于0，得到最优的$\alpha$.

$$
a_m^*=\frac{1}{2} log{\frac{1-e_m}{e_m}}
$$

其中$e_m$是分类误差率:

$$
e_m=\frac{\sum_{i=1}^{N}\hat{w}_{mi}I(y_i \neq G_m(x_i))}{\sum_{i=1}^{N}\hat{w}_{mi}}=\sum_{i=1}^{N}\hat{w}_{mi}I(y_i \neq G_m(x_i))
$$

### 3.2 Adaboost能否做回归问题？

Adaboost也能够应用到回归问题，相应的算法如下:
输入: $T={(x_i, y_1),(x_i, y_1),...,(x_N, y_N)}$, 弱学习器迭代次数$M$。
输出：强分类器$f(x)$.

1. 初始化权重，
   
   $$
   D(1)={w_{11},w_{12},...,w_{1N}}; w_{1i}=\frac{1}{N}; i=1,2,..,N
   $$
2. 根据$m=1,2,...,M$;
   
   + 学习得到$G_m(x)$
   + 计算训练集上最大误差
     
     $$
     E_m=max|y_i-G_m(x_i)|, i=1,2,..,N
     $$
   + 计算样本的相对平方误差:
     
     $$
     e_{mi}=\frac{(y_i-G_m(x_i))^2}{E_m^2}
     $$
   + 计算回归误差率:
     
     $$
     e_m=\sum_{i=1}^{N}w_{mi}e_{mi}
     $$
   + 计算学习器系数:
     
     $$
     \alpha_m=\frac{e_m}{1-e_m}
     $$
   + 更新样本权重：
     
     $$
     w_{m+1,i}=\frac{w_{mi}}{Z_m}{\alpha_{m}^{1-e^{m,i}}}
     $$
     
     其中$Z_m$是规范化因子，
     
     $$
     Z_m=\sum_{i=1}^{m}w_{mi}{\alpha_{m}^{1-e^{m,i}}}
     $$
3. 得到强学习器：
   
   $$
   f(x)=\sum_{m=1}{M}G_{m}^*(x)
   $$

**注:** **不管是分类问题还是回归问题，根据误差改变权重就是Adaboost的本质，可以基于这个构建相应的强学习器。**

### 3.3 boosting和bagging之间的区别,从偏差-方差的角度解释Adaboost？

集成学习提高学习精度，降低模型误差，模型的误差来自于方差和偏差，其中bagging方式是降低模型方差，一般选择多个相差较大的模型进行bagging。boosting是主要是通过降低模型的偏差来降低模型的误差。其中Adaboost每一轮通过误差来改变数据的分布，使偏差减小。

### 3.4 为什么Adaboost方式能够提高整体模型的学习精度？

根据前向分布加法模型，Adaboost算法每一次都会降低整体的误差，虽然单个模型误差会有波动，但是整体的误差却在降低，整体模型复杂度在提高。

### 3.5 Adaboost算法如何加入正则项?

$$
f_m(x)=f_{m-1}(x)+\eta \alpha_{m}G_{m}(x)
$$

### 3.6 Adaboost使用m个基学习器和加权平均使用m个学习器之间有什么不同？

Adaboost的m个基学习器是有顺序关系的，第k个基学习器根据前k-1个学习器得到的误差更新数据分布，再进行学习，每一次的数据分布都不同，是使用同一个学习器在不同的数据分布上进行学习。加权平均的m个学习器是可以并行处理的，在同一个数据分布上，学习得到m个不同的学习器进行加权。

### 3.7 Adaboost和GBDT之间的区别？

相同点：

```
Adaboost和GBDT都是通过减低偏差提高模型精度，都是前项分布加法模型的一种，
```

不同点:

```
Adaboost每一个根据前m-1个模型的误差更新当前数据集的权重，学习第m个学习器；
```

```
GBDT是根据前m-1个的学习剩下的label的偏差，修改当前数据的label进行学习第m个学习器，一般使用梯度的负方向替代偏差进行计算。
```

### 3.8 Adaboost的迭代次数(基学习器的个数)如何控制？

一般使用earlystopping进行控制迭代次数。

### 3.9 Adaboost算法中基学习器是否很重要，应该怎么选择基学习器？

sklearn中的adaboost接口给出的是使用决策树作为基分类器，一般认为决策树表现良好，其实可以根据数据的分布选择对应的分类器，比如选择简单的逻辑回归，或者对于回归问题选择线性回归。

### 3.10 MultiBoosting算法将Adaboost作为Bagging的基学习器，Iterative Bagging将Bagging作为Adaboost的基学习器。比较两者的优缺点？

两个模型都是降低方差和偏差。主要的不同的是顺序不同。MultiBosoting先减低模型的偏差再减低模型的方差，这样的方式
MultiBoosting由于集合了Bagging，Wagging，AdaBoost，可以有效的降低误差和方差，特别是误差。但是训练成本和预测成本都会显著增加。
Iterative Bagging相比Bagging会降低误差，但是方差上升。由于Bagging本身就是一种降低方差的算法，所以Iterative Bagging相当于Bagging与单分类器的折中。

### 3.11 训练过程中，每轮训练一直存在分类错误的问题，整个Adaboost却能快速收敛，为何？

每轮训练结束后，AdaBoost 会对样本的权重进行调整，调整的结果是越到后面被错误分类的样本权重会越高。而后面的分类器为了达到较低的带权分类误差，会把样本权重高的样本分类正确。这样造成的结果是，虽然每个弱分类器可能都有分错的样本，然而整个 AdaBoost 却能保证对每个样本进行正确分类，从而实现快速收敛。

### 3.12 Adaboost 的优缺点？

```
优点：能够基于泛化性能相当弱的的学习器构建出很强的集成，不容易发生过拟合。
```

```
缺点：对异常样本比较敏感，异常样本在迭代过程中会获得较高的权值，影响最终学习器的性能表现。
```

## 参考资料：

1. 台湾清华大学李端兴教授2017年秋机器学习概论课程(CS 4602)PPT
2. 周志华 《机器学习》第8章 集成学习
3. [July的博客](https://blog.csdn.net/v_JULY_v/article/details/40718799)
4. http://fornlp.com/%E5%91%A8%E5%BF%97%E5%8D%8E-%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B-%E7%AD%94%E6%A1%88%E6%95%B4%E7%90%86/

---

# 文件：AI算法\machine-learning\Apriori.md

---

# 协同过滤面试题

## 1. 协同过滤推荐有哪些类型

* 基于用户(user-based)的协同过滤
  
  基于用户(user-based)的协同过滤主要考虑的是用户和用户之间的相似度，只要找出相似用户喜欢的物品，并预测目标用户对对应物品的评分，就可以找到评分最高的若干个物品推荐给用户。
* 基于项目(item-based)的协同过滤
  
  基于项目(item-based)的协同过滤和基于用户的协同过滤类似，只不过这时我们转向找到物品和物品之间的相似度，只有找到了目标用户对某些物品的评分，那么我们就可以对相似度高的类似物品进行预测，将评分最高的若干个相似物品推荐给用户
* 基于模型(model based)的协同过滤
  
  用机器学习的思想来建模解决，主流的方法可以分为：用关联算法，聚类算法，分类算法，回归算法，矩阵分解，神经网络,图模型以及隐语义模型来解决。

## 2. 基于模型的协同过滤

* 用关联算法做协同过滤
  
  做频繁集挖掘，找到满足支持度阈值的关联物品的频繁N项集或者序列。将频繁项集或序列里的其他物品按一定的评分准则推荐给用户，这个评分准则可以包括**支持度**，**置信度**和**提升度**等。 常用的关联推荐算法有**Apriori**，**FP Tree**和**PrefixSpan**
* 用聚类算法做协同过滤
  
  - 基于用户聚类，则可以将用户按照一定距离度量方式分成不同的目标人群，将**同样目标人群评分高的物品推荐给目标用户**。
  - 基于物品聚类，则是**将用户评分高物品的相似同类物品推荐给用户**。常用的聚类推荐算法有**K-Means**, **BIRCH**, **DBSCAN**和**谱聚类**
* 用分类算法做协同过滤
  
  设置一份评分阈值，评分高于阈值的就是推荐，评分低于阈值就是不推荐，我们**将问题变成了一个二分类问题**。虽然分类问题的算法多如牛毛，但是目前使用最广泛的是逻辑回归。因为**逻辑回归的解释性比较强**，每个物品是否推荐我们都有一个明确的概率放在这，同时可以对数据的特征做工程化，得到调优的目的。常见的分类推荐算法有逻辑回归和朴素贝叶斯，两者的特点是解释性很强。
* 用回归算法做协同过滤
  
  评分可以是一个连续的值而不是离散的值，**通过回归模型**我们可以得到目标用户对某商品的**预测打分**。常用的回归推荐算法有Ridge回归，回归树和支持向量回归。
* 用矩阵分解做协同过滤
  
  用矩阵分解做协同过滤是目前使用也很广泛的一种方法。由于传统的奇异值分解SVD要求矩阵不能有缺失数据，必须是稠密的，而我们的用户物品评分矩阵是一个很典型的稀疏矩阵，直接使用传统的SVD到协同过滤是比较复杂的。
* 用神经网络做协同过滤
  
  用神经网络乃至深度学习做协同过滤应该是以后的一个趋势。目前比较主流的用两层神经网络来做推荐算法的是限制玻尔兹曼机(RBM)
* 用隐语义模型做协同过滤
  
  隐语义模型主要是基于NLP的，涉及到**对用户行为的语义分析来做评分推荐**，主要方法有隐性语义分析LSA和隐含狄利克雷分布LDA，
* 用图模型做协同过滤
  
  用图模型做协同过滤，则将用户之间的相似度放到了一个图模型里面去考虑，常用的算法是SimRank系列算法和马尔科夫模型算法。

## 3. 频繁项集的评估标准

* 支持度:
  
  * 支持度就是几个关联的数据在数据集中出现的次数占总数据集的比重。或者说几个数据关联出现的概率。
    $$
    \text {Support} (X, Y)=P(X Y)=\frac{\text { number }(X Y)}{\text { num (AllSamples) }}
    $$
* 置信度:
  
  * 一个数据出现后，另一个数据出现的概率，或者说数据的条件概率。
  
  $$
  text {Confidence }(X \Leftarrow Y)=P(X | Y)=\frac{P(X Y)}{ P(Y)}
  $$
* 提升度 ：
  
  * 提升度表示含有Y的条件下，同时含有X的概率，与X总体发生的概率之比
  
  $$
  text {Lift }(X \Leftarrow Y)=\frac{P(X | Y)}{ P(X)} = \frac{\text { Confidence }(X \Leftarrow Y) }{ P(X)}
  $$
* 注意：
  
  * 支持度高的数据不一定构成频繁项集，但是支持度太低的数据肯定不构成频繁项集。
  * 提升度体先了$X$和$Y$之间的关联关系, 提升度大于1则$X\Leftarrow Y$是有效的强关联规则， 提升度小于等于1则$X\Leftarrow Y$是无效的强关联规则 。一个特殊的情况，如果$X$和$Y$独立,则$\operatorname{Lift}(X \Leftarrow Y)=1$，因此$P(X | Y)=P(X)$

## 4. 使用Aprior算法找出频繁k项集

输入：数据集合$D$，支持度阈值$\alpha$

输出：最大的频繁$k$项集

* 扫描整个数据集，得到所有出现过的数据，作为候选频繁1项集。$k=1$，频繁0项集为空集。
* 挖掘频繁$k$项集
  
  * 扫描数据计算候选频繁$k$项集的支持度
  * 去除候选频繁$k$项集中支持度低于阈值的数据集,得到频繁$k$项集。如果得到的频繁$k$项集为空，则直接返回频繁$k-1$项集的集合作为算法结果，算法结束。如果得到的频繁$k$项集只有一项，则直接返回频繁$k$项集的集合作为算法结果，算法结束。
  * 基于频繁$k$项集，连接生成候选频繁$k+1$项集。
* 令$k=k+1$，转入步骤挖掘频繁$k$项集。

从算法的步骤可以看出，Aprior算法每轮迭代都要扫描数据集，因此在数据集很大，数据种类很多的时候，算法效率很低。

具体实现:

<img src="https://images2015.cnblogs.com/blog/1042406/201701/1042406-20170117161036255-1753157633.png" style="zoom:50%;" />

## 5. 使用Aprior算法找出强关联规则

- 强关联规则:
  
  - 如果规则$R$:$\Rightarrow $满足 :
  
  $$
  \tag{1} { support }(X \Rightarrow Y) \geq \min {sup}
  $$
  
  $$
  \tag{2} confidence (X \Rightarrow Y) \geq \min conf
  $$
  
  称关联规则$X\Rightarrow Y$为强关联规则,否则称关联规则$X\Rightarrow Y$为弱关联规则。在挖掘关联规则时,产生的关联规则要经过$\min sup$和$\min conf$的衡量筛选出来的强关联规则才能用商家的决策

---

# 文件：AI算法\machine-learning\Catboost.md

---

# Catboost面试题

## 1. 简单介绍Catboost？

CatBoost是一种以对称决策树 为基学习器的GBDT框架，主要为例合理地**处理类别型特征**，CatBoost是由Categorical和Boosting组成。CatBoost还解决了**梯度偏差**以及**预测偏移**的问题，从而减少过拟合的发生，进而提高算法的准确性和泛化能力。

## 2. 相比于XGBoost、LightGBM，CatBoost的创新点有哪些？

- 自动将类别型特征处理为数值型特征。
- Catboost对**类别特征进行组合**，极大的丰富了特征维度。
- 采用**排序提升**的方法对抗训练集中的噪声点，从而避免梯度估计的偏差，进而解决预测偏移的问题。
- 采用了**完全对称树**作为基模型。

## 3. Catboost是如何处理类别特征的？

- 基数比较低的类别型特征
  
  利用One-hot编码方法将特征转为数值型
- 基数比较高的类别型特征
  
  - 首先会计算一些数据的statistics。计算某个category出现的频率，加上超参数，生成新的numerical features。这一策略要求同一标签数据不能排列在一起（即先全是之后全是这种方式），训练之前需要打乱数据集。
  - 第二，使用数据的不同排列（实际上是个）。在每一轮建立树之前，先扔一轮骰子，决定使用哪个排列来生成树。
  - 第三，考虑使用categorical features的不同组合。例如颜色和种类组合起来，可以构成类似于blue dog这样的特征。当需要组合的categorical features变多时，CatBoost只考虑一部分combinations。在选择第一个节点时，只考虑选择一个特征，例如A。在生成第二个节点时，考虑A和任意一个categorical feature的组合，选择其中最好的。就这样使用贪心算法生成combinations。
  - 第四，除非向gender这种维数很小的情况，不建议自己生成One-hot编码向量，最好交给算法来处理。

## 4. Catboost如何避免梯度偏差

经典梯度提升算法每个步骤中使用的梯度由当前模型中的相同的数据点(节点)来估计，这导致估计梯度在特征空间的任何域中的分布与该域中梯度的真实分布相比发生了偏移，从而导致过拟合。

对于每一个样本单独训练一个模型，使用模型估计样本的梯度，并用估计的结果对模型进行评分

## 5. Catboost如何避免预测偏移？

预测偏移是由梯度偏差造成的。在GDBT的每一步迭代中, 损失函数使用相同的数据集求得当前模型的梯度, 然后训练得到基学习器, 但这会导致梯度估计偏差, 进而导致模型产生过拟合的问题。CatBoost通过采用**排序提升** 的方式替换传统算法中梯度估计方法，进而减轻梯度估计的偏差。

## 6. 解释一下排序提升

在传统的GBDT框架当中，构建下一棵树分为两个阶段：选择树结构和在树结构固定后计算叶子节点的值。CatBoost主要在第一阶段进行优化。在建树的阶段，CatBoost有两种提升模式，Ordered和Plain。Plain模式是采用内建的ordered TS对类别型特征进行转化后的标准GBDT算法。Ordered则是对Ordered boosting算法的优化。

## 7. Catboost为什么要使用对称树？

- 对称树是平衡的，不容易过拟合
- 统一层使用相同的分割准则
- 每个叶子节点的索引可以被编码为长度等于树深度的二进制向量
  
  首先将所有浮点特征、统计信息和独热编码特征进行二值化，然后使用二进制特征来计算模型预测值

## 8. CatBoost的优缺点

**优点：**

- **性能卓越：** 在性能方面可以匹敌任何先进的机器学习算法；
- **鲁棒性/强健性：** 它减少了对很多超参数调优的需求，并降低了过度拟合的机会，这也使得模型变得更加具有通用性；
- **实用：** 可以处理类别型、数值型特征；
- **可扩展：** 支持自定义损失函数；

**缺点：**

- 对于类别型特征的处理需要大量的内存和时间；
- 不同随机数的设定对于模型预测结果有一定的影响；

---

# 文件：AI算法\machine-learning\CRF.md

---

# 条件随机场面试题

Author: 李文乐; Email: cocoleYY@outlook.com

## 1. 简单介绍条件随机场

---

条件随机场（conditional random field，简称 CRF）是给定一组输入随机变量条 件下另一组输出随机变量的条件概率分布模型，其特点是**假设输出随机变量构成马尔可夫随机场**，是一种鉴别式机率模型，是随机场的一种，常用于标注或分析序列资料，如自然语言文字或是生物序列。
如同马尔可夫随机场，条件随机场为无向图模型，图中的顶点代表随机变量，顶点间的连线代表随机变量间的相依关系，在条件随机场当中，随机变量 Y 的分布为条件机率，给定的观察值则为随机变量 X。
原则上，条件随机场的图模型布局是可以任意给定的，一般**常用的布局是链接式**的架构，链接式架构不论在训练（training）、推论（inference）、或是解码（decoding）上，都存在有效率的算法可供演算。
条件随机场跟隐马尔可夫模型常被一起提及，条件随机场对于输入和输出的机率分布，没有如隐马尔可夫模型那般强烈的假设存在 [补充：因为HMM模型假设后面状态和前面无关]。

##2. 条件随机场预测的维特比算法求解过程：

输入：模型特征向量F(y,x)和权值向量w，观测序列$x=(x_1,x_2,…,x_n)$;
输出：最优路径$y^*=(y_1^*,y_2^*,…,y_n^*) $

初始化：

$$
\delta_{1}(j)=w \cdot F_{1}\left(y_{0}=\operatorname{start}, y_{1}=j, x\right), \quad j=1,2, \cdots, m
$$

递推：

$$
\delta_{i}(l)=\max _{1<j<m}\left\{\delta_{i-1}(j)+w \cdot F_{i}\left(y_{i-1}=j, y_{i}=l, x\right)\right\}, \quad l=1,2, \cdots, m
$$

$$
\Psi_{i}(l)=\arg \max _{1 \leqslant j \leqslant m}\left\{\delta_{t-1}(j)+w \cdot F_{i}\left(y_{i-1}=j, y_{i}=l, x\right)\right\}, \quad l=1,2, \cdots, m
$$

终止：

$$
\max _{y}(w \cdot F(y, x))=\max _{1<j<m} \delta_{n}(j)
$$

$$
y_{n}^{*}=\arg \max _{1 \leqslant j \leqslant m} \delta_{n}(j)
$$

返回路径:

$$
y_{i}^{*}=\Psi_{i+1}\left(y_{i+1}^{*}\right), \quad i=n-1, n-2, \cdots, 1
$$

##3. 链式条件随机场[chain-structured CRF]条件概率公式：

$$
P(\mathbf{y} \mid \mathbf{x})=\frac{1}{Z} \exp \left(\sum_{j} \sum_{i=1}^{n-1} \lambda_{j} t_{j}\left(y_{i+1}, y_{i}, \mathbf{x}, i\right)+\sum_{k} \sum_{i=1}^{n} \mu_{k} s_{k}\left(y_{i}, \mathbf{x}, i\right)\right)
$$

## 4. HMM、MEMM和CRF模型的比较

* HMM模型是对转移概率（隐藏状态转移到隐藏状态的概率）和表现概率（隐藏状态到观察状态的概率）直接建模，统计共现概率；
* MEMM模型是对转移概率和表现概率建立联合概率，统计时统计的是条件概率，而非共现概率。MEMM容易陷入局部最优，主要因为是MEMM只在局部做归一化；
* CRF模型则统计的是全局概率，在归一化时考虑了数据在全局的分布，而不仅仅是局部归一化，这样也就解决了MEMM中的标记偏置问题；

## 5. 注意要点

---

- 概率图模型的表示
  概率图模型结合了概率论和图论的知识，用图模式(节点和边)表达基于概率相关关系的模型的总称。图模型的引入使得人们在处理复杂概率问题时，可以将复杂问题进行适当的分解；表示理论将图模型分为如下两个类别：贝叶斯网络[Bayesian Netword]和马尔科夫随机场[Markov Random Field]，前者采用有向无环图来表达事件的因果关系，后者采用无向图来表达变量间的相互作用；
- 贝叶斯网络和马尔科夫随机场的分解计算问题
  贝叶斯网络中每个节点都对应一个先验概率分布或者条件概率分布，因此整体联合概率分布可以直接分解为所有单个节点分布的乘积；对于马尔科夫随机场，由于变量间没有明确的因果关系，它的联合概率分布通常会表达为一系列势函数[Potential Function]的乘积，因为乘积之和通常不为1，所以要进行归一化才能成为一个有效的概率分布。
- 对于概率图模型，模型学习的精度通常受三方面影响
  
  - 语料库样本集对总体的代表性；
  - 模型算法理论基础及所针对的问题。不同模型的理论不同，所擅长处理的NLP任务也不同，比如：朴素贝叶斯模型处理短文本分类效果很好，最大熵模型在处理中文词性标注表现很好，条件随机场处理中文分词，语义组块等方便精度很好，Semi-CRF在处理命名实体识别精度很好。
  - 模型算法的复杂度。属于工程问题，一般讲，要求模型参数估计的越精确，模型复杂度越高，学习时间越长，推断和预测的精度也越高。
- Bi-LSTM-CRF算法解析
  
  ![image-20210903204605132](img/CRF/image-20210903204605132.png)
  
  Bi-LSTM-CRF模型的输入是每个单词的词向量，经过双向LSTM层提取特征并输出为5个label的得分，再将该得分输入进CRF层，得到这句话最终最大可能的识别标签。因为BiLSTM层得到的label并不总是满足实际情况，CRF层能够添加一些约束使得预测标签是有效的。这些约束便是从训练数据的过程中学习得到的。
  
  - 常见的概率图模型中，哪些是生成模型和哪些是判别模型？
    
    - 生成式 模型是对联合概率分布$P(X,Y,Z)$进行建模，在给定观测集合X的条件下，通过计算 边缘分布来得到对变量集合Y的推断，即

$$
P(Y \mid X)=\frac{P(X, Y)}{P(X)}=\frac{\sum_{Z} P(X, Y, Z)}{\sum_{Y . Z} P(X, Y, Z)}
$$

- 判别式模型是直接对条件概率分布$P(Y,Z|X)$进行建模，然后消掉无关变量Z就可以 得到对变量集合Y的预测，即:

$$
P(Y \mid X)=\sum_{Z} P(Y, Z \mid X)
$$

常见的概率图模型有朴素贝叶斯、最大熵模型、贝叶斯网络、隐马尔可夫模 型、条件随机场、pLSA、LDA等。基于前面的问题解答，我们知道朴素贝叶斯、贝叶斯网络、pLSA、LDA等模型都是先对联合概率分布进行建模，然后再通过计算边缘分布得到对变量的预测，所以它们都属于生成式模型；

而最大熵模型是直 接对条件概率分布进行建模，因此属于判别式模型。隐马尔可夫模型和条件随机场模型是对序列数据进行建模的方法，其中隐马尔 可夫模型属于生成式模型，条件随机场属于判别式模型。

## 参考

1.条件随机场定义参考维基百科
2.Bi-LSTM-CRF算法解析参考: https://createmomo.github.io/
3.数学之美 - 吴军
4.百面机器学习 - 诸葛越&葫芦娃
5.NLP汉语自然语言处理原理与实践 - 郑捷
6.http://blog.sina.com.cn/s/blog_6d1875160101gy4e.html

---

# 文件：AI算法\machine-learning\DecisionTree.md

---

# 

# 决策树面试题

## 1. 简单介绍决策树算法

决策树算法是一种逼近离散函数值的方法。它是一种典型的分类方法，首先对数据进行处理，利用归纳算法生成可读的规则和决策树，然后使用决策对新数据进行分析。**本质上决策树是通过一系列规则对数据进行分类的过程。**

决策树将算法组织成一颗树的形式。其实这就是将平时所说的**if-then语句**构建成了树的形式。决策树主要包括**三个部分：内部节点、叶节点、边。内部节点是划分的特征，边代表划分的条件，叶节点表示类别。**

构建决策树 就是一个递归的选择内部节点，计算划分条件的边，最后到达叶子节点的过程。 决策树在本质上是一组嵌套的if-else判定规则，从数学上看是分段常数函数，对应于用平行于坐标轴的平面对空间的划分。判定规则是人类处理很多问题时的常用方法，这些规则是我们通过经验总结出来的，而决策树的这些规则是通过训练样本自动学习得到的。

训练时，通过最大化Gini或者其他指标来寻找最佳分裂。决策树可以输特征向量每个分量的重要性。

**决策树是一种判别模型，既支持分类问题，也支持回归问题，是一种非线性模型（分段线性函数不是线性的）。它天然的支持多分类问题。**

## 2. 决策树和条件概率分布的关系？

**决策树可以表示成给定条件下类的条件概率分布。**

决策树中的每一条路径对应的都是划分的一个条件概率分布. 每一个叶子节点都是通过多个条件之后的划分空间，在叶子节点中计算每个类的条件概率，必然会倾向于某一个类，即这个类的概率最大。

## 3. 信息增益比相对信息增益有什么好处？

* 使用信息增益时：模型**偏向于选择取值较多**的特征
* 使用信息增益比时：**对取值多的特征加上的惩罚**，对这个问题进行了校正。

## 4. ID3算法—>C4.5算法—> CART算法

* $ID3$
  
  * $ID3$算法没有考虑连续特征，比如长度，密度都是连续值，无法在ID3运用。这大大限制了ID3的用途。
  * $ID3$算法采用信息增益大的特征优先建立决策树的节点，偏向于取值比较多的特征
  * $ID3$算法对于缺失值的情况没有做考虑
  * $ID3$算法没有考虑过拟合的问题
* $C4.5$在$ID3$算法上面的改进
  
  * 连续的特征离散化
  * 使用信息增益比
  * 通过剪枝算法解决过拟合
* $C4.5$的不足：
  
  * $C4.5$生成的是多叉树
  * $C4.5$只能用于分类，如果能将决策树用于回归的话可以扩大它的使用范围。
  * $C4.5$由于使用了熵模型，里面有大量的耗时的对数运算,如果是连续值还有大量的排序运算
* $CART$算法
  
  * 可以做回归，也可以做分类，
  * 使用基尼系数来代替信息增益比
  * $CART$分类树离散值的处理问题，采用的思路是不停的二分离散特征。

## 5. 决策树的缺失值是怎么处理的

主要需要解决的是两个问题，一是在样本某些特征缺失的情况下选择划分的属性，二是选定了划分属性，对于在该属性上缺失特征的样本的处理。

* 如何在特征值缺失的情况下进行划分特征的选择？
  
  * 每个样本设置一个权重（初始可以都为1）
  * 划分数据，一部分是有特征值$a$的数据，另一部分是没有特征值$a$的数据,记为$\tilde{D}$，
  * **对**没有缺失特征值$a$的**数据集$\tilde{D}$，**来和对应的特征$A$的各个特征值一起**计算加权重后的信息增益比**，最后乘上一个系数$\rho$ 。

$$
\rho=\frac{\sum_{x \in \tilde{D}} w_{x}}{\sum_{x \in {D}} w_{x}}
$$

$$
\tilde{p}_{k}=\frac{\sum_{x \in \tilde{D}_{k}} w_{x}}{\sum_{x \in \tilde{D}} w_{x}} \quad(1 \leq \mathrm{k} \leq|y|)
$$

$$
\tilde{r}_{v}=\frac{\sum_{x \in \tilde D^{v}} w_{x}}{\sum_{x \in \tilde{D}} w_{x}} \quad(1 \leq v \leq V)
$$

```
假设特征
```

$A$有$v$个取值$\{a_1,a_2 \dots a_v\}$

```

```

$\tilde D$：该特征上没有缺失值的样本

```

```

$\tilde D_k$：$\tilde D$中属于第$k$类的样本子集

```

```

$\tilde D^v$：$\tilde D$中在特征$a$上取值为$a_v$的样本子集

```

```

$\rho$：无特征$A$缺失的样本加权后所占加权总样本的比例。

```

```

$\tilde{p}_{k}$：无缺失值样本第$k$类所占无缺失值样本的比例

```

```

$\tilde{r}_{v}$：无缺失值样本在特征$a$上取值$a^v$的样本所占无缺失值样本的比例

```
新的信息增益公式：
```

$$
\begin{aligned}
&\operatorname{Gain}(D, a)=\rho \times \operatorname{Gain}(\tilde{D}, a)=\rho \times\left(\operatorname{Ent}(\tilde{D})-\sum_{v=1}^{V} \tilde{r}_{v} \operatorname{Ent}\left(\tilde{D}^{v}\right)\right)\\
&\operatorname{Ent}(\tilde{D})=-\sum_{k=1}^{|y|} \tilde{p}_{k} \log _{2} \tilde{p}_{k}
\end{aligned}
$$

* 给定划分特征，若样本在该特征上的值是缺失的，那么该如何对这个样本进行划分？
  
  ```
  （即到底把这个样本划分到哪个结点里？）
  ```
  
  * 让包含缺失值的样本以不同的概率划分到不同的子节点中去。
  
  ```
  比如缺失特征A的样本a之前权重为1，特征A有3个特征值A1,A2,A3。 3个特征值对应的无缺失A特征的样本个数为2,3,4.则a同时划分入A1，A2，A3。对应权重调节为2/9,3/9, 4/9。
  ```

## 6. 决策树的目标函数是什么？

$$
C_{\alpha}(T)=\sum_{t=1}^{|T|} N_{t} H_{t}(T)+a|T|
$$

$$
H_{t}(T)=-\sum_{k} \frac{N_{t k}}{N_{t}} \log \frac{N_{t k}}{N_{t}}
$$

其中$|T|$代表叶节点个数

$N_t$表示具体某个叶节点的样本数

$H_t(T)$ 表示叶节点$t$上的经验熵

$\alpha|T|$为正则项，$\alpha \geqslant 0 $ 为参数

## 7. 决策树怎么处理连续性特征？

因为连续特征的可取值数目不再有限，因此不能像前面处理离散特征枚举离散特征取值来对结点进行划分。因此需要连续特征离散化，常用的离散化策略是二分法，这个技术也是$C4.5$中采用的策略。下面来具体介绍下，如何采用二分法对连续特征离散化：

* 训练集D，连续特征$A$，其中A有n个取值
* 对$A$的取值进行从小到大排序得到：$\{a_1,a_2\dots a_n\}$
* 寻找划分点$t$，$t$将D分为子集$D_{t}^{-}$与$D_{t}^{+}$
  
  * $D_{t}^{-}$：特征$A$上取值不大于$t$的样本
  * $D_{t}^{+}$：特征$A$上取值大于$t$的样本
* 对相邻的特征取值$a_i$与$a_{i+1}$，t再区间$[a_i,a_{i+1})$中取值所产生的划分结果相同，因此对于连续特征$A$,包含有$n-1$个元素的后选划分点集合

$$
T_a = \{\frac{a_i + a_{i+1}}{2}|1\leq{i}\leq{n-1} \}
$$

* 把区间$[a_i,a_{i+1})$的中位点$\frac{a_i + a_{i+1}}{2}$作为候选划分点
* 按照处理离散值那样来选择最优的划分点,使用公式：
  
  $$
  Gain(D,a) =\underbrace{max}_{t\in T_a}Gain(D,a,t) = \underbrace{max}_{t\in T_a}\ (Ent(D) - \sum_{\lambda \in \{-,+ \}}\frac{|D_t^{\lambda}|}{|D|}Ent(D_t^{\lambda}))
  $$
  
  其中$Gain(D,a,t)$是样本集$D$基于划分点$t$二分之后的信息增益。划分点时候选择使用$Gain(D,a,t)$最大的划分点。

## 8. 决策树对离散值的处理

思想和$C4.5$相同，都是将连续的特征离散化。唯一区别在选择划分点时，C4.5是信息增益比，CART是基尼系数。

CART采用的是不停的二分。会考虑把特征$A$分成${A1}$和${A2,A3}$、${A2}$和${A1,A3}$、${A3}$和${A1,A2}$三种情况，找到基尼系数最小的组合，比如${A2}$和${A1,A3}$，然后建立二叉树节点，一个节点是$A2$对应的样本，另一个节点是${A1,A3}$对应的样本。由于这次没有把特征$A$的取值完全分开，后面还有机会对子节点继续选择特征$A$划分$A1$和$A3$。这和$ID3、C4.5$不同，在$ID3$或$C4.5$的一颗子树中，离散特征只会参与一次节点的建立。

## 9. 决策树怎么防止过拟合？

* 对于决策树进行约束：根据情况来选择或组合
  
  * 设置每个叶子节点的最小样本数，可以避免某个特征类别只适用于极少数的样本。
  * 设置每个节点的最小样本数，从根节点开始避免过度拟合。
  * 设置树的最大深度，避免无限往下划分。
  * 设置叶子节点的最大数量，避免出现无限多次划分类别。
  * 设置评估分割数据是的最大特征数量，避免每次都考虑所有特征为求“最佳”，而采取随机选择的方式避免过度拟合。
* 预剪枝(提前停止)：控制**深度、当前的节点数、分裂对测试集的准确度提升大小**
  
  * 限制树的高度，可以利用交叉验证选择
  * 利用分类指标，如果下一次切分没有降低误差，则停止切分
  * 限制树的节点个数，比如某个节点小于100个样本，停止对该节点切分
* 后剪枝(自底而上)：**生成决策树、交叉验证剪枝：子树删除，节点代替子树、测试集准确率判断决定剪枝**
  
  * 在决策树构建完成之后，根据加上正则项的结构风险最小化自下向上进行的剪枝操作. 剪枝的目的就是防止过拟合，是模型在测试数据上变现良好，更加鲁棒。

## 10. 如果特征很多，决策树中最后没有用到的特征一定是无用吗？

不是无用的，从两个角度考虑：

* **特征替代性**，如果可以已经使用的特征$A$和特征$B$可以提点特征$C$，特征$C$可能就没有被使用，但是如果把特征$C$单独拿出来进行训练，依然有效
* 决策树的每一条路径就是**计算条件概率的条件**，前面的条件如果包含了后面的条件，只是这个条件在这棵树中是无用的，如果把这个条件拿出来也是可以帮助分析数据.

## 11.决策树的优缺点？

* 优点:
  
  * 简单直观，生成的决策树很直观。
  * 基本不需要预处理，不需要提前归一化，处理缺失值。
  * 既可以处理离散值也可以处理连续值。很多算法只是专注于离散值或者连续值。
  * 可以处理多维度输出的分类问题。
  * 相比于神经网络之类的黑盒分类模型，决策树在逻辑上可以得到很好的解释
  * 可以交叉验证的剪枝来选择模型，从而提高泛化能力。
  * 对于异常点的容错能力好，健壮性高。
  * 用白盒模型，可清洗观察每个步骤，对大数据量的处理性能较好，更贴近人类思维。
* 缺点:
  
  * 决策树算法非常容易过拟合，导致泛化能力不强。可以通过设置节点最少样本数量和限制决策树深度来改进。
  * 决策树会因为样本发生一点点的改动，就会导致树结构的剧烈改变。这个可以通过集成学习之类的方法解决。
  * 寻找最优的决策树是一个NP难的问题，我们一般是通过启发式方法，容易陷入局部最优。可以通过集成学习之类的方法来改善。
  * 有些比较复杂的关系，决策树很难学习，比如异或。这个就没有办法了，一般这种关系可以换神经网络分类方法来解决。
  * 如果某些特征的样本比例过大，生成决策树容易偏向于这些特征。这个可以通过调节样本权重来改善。

## 12. 树形结构为什么不需要归一化?

* 计算信息增益前，按照特征值进行排序，排序的顺序不变，那么所属的分支以及分裂点就不会有不同。
* **数值缩放不影响分裂点位置，对树模型的结构不造成影响**。

## 13. 如果特征很多，决策树中最后没有用到的特征一定是无用吗？

不是无用的,从两个角度考虑：

* 特征替代性，如果可以已经使用的特征$A$和特征$B$可以提点特征$C$，特征$C$可能就没有被使用，但是如果把特征$C$单独拿出来进行训练，依然有效.
* 决策树的每一条路径就是计算条件概率的条件，前面的条件如果包含了后面的条件，只是这个条件在这棵树中是无用的，如果把这个条件拿出来也是可以帮助分析数据。

## 参考资料

[c4.5为什么使用信息增益比来选择特征？](https://www.zhihu.com/question/22928442/answer/440836807)

---

# 文件：AI算法\machine-learning\EnsembleLearning.md

---

# 集成学习面试题

## 1. 什么是集成学习算法？

**集成学习算法是一种优化手段或者策略**，将多个较弱的模型集成模型组，一般的弱分类器可以是决策树，SVM，KNN等构成。其中的模型可以单独进行训练，并且它们的预测能以某种方式结合起来去做出一个总体预测。

## 2. 集成学习主要有哪几种框架？

集成学习从集成思想的架构分为Bagging，Boosting，Stacking三种。

## 3. 简单介绍一下bagging，常用bagging算法有哪些？

* Bagging
  
  * **多次采样，训练多个分类器，集体投票，旨在减小方差**，
* 基于数据**随机重抽样**的分类器构建方法。从训练集中进行子抽样组成每个基模型所需要的子训练集，对所有基模型预测的结果进行综合产生最终的预测结果。
* 算法流程：
  
  * 输入为样本集$D={(x_1，y_1)，(x_2，y_2) \dots (x_m，y_m)}$，弱学习器算法，弱分类器迭代次数$T$。
  * 输出为最终的强分类器$f(x)$
* 对于$t=1，2 \dots T$
  
  * 对训练集进行第t次随机采样，共采集$T$次，得到包含$T$个样本的采样集$D_t$
  * 用采样集$D_t$训练第$t$个弱学习器$G_t(x)$
* 如果是分类算法预测，则$T$个弱学习器投出最多票数的类别或者类别之一为最终类别。如果是回归算法，$T$个弱学习器得到的回归结果进行算术平均得到的值为最终的模型输出。
* 常用bagging算法：随机森林算法

## 4. 简单介绍一下boosting，常用boosting算法有哪些？

* Boosting
  
  * **基分类器层层叠加，聚焦分错的样本，旨在减小方差**
* 训练过程为阶梯状，基模型按次序进行训练（实现上可以做到并行），基模型的训练集按照某种策略每次都进行一定的转化，每次都是提高前一次分错了的数据集的权值，最后对所有基模型预测的结果进行线性组合产生最终的预测结果。
* 算法流程：
  
  * 给定初始训练数据，由此训练出第一个基学习器；
  * 根据基学习器的表现对样本进行调整，在之前学习器做错的样本上投入更多关注；
  * 用调整后的样本，训练下一个基学习器；
  * 重复上述过程T次，将T个学习器加权结合。
* 常用boosting算法：
  
  * Adaboost
  * GBDT
  * XGBoost

## 5. boosting思想的数学表达式是什么？

$$
f(x)=w_{0}+\sum_{m=1}^{M} w_{m} \phi_{m}(x)
$$

其中$w$是权重，$\phi$是弱分类器的集合，可以看出最终就是基函数的线性组合。

## 6. 简单介绍一下stacking

* Stacking
  
  * **多次采样，训练多个分类器，将输出作为最后的输入特征**
* 将训练好的所有基模型对训练集进行预测，第个$i$基模型对第$i$个训练样本的预测值将作为新的训练集中第$i$个样本的第$i$个特征值，最后基于新的训练集进行训练。同理，预测的过程也要先经过所有基模型的预测形成新的测试集，最后再对测试集进行预测。
* stacking常见的使用方式：
  
  * 由k-NN、随机森林和朴素贝叶斯基础分类器组成，它的预测结果由作为元分类器的逻回归组合。

## 7. 你意识到你的模型受到低偏差和高方差问题的困扰，应该使用哪种算法来解决问题呢？为什么？

低偏差意味着模型的预测值接近实际值。换句话说，该模型有足够的灵活性，以模仿训练数据的分布。貌似很好，但是别忘了，一个灵活的模型没有泛化能力。这意味着，当这个模型用在对一个未曾见过的数据集进行测试的时候，它会令人很失望。
在这种情况下，我们可以使用bagging算法（如随机森林），以解决高方差问题。bagging算法把数据集分成重复随机取样形成的子集。然后，这些样本利用单个学习算法生成一组模型。接着，利用投票（分类）或平均（回归）把模型预测结合在一起。另外，为了应对大方差，我们可以：

* 使用正则化技术，惩罚更高的模型系数，从而降低了模型的复杂性。
* 使用可变重要性图表中的前n个特征。
* 可以用于当一个算法在数据集中的所有变量里很难寻找到有意义信号的时候。

## 8. 常用的基分类器是什么？

最常用的基分类器是决策树,原因:

* 决策树可以较为方便地将样本的权重整合到训练过程中，而不需要使用过采样的方法来调整样本权重。
* 决策树的表达能力和泛化能力，可以通过调节树的层数来做折中。
* 数据样本的扰动对于决策树的影响较大，因此不同子样本集合生成的决策树基分类器随机性较大，这样的“不稳定学习器”更适合作为基分类器。此外，在决策树节点分裂的时候，随机地选择一个特征子集，从中找出最优分裂属性，**很好地引入了随机性。**

## 9. 可否将随机森林中的基分类器，由决策树替换为线性分类器或K-近邻？请解释为什么？

不能：

* Bagging的主要好处是集成后的分类器的方差，比基分类器的方差小。
* 随机森林属于Bagging类的集成学习，对样本分布较为敏感的分类器更适用于Bagging。
* 线性分类器或者K-近邻都是较为稳定的分类器，本身方差就不大。
* 线性分类器或者K-近邻可能会由于Bagging的采样，导致在训练中更难收敛，增大偏差。

## 10. GBDT和RF如何计算特征重要性

* RF有两种方法：
  - 通过计算Gini系数的减少量VIm=GI−(GIL+GIR)判断特征重要性，越大越重要。
  - 对于一颗树，先使用**袋外错误率**(OOB)样本计算测试误差a，再随机打乱OOB样本中第i个特征（上下打乱特征矩阵第i列的顺序）后计算测试误差b，a与b差距越大特征i越重要。

- GBDT计算方法：
  
  - 所有回归树中通过特征i分裂后平方损失的减少值的和/回归树数量 得到特征重要性。 在sklearn中，GBDT和RF的特征重要性计算方法是相同的，都是基于单棵树计算每个特征的重要性，探究每个特征在每棵树上做了多少的贡献，再取个平均值。
- Xgb主要有三种计算方法：
  
  - importance_type=weight（默认值），特征重要性使用特征在所有树中作为划分属性的次数。
  - mportance_type=gain，特征重要性使用特征在作为划分属性时loss平均的降低量。
  - importance_type=cover，特征重要性使用特征在作为划分属性时对样本的覆盖度。

---

# 文件：AI算法\machine-learning\FrequentPattern.md

---

# 频繁模式(frequent pattern)

频繁模式一般是指频繁地出现在数据集中的模式。这种频繁模式和关联规则是数据挖掘中想要挖掘的知识。我们都知道一个很有趣的故事，就是啤酒和尿布的故事，

在某些特定的情况下，“啤酒”与“尿布”两件看上去毫无关系的商品，会经常出现在同一个购物篮中，且大多出现在年轻的父亲身上。

分析背后原因是，在美国有婴儿的家庭中，一般是母亲在家中照看婴儿，年轻的父亲去超市买尿布。父亲在购买尿布的同时，往往会顺便为自己购买啤酒。

由此，沃尔玛就在卖场尝试将啤酒与尿布摆放在相同区域，让年轻的父亲可以同时找到这两件商品，并很快地完成购物，从而极大提升商品销售收入。

数据挖掘就是想要挖掘出这种有趣的模式，可以称做频繁模式和关联规则的挖掘，一般情况下使用支持度(support)和置信度(confidence)来表示关联的程度，领域的专家设置最小支持度和最小置信度阈值，如果某个模式大于最小支持度和最小置信度，就认为是频繁模式。

为了挖掘这种模式，一般常用的有两种算法:

1. Apriori
2. Fp-tree

在介绍这两个算法之前需要给出一些定义:

1. A=>B的支持度:
   
   $$
   upport(A=>B)=p(A\cup B) \tag{1}
   $$
2. A=>B的置信度:
   
   $$
   onfidence(A=>B)=P(B|A)
   $$
   
   $$
   \frac{support(A \cup B)}{supoort(A)}=\frac{support_count(A \cup B)}{supoort_count(A)} \tag{2}
   $$
3. 一般关联规则的挖掘有两步过程:
   
   + 找出所有的频繁项集: 每一个频繁出现的次数大于等于最小支持度技术min_sup.
   + 由频繁相机产生强关联规则: 这些规则必须满足最小支持度和最小置信度.

## Apriori

Apriori通过限制候选产生发现频繁项集,它是为布尔关联规则挖掘频繁项集的原创性算法. 根据先验知识(频繁项集的所有非空子集也一定是频繁的).Apriri算法使用一种称为逐层搜索的迭代过程，其中k项集用于探索(k+1)项集.
Apriori主要有两步完成: 连接步和剪枝步。
这个算法给出一个例子更容易理解:
<img src="../assert/exm6.png">
解答(详细过程请参考《数据挖掘概念与技术第三版》 p250)
<img src="../assert/apr.png">

## FPTree

FPTree是基于频繁模式的增长，不产生候选挖掘频繁项集的挖掘方法，
使用频繁模式增长方法,我们重新考察例图6.2事务数据库 D 的挖掘。
数据库的第一次扫描与 Apriori 相同,它导出频繁项(1-项集)的集合,并得到它们的支持度计数(频繁性)。设最小支持度计数为 2。频繁项的集合按支持度计数的递减序排序。结果集或表记作 L 。这样,我们有：L = [I2:7, I1:6, I3:6, I4:2, I5:2]。
FP-树构造如下:

1. 首先,创建树的根结点,用“null”标记。
2. 二次扫描数据库 D。每个事务中的项按 L 中的次序处理(即,根据递减支持度计数排序)并对每个事务创建一个分枝.
3. 例如,
   第一个事务“T100: I1, I2, I5”按 L 的次序包含三个项{ I2, I1, I5},导致构造树的第一个分
   枝<(I2:1), (I1:1), (I5:1)>。该分枝具有三个结点,其中,I2 作为根的子女链接,I1 链接到 I2,
   I5 链接到 I1。第二个事务 T200 按 L 的次序包含项 I2 和 I4,它导致一个分枝,其中,I2 链接到根,
   I4 链接到 I2。然而,该分枝应当与 T100 已存在的路径共享前缀<I2>。这样,我们将结点 I2 的计
   数增加 1,并创建一个新结点(I4:1),它作为(I2:2)的子女链接。一般地,当为一个事务考虑增加
   分枝时,沿共同前缀上的每个结点的计数增加 1,为随在前缀之后的项创建结点并链接。
4. 为方便树遍历,创建一个项头表,使得每个项通过一个结点链指向它在树中的出现。扫描所有
   的事务之后得到的树展示在图 6.8 中,附上相关的结点链。这样,数据库频繁模式的挖掘问题就转换成挖掘 FP-树问题.
   <img src="../assert/fptree.png">
5. 根据fp tree得到频繁项集，根据支持度计数依次考虑每一个满足的元素，首先考虑计数最小的ID I5. 从根节点遍历所有到I5的路径，记录这个路径作为条件模式基,之后根据最小支持度得到条件Fp-tree，最后产生频繁项集. 具体的操作表格如下:
   <img src="../assert/fp.png">

**注:** 详细见数据挖掘概念与技术第6章

# 核心公式

1. 如何评估哪些模式是有趣的?

> 相关规则是A=>B[support, confidence]进一步扩充到相关分析A=>B[support, confidence, correlation]，常用的相关性度量:
> 
> + 提升度(lift),计算公式如下:
> 
> $$
> lift(A,b)=\frac{p(A \cup B)}{p(A)p(B)}=\frac{P(B|A)}{p(B)}=\frac{conf(A=>B)}{sup(B)} \tag{3}
> $$
> 
> + 使用$\chi^2$进行相关分析

2. 常用的模式评估度量

> + 全置信度(all_confidence)
> 
> $$
> all_conf(A,B)=\frac{A \cup B}{max\{sup(A),sup(B)\}}=min\{p(A|B),p(B|A)\} \tag{4}
> $$
> 
> + 最大置信度(max_confidence)
> 
> $$
> max_conf(A,B)=max\{P(A|B),p(B|A)\} \tag{5}
> $$
> 
> + Kulczynski(Kulc)度量
> 
> $$
> Kulc(A,B)=\frac{1}{2}(P(A|B)+P(B|A)) \tag{6}
> $$
> 
> + 余弦度量
> 
> $$
> cosine(A,B)=\frac{P(A\cup B)}{\sqrt{P(A) \times P(B)}}=\frac{sup(A \cup B)}{\sqrt{(sup(A) \times sup(B))}}=\sqrt{P(A|B)\times P(B|A)} \tag{7}
> $$

对于指示有趣的模式联系，全置信度、最大置信度、Kulczynsji和余弦哪个最好? 为了回答这个问题，引进不平衡比(Imbalance Ratio, IR)

$$
IR(A,B)=\frac{|sup(A)-sup(B)|}{sup(A)+sup(B)-sup(A\cup B)} \tag{8}
$$

# 算法十问

1. 强规则一定是有趣的吗?

> 不一定，规则是否有兴趣可能用主观或客观的标准来衡量。最终,只有用户能够确定规则是否是有趣的,并且这种判断是主观的,因不同用户而异。

2. 如何提高Apriori算法的效率?

> + **事务压缩**(压缩进一步迭代扫描的事务数):不包含任何 k-项集的事务不可能包含任何(k+1)-项集。这样,这种事务在其后的考虑时,可以加上标记或删除,因为为产生 j-项集(j > k),扫描数据库时不再需要它们。
> + **基于散列的技术**(散列项集计数):一种基于散列的技术可以用于压缩候选 k-项集 Ck (k >1)。
>   **划分**(为找候选项集划分数据):可以使用划分技术,它只需要两次数据库扫描,以挖掘频繁项集。
> + **选样**(在给定数据的一个子集挖掘):选样方法的基本思想是:选取给定数据库 D 的随机样本 S,然后,在 S 而不是在 D 中搜索频繁项集。用这种方法,我们牺牲了一些精度换取了有效性。
> + **动态项集计数**(在扫描的不同点添加候选项集):动态项集计数技术将数据库划分为标记开始点的块。

3. Apriori算法的优缺点?

> 1. 优点：
> 
> + 简单、易理解
> + 数据要求低。
> 
> 2. 缺点：
> 
> + 在每一步产生候选项目集时循环产生的组合过多，没有排除不应该参与组合的元素。
> + 每次计算项集的支持度时，都对数据库中的全部记录进行了一遍扫描比较，如果是一个大型的数据库时，这种扫描会大大增加计算机的I/O开销。
> 
> 3. 改进:
> 
> + 利用建立临时数据库的方法来提高Apriori算法的效率。
> + Fp-tree 算法。以树形的形式来展示、表达数据的形态；可以理解为水在不同河流分支的流动过程。
> + 垂直数据分布。相当于把原始数据进行行转列的操作，并且记录每个元素的个数。

4. FPtree vs Apriori算法

> FP-tree算法相对于Apriori算法，时间复杂度和空间复杂都有了显著的提高。但是对海量数据集，时空复杂度仍然很高，此时需要用到数据库划分等技术。

# 面试真题

1. 简述Apriori算法的思想，谈谈该算法的应用领域并举例?
   思想：其发现关联规则分两步，第一是通过迭代，检索出数据源中所有烦琐项集，即支持度不低于用户设定的阀值的项即集，第二是利用第一步中检索出的烦琐项集构造出满足用户最小信任度的规则，其中，第一步即挖掘出所有频繁项集是该算法的核心，也占整个算法工作量的大部分。在商务、金融、保险等领域皆有应用。在建筑陶瓷行业中的交叉销售应用，主要采用了Apriori算法.
2. 简述FPtree的原理和Apriori的不同?
3. 豆瓣电影数据集关联规则挖掘?
   如果让你分析电影数据集中的导演和演员信息，从而发现两者之间的频繁项集及关联规则，你会怎么做？

# 参考

1. https://saliormoon.github.io/2016/07/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%9D%A2%E8%AF%95%E9%97%AE%E9%A2%981/
2. 数据挖掘概念与技术第三版
3. https://baijiahao.baidu.com/s?id=1607039314145277013&wfr=spider&for=pc
4. 

---

# 文件：AI算法\machine-learning\HMM.md

---

# HMM

#### Author: 李文乐; Email: cocoleYY@outlook.com

## 直观理解

---

马尔可夫链（英语：Markov chain），又称离散时间马尔可夫链（discrete-time Markov chain，缩写为DTMC），因俄国数学家安德烈·马尔可夫（俄语：Андрей Андреевич Марков）得名，为状态空间中经过从一个状态到另一个状态的转换的随机过程。
隐马尔可夫模型包含5个要素：**初始概率分布，状态转移概率分布，观测概率分布，所有可能状态的集合，所有可能观测的集合**。
隐马尔可夫模型HMM是结构最简单的动态贝叶斯网络，是**有向图模型**。

## 核心公式

---

1. 依据马尔可夫性，所有变量的联合概率分布为：

> ![](https://m.qpic.cn/psb?/V11thrEZ18EV2M/y3zg1TDuIJGhEoCSULdtzszvuKIYGR91GX0V9e6t8mY!/b/dL8AAAAAAAAA&bo=zQE4AAAAAAARB8Q!&rf=viewer_4)

## 注意要点

---

- 统计语言模型[Statistical Language Model]

> 是自然语言处理的重要技术，对于要处理的一段文本，我们可以看做是离散的时间序列，并且具有上下文依存关系；该模型可以应用在语音识别和机器翻译等领域，其模型表达式如下：
> ![](http://m.qpic.cn/psb?/V11thrEZ18EV2M/VCYlXkt5CTD8qCeTNEGGbOzbC0P1ulagomRNpGJVoh8!/b/dLgAAAAAAAAA&bo=lgFMAAAAAAARF*s!&rf=viewer_4)
> 如果只考虑前n-1个单词的影响，称为n元语法(n-grams),那么语言模型变为：
> ![](https://m.qpic.cn/psb?/V11thrEZ18EV2M/ruqcInmRICKbhoBQyRGXCVAVhHHU9GoUd7I0PE4uI5U!/b/dDUBAAAAAAAA&bo=0AFHAAAAAAARB6Y!&rf=viewer_4)
> 注意：很多时候我们无法考量太久以前的词，一是因为距离太远的词与当前词关系不大，二是因为距离越长模型参数越多，并且成指数级增长，因此4元以上几乎没人使用。当n=2的时候，就是只考虑前一个单词的一阶马尔科夫链模型，大家都知道在NLP任务中，上下文信息相关性的跨度可能非常大，马尔科夫模型无法处理这样的问题，需要新的模型可以解决这种长程依赖性(Long Distance Dependency)。
> 这里可以回忆一下RNN/LSTM网络，通过隐状态传递信息，可以有效解决长程依赖问题，但当处理很长的序列的时候，它们仍然面临着挑战，即梯度消失。

- 两点马尔可夫性质：[可以理解为无记忆性；留意：NLP问题会涉及哦]

> （1）. 下一个状态的概率分布只与当前状态有关
> ![](https://m.qpic.cn/psb?/V11thrEZ18EV2M/VlLQJYru9cCYXpDnysn3kTOfnC*iVWjZazU*srv20nw!/b/dDYBAAAAAAAA&bo=BAIyAAAAAAARBwQ!&rf=viewer_4)

> （2）. 下一个时刻的观测只与其相对应的状态有关
> ![](http://m.qpic.cn/psb?/V11thrEZ18EV2M/pHu31gXWQnnUuPqUPF.OGld*1N5VtsQ9YAhwVwegRBI!/b/dAYBAAAAAAAA&bo=CgIvAAAAAAARFwc!&rf=viewer_4)

- 最大熵马尔可夫模型为什么会产生标注偏置问题？如何解决？
- HMM为什么是生成模型

> 因为HMM直接对联合概率分布建模；相对而言，条件随机场CRF直接对条件概率建模，所以是判别模型。

- HMM在处理NLP词性标注和实体识别任务中的局限性

> 在序列标注问题中，隐状态（标注）不仅和单个观测状态相关，还 和观察序列的长度、上下文等信息相关。例如词性标注问题中，一个词被标注为 动词还是名词，不仅与它本身以及它前一个词的标注有关，还依赖于上下文中的 其他词

- 隐马尔可夫模型包括概率计算问题、预测问题、学习问题三个基本问题

> （1）概率计算问题：已知模型的所有参数，计算观测序列Y出现的概率，可 使用前向和后向算法求解。
> （2）预测问题：已知模型所有参数和观测序列Y，计算最可能的隐状态序 列X，可使用经典的动态规划算法——维特比算法来求解最可能的状态序列。
> （3）学习问题：已知观测序列Y，求解使得该观测序列概率最大的模型参 数，包括隐状态序列、隐状态之间的转移概率分布以及从隐状态到观测状态的概 率分布，可使用Baum-Welch算法进行参数的学习，Baum-Welch算法是最大期望算 法的一个特例。

- 浅谈最大熵模型

> 最大熵这个词听起来很玄妙，其实就是保留全部的不确定性，将风险降到最小。
> 应用在词性标注，句法分析，机器翻译等NLP任务中。
> ![](https://m.qpic.cn/psb?/V11thrEZ18EV2M/b7zb1D1Obg8wK8WVUacEg*PGY1f5voNT.CQpcGwNTjQ!/b/dFMBAAAAAAAA&bo=tgFVAgAAAAADB8I!&rf=viewer_4)

## 面试真题

---

1. 如何对中文分词问题用HMM模型进行建模的训练？
   ![](https://m.qpic.cn/psb?/V11thrEZ18EV2M/qrxf7RmpPpOope..bx*jIfLkDAarZNo2vV*eUKu1238!/b/dDQBAAAAAAAA&bo=OQIRAwAAAAADBws!&rf=viewer_4)
2. 最大熵HMM模型为什么会产生标注偏置问题，如何解决？
   ![](https://m.qpic.cn/psb?/V11thrEZ18EV2M/xW2pgRjkJbr9ERjCFYtgDV7m0yu5mCJKQiP56pLUFS8!/b/dMQAAAAAAAAA&bo=5QHWBgAAAAADBxY!&rf=viewer_4)

## 参考

1.隐马尔可夫链定义参考维基百科
2.统计学 李航
3.数学之美
4.百面机器学习

---

# 文件：AI算法\machine-learning\kmeans.md

---

Author:Yvette  明明就;Email:yvette.tsai22@gmail.com

# K-means面试题

## 1. 聚类算法（clustering Algorithms）介绍

聚类是一种无监督学习—对大量未知标注的数据集，按数据的内在相似性将数据集划分为多个类别，使类别内的数据相似度较大而类别间的数据相似度较小。

聚类算法可以分为原型聚类（k均值算法（K-means）、学习向量量化、（Learning Vector Quantization -LVQ）、高斯混合聚类（Mixture-of-Gaussian），密度聚类（DBSCAN），层次聚类（AGNES）等。

## 2. kmeans原理详解

K-means是一种常见的聚类算法，也叫k均值或k平均。通过迭代的方式，每次迭代都将数据集中的各个点划分到距离它最近的簇内，这里的距离即数据点到簇中心的距离。

kmean步骤：

1. 随机初始化k个簇中心坐标
2. 计算数据集内所有点到k个簇中心的距离，并将数据点划分近最近的簇
3. 更新簇中心坐标为当前簇内节点的坐标平均值
4. 重复2、3步骤直到簇中心坐标不再改变（收敛了）

## 3.  优缺点及改进算法

优点：效率高、适用于大规模数据集

| 缺点           | 改进      | 描述                                                                                                     |
| -------------- | --------- | -------------------------------------------------------------------------------------------------------- |
| k值的确定      | ISODATA   | 当属于某个簇的样本数过少时把这个簇去除，当属于某个簇的样本数过多、分散程度较大时把这个簇分为两个子簇 |
| 对奇异点敏感   | k-median  | 中位数代替平均值作为簇中心                                                                               |
| 只能找到球状群 | GMM       | 以高斯分布考虑簇内数据点的分布                                                                           |
| 分群结果不稳定 | K-means++ | 初始的聚类中心之间的相互距离要尽可能的远                                                                 |

## 4. k值的选取

K-means算法要求事先知道数据集能分为几群，主要有两种方法定义k。

- 手肘法：通过绘制k和损失函数的关系图，选拐点处的k值。
- 经验选取人工据经验先定几个k，多次随机初始化中心选经验上最适合的。

通常都是以经验选取，因为实际操作中拐点不明显，且手肘法效率不高。

## 5. K-means算法中初始点的选择对最终结果的影响

K-means选择的初始点不同获得的最终分类结果也可能不同，随机选择的中心会导致K-means陷入局部最优解。

## 6. 为什么在计算K-means之前要将数据点在各维度上归一化

因为数据点各维度的量级不同。
举个例子，最近正好做完基于RFM模型的会员分群，每个会员分别有R（最近一次购买距今的时长）、F（来店消费的频率）和M（购买金额）。如果这是一家奢侈品商店，你会发现M的量级（可能几万元）远大于F（可能平均10次以下），如果不归一化就算K-means，相当于F这个特征完全无效。如果我希望能把常客与其他顾客区别开来，不归一化就做不到。

## 7.  K-means不适用哪些数据

1. 数据特征极强相关的数据集，因为会很难收敛（损失函数是非凸函数），一般要用kernal K-means，将数据点映射到更高维度再分群。
2. 数据集可分出来的簇密度不一，或有很多离群值（outliers），这时候考虑使用密度聚类。

## 8.  K-means 中常用的距离度量

K-means中比较常用的距离度量是欧几里得距离和余弦相似度。

## 9. K-means是否会一直陷入选择质心的循环停不下来（为什么迭代次数后会收敛）？

从K-means的第三步我们可以看出，每回迭代都会用簇内点的**平均值**去更新簇中心，所以最终簇内的平方误差和（SSE, sum of squared error）一定最小。 平方误差和的公式如下：

$$
L(X) = \sum_{i=1}^{k}{\sum_{j\in C_i}{(x_{ij}-\bar{x_i})^2}}
$$

## 10. 聚类和分类区别

1. 产生的结果相同（将数据进行分类）
2. 聚类事先没有给出标签（无监督学习）

## 11. 如何对K-means聚类效果进行评估

回到聚类的定义，我们希望得到**簇内数据相似度尽可能地大，而簇间相似度尽可能地小**。常见的评估方式：

| 名称                                 |                                                                                                                 公式                                                                                                                 | 含义                                                        | 如何比较 |
| :----------------------------------- | :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: | :---------------------------------------------------------- | :------- |
| sum of squares within clusters(SSW)  |                                                                                        $\sum_{i=1}^{K}{ \parallel x_i-c_{l_i} \parallel ^2}$                                                                                        | 所有簇内差异之和                                            | 越小越好 |
| sum of squares between clusters(SSB) |                                                                                       $\sum_{i=1}^{K}{n_i \parallel c_i-\bar{x} \parallel ^2}$                                                                                       | 簇心与簇内均值差异的加权和                                  | 越大越好 |
| Calinski-Harabasz                    |                                                                                              $\frac{\frac{SSB}{K-1}}{\frac{SSW}{N-K}}$                                                                                              | 簇间距离和簇内距离之比（除数是惩罚项，因为SSW下降地比较快） | 越大越好 |
| Ball&Hall                            |                                                                                                           $\frac{SSW}{K}$                                                                                                           | 几乎同SSW                                                   | 越小越好 |
| Dunn’s index                        | $\frac{\min_{i=1}^M{\min_{j=i+1}^M{d(c_i, c_j)}}}{\max_{k=1}^M{diam(c_k)}}$ $where d(c_i, c_j)=\min_{x \in c_i, x' \in c_j}{\parallel x-x' \parallel}^2 and$  $diam(c_k)=\max_{x, x' \in c_k}{\parallel x-x' \parallel}^2$ | 本质上也是簇间距离和簇内距离之比                            | 越大越好 |

另一个常见的方法是画图，将不同簇的数据点用不同颜色表示。这么做的好处是最直观，缺点是无法处理高维的数据，它最多能展示三维的数据集。
如果维数不多也可以做一定的降维处理（PCA）后再画图，但会损失一定的信息量。

聚类算法几乎没有统一的评估指标，可能还需要根据聚类目标想评估方式，如对会员作分群以后，我想检查分得的群体之间是否确实有差异，这时候可以用MANOVA计算，当p值小于0.01说明分群合理。

## 12. K-means中空聚类的处理

如果所有的点在指派步骤都未分配到某个簇，就会得到空簇。如果这种情况发生，则需要某种策略来选择一个替补质心，否则的话，平方误差将会偏大。一种方法是选择一个距离当前任何质心最远的点。这将消除当前对总平方误差影响最大的点。另一种方法是从具有最大SEE的簇中选择一个替补的质心。这将分裂簇并降低聚类的总SEE。如果有多个空簇，则该过程重复多次。另外编程实现时，要注意空簇可能导致的程序bug。

## 参考资料

1. Mann A K, Kaur N. Review paper on clustering techniques[J]. Global Journal of Computer Science and Technology, 2013.
2. https://blog.csdn.net/hua111hua/article/details/86556322
3. REZAEI M. Clustering validation[J].

---

# 文件：AI算法\machine-learning\kNN.md

---

# KNN面试题

## 1.简述一下KNN算法的原理

KNN算法利用训练数据集对特征向量空间进行划分。KNN算法的核心思想是在一个含未知样本的空间，可以根据样本最近的k个样本的数据类型来确定未知样本的数据类型。
该算法涉及的3个主要因素是：**k值选择，距离度量，分类决策**。

## 2. 如何理解kNN中的k的取值？

在应用中，k值一般取比较小的值，并采用交叉验证法进行调优。

## 3. 在kNN的样本搜索中，如何进行高效的匹配查找？

线性扫描(数据多时，效率低)
构建数据索引——Clipping和Overlapping两种。前者划分的空间没有重叠，如k-d树；后者划分的空间相互交叠，如R树。（对R树了解很少，可以之后再去了解）

## 4. KNN算法有哪些优点和缺点？

- 优点：
  
  ```
  算法思想较简单，既可以做分类也可以做回归；可以用于非线性分类/回归；训练时间复杂度为O(n)；准确率高，对数据没有假设，对离群点不敏感。
  ```
- 缺点：
  
  ```
  计算量大；存在类别不平衡问题；需要大量的内存，空间复杂度高。
  ```

## 5. 不平衡的样本可以给KNN的预测结果造成哪些问题，有没有什么好的解决方式？

输入实例的K邻近点中，大数量类别的点会比较多，但其实可能都离实例较远，这样会影响最后的分类。 可以使用权值来改进，距实例较近的点赋予较高的权值，较远的赋予较低的权值。

## 6. 为了解决KNN算法计算量过大的问题，可以使用分组的方式进行计算，简述一下该方式的原理。

先将样本按距离分解成组，获得质心，然后计算未知样本到各质心的距离，选出距离最近的一组或几组，再在这些组内引用KNN。     本质上就是事先对已知样本点进行剪辑，事先去除对分类作用不大的样本，该方法比较适用于样本容量比较大时的情况。

##7. K-Means与KNN有什么区别

- KNN
  
  + KNN是分类算法
  + 监督学习
  + 喂给它的数据集是带label的数据，已经是完全正确的数据
  + 没有明显的前期训练过程，属于memory-based learning
  + K的含义：来了一个样本x，要给它分类，即求出它的y，就从数据集中，在x附近找离它最近的K个数据点，这K个数据点，类别c占的个数最多，就把x的label设为c
- K-Means
  
  + 1.K-Means是聚类算法
  + 2.非监督学习
  + 3.喂给它的数据集是无label的数据，是杂乱无章的，经过聚类后才变得有点顺序，先无序，后有序
  + 有明显的前期训练过程
  + K的含义：K是人工固定好的数字，假设数据集合可以分为K个簇，由于是依靠人工定好，需要一点先验知识
- 相似点
  
  - 都包含这样的过程，给定一个点，在数据集中找离它最近的点。即二者都用到了NN(Nears Neighbor)算法，一般用KD树来实现NN。

##9. KD树改进

- Kd-tree在维度较小时（例如：K≤30），算法的查找效率很高，然而当Kd-tree用于对高维数据（例如：K≥100）进行索引和查找时，就面临着维数灾难（curse of dimension）问题，查找效率会随着维度的增加而迅速下降。通常，实际应用中，我们常常处理的数据都具有高维的特点，例如在图像检索和识别中，每张图像通常用一个几百维的向量来表示，每个特征点的局部特征用一个高维向量来表征（例如：128维的SIFT特征）。因此，为了能够让Kd-tree满足对高维数据的索引，Jeffrey S. Beis和David G. Lowe提出了一种改进算法——Kd-tree with BBF（Best Bin First），该算法能够实现近似K近邻的快速搜索，在保证一定查找精度的前提下使得查找速度较快。
- 在介绍BBF算法前，我们先来看一下原始Kd-tree是为什么在低维空间中有效而到了高维空间后查找效率就会下降。在原始kd-tree的最近邻查找算法中（第一节中介绍的算法），为了能够找到查询点Q在数据集合中的最近邻点，有一个重要的操作步骤：**回溯**，该步骤是在未被访问过的且与Q的超球面相交的子树分支中查找可能存在的最近邻点。随着维度K的增大，与Q的超球面相交的超矩形（子树分支所在的区域）就会增加，这就意味着需要回溯判断的树分支就会更多，从而算法的查找效率便会下降很大。
- 从上述标准的kd树查询过程可以看出其搜索过程中的“回溯”是由“查询路径”决定的，并没有考虑查询路径上一些数据点本身的一些性质。一个简单的改进思路就是将“查询路径”上的结点进行排序，如按各自分割超平面（也称bin）与查询点的距离排序，也就是说，回溯检查总是从优先级最高（Best Bin）的树结点开始。
  
  **bbf的算法**:输入：kd树，查找点x.输出：kd树种距离查找点最近的点以及最近的距离
  
  1. 若kd树为空，则设定两者距离为无穷大，返回；如果kd树非空，则将kd树的根节点加入到优先级队列中；
  2. 从优先级队列中出队当前优先级最大的结点，计算当前的该点到查找点的距离是否比最近邻距离小，如果是则更新最近邻点和最近邻距离。如果查找点在切分维坐标小于当前点的切分维坐标，则把他的右孩子加入到队列中，同时检索它的左孩子，否则就把他的左孩子加入到队列中，同时检索它的右孩子。这样一直重复检索，并加入队列，直到检索到叶子节点。然后在从优先级队列中出队优先级最大的结点；
  3. 重复1和1中的操作，直到优先级队列为空，或者超出规定的时间，返回当前的最近邻结点和距离。

## 参考

1. https://blog.csdn.net/weixin_44915167/article/details/89315734
2. https://www.cnblogs.com/nucdy/p/6349172.html
3. https://blog.csdn.net/v_july_v/article/details/8203674
4. https://blog.csdn.net/junshen1314/article/details/51121582
5. https://blog.csdn.net/lhanchao/article/details/52535694
6. https://blog.csdn.net/fool_ran/article/details/85246432
7. 李航 统计学习方法

---

# 文件：AI算法\machine-learning\LightGBM.md

---

![](img/LightGBM/LightGBM.PNG)

# LightGBM面试题

## 1. 简单介绍一下LightGBM？

LightGBM是一个梯度 boosting 框架，使用基于学习算法的决策树。 它可以说是分布式的，高效的。

从 LightGBM 名字我们可以看出其是轻量级（Light）的梯度提升机（GBM），其相对 XGBoost 具有训练速度快、内存占用低的特点。

LightGBM 是为解决GBDT训练速度慢，内存占用大的缺点，此外还提出了：

- 基于Histogram的决策树算法
- 单边梯度采样 Gradient-based One-Side Sampling(GOSS)
- 互斥特征捆绑 Exclusive Feature Bundling(EFB)
- 带深度限制的Leaf-wise的叶子生长策略
- 直接支持类别特征(Categorical Feature)
- 支持高效并行
- Cache命中率优化

## 2. 介绍一下直方图算法？

直方图算法就是使用直方图统计，将大规模的数据放在了直方图中，分别是每个bin中**样本的梯度之和** 还有就是每个bin中**样本数量**

- 首先确定对于每一个特征需要多少个箱子并为每一个箱子分配一个整数；
- 将浮点数的范围均分成若干区间，区间个数与箱子个数相等
- 将属于该箱子的样本数据更新为箱子的值
- 最后用直方图表示

优点：

**内存占用更小**：相比xgb不需要额外存储预排序，且只保存特征离散化后的值(整型)

**计算代价更小**: 相比xgb不需要遍历一个特征值就需要计算一次分裂的增益，只需要计算k次(k为箱子的个数)

**直方图做差加速**：一个叶子的直方图可以由它的父亲节点的直方图与它兄弟的直方图做差得到，在速度上可以提升一倍

## 3. 介绍一下Leaf-wise和 Level-wise？

XGBoost 采用 Level-wise，策略遍历一次数据可以同时分裂同一层的叶子，容易进行多线程优化，也好控制模型复杂度，不容易过拟合。但实际上Level-wise是一种低效的算法，因为它不加区分的对待同一层的叶子，实际上很多叶子的分裂增益较低，没必要进行搜索和分裂

LightGBM采用Leaf-wise的增长策略，该策略每次从当前所有叶子中，找到分裂增益最大的一个叶子，然后分裂，如此循环。因此同Level-wise相比，Leaf-wise的优点是：在分裂次数相同的情况下，Leaf-wise可以降低更多的误差，得到更好的精度；Leaf-wise的缺点是：可能会长出比较深的决策树，产生过拟合。因此LightGBM会在Leaf-wise之上增加了一个最大深度的限制，在保证高效率的同时防止过拟合

## 4. 介绍一下单边梯度采样算法(GOSS)？

GOSS算法从减少样本的角度出发，排除大部分小梯度的样本，仅用剩下的样本计算信息增益，它是一种在减少数据量和保证精度上平衡的算法。与此同时，未了不改变数据的总体分布，GOSS对要进行分裂的特征按照绝对值大小进行排序，选取最大的a个数据，在剩下梯度小的数据中选取b个，这b个数据乘以权重$\frac{1-a}{b}$,最后使用这a+b个数据计算信息增益。

## 5. 介绍互斥特征捆绑算法(EFB)？

互斥特征捆绑算法（Exclusive Feature Bundling, EFB）指出如果将一些特征进行融合绑定，则可以降低特征数量。
LightGBM的EFB算法将这个问题转化为图着色的问题来求解，将所有的特征视为图的各个顶点，将不是相互独立的特征用一条边连接起来，边的权重就是两个相连接的特征的总冲突值，这样需要绑定的特征就是在图着色问题中要涂上同一种颜色的那些点（特征）。另外，算法可以允许一小部分的冲突，我们可以得到更少的绑定特征，进一步提高计算效率。

## 6. 特征之间如何捆绑？

比如，我们在bundle中绑定了两个特征A和B，A特征的原始取值为区间 $[0,10)$，B特征的原始取值为区间$[0,20)$，我们可以在B特征的取值上加一个偏置常量10，将其取值范围变为$[10,30)$，绑定后的特征取值范围为$[0,30)$

## 7. LightGBM是怎么支持类别特征？

* 离散特征建立直方图的过程
  
  统计该特征下每一种离散值出现的次数，并从高到低排序，并过滤掉出现次数较少的特征值, 然后为每一个特征值，建立一个bin容器。
* 计算分裂阈值的过程
  
  * 先看该特征下划分出的bin容器的个数，如果bin容器的数量小于4，直接使用one vs other方式, 逐个扫描每一个bin容器，找出最佳分裂点;
  * 对于bin容器较多的情况, 先进行过滤，只让子集合较大的bin容器参加划分阈值计算, 对每一个符合条件的bin容器进行公式计算
    
    $$
    \frac{该bin容器下所有样本的一阶梯度之和 }{ 该bin容器下所有样本的二阶梯度之和} + 正则项
    $$
* **这里为什么不是label的均值呢？其实"label的均值"只是为了便于理解，只针对了学习一棵树且是回归问题的情况， 这时候一阶导数是Y, 二阶导数是1**)，得到一个值，根据该值对bin容器从小到大进行排序，然后分从左到右、从右到左进行搜索，得到最优分裂阈值。但是有一点，没有搜索所有的bin容器，而是设定了一个搜索bin容器数量的上限值，程序中设定是32，即参数max_num_cat。
* LightGBM中对离散特征实行的是many vs many 策略，这32个bin中最优划分的阈值的左边或者右边所有的bin容器就是一个many集合，而其他的bin容器就是另一个many集合。
* 对于连续特征，划分阈值只有一个，对于离散值可能会有多个划分阈值，每一个划分阈值对应着一个bin容器编号，当使用离散特征进行分裂时，只要数据样本对应的bin容器编号在这些阈值对应的bin集合之中，这条数据就加入分裂后的左子树，否则加入分裂后的右子树。

## 8. LightGBM的优缺点

优点：

- 直方图算法极大的降低了时间复杂度；
- 单边梯度算法过滤掉梯度小的样本，减少了计算量；
- 基于 Leaf-wise 算法的增长策略构建树，减少了计算量；
- 直方图算法将存储特征值转变为存储 bin 值，降低了内存消耗
- 互斥特征捆绑算法减少了特征数量，降低了内存消耗

缺点：

- LightGBM在Leaf-wise可能会长出比较深的决策树，产生过拟合
- LightGBM是基于偏差的算法，所以会对噪点较为敏感；

## 9. GBDT是如何做回归和分类的

- **回归**
  
  生成每一棵树的时候，第一棵树的一个叶子节点内所有样本的label的均值就是这个棵树的预测值，后面根据残差再预测，最后根据将第一棵树的预测值+权重*(其它树的预测结果)
  
  ![image-20210629173116854](../../../../../Library/Application Support/typora-user-images/image-20210629173116854.png)

* **分类**
  
  分类时针对样本有三类的情况，
  
  * 首先同时训练三颗树。
    - 第一棵树针对样本 x 的第一类，输入为（x, 0）。
    - 第二棵树输入针对样本 x 的第二类，假设 x 属于第二类，输入为（x, 1）。
    - 第三棵树针对样本 x 的第三类，输入为（x, 0）。
    - 参照 CART 的生成过程。输出三棵树对 x 类别的预测值 f1(x), f2(x), f3(x)。
  * 在后面的训练中，我们仿照多分类的逻辑回归，使用 softmax 来产生概率。
    - 针对类别 1 求出残差 f11(x) = 0 − f1(x)；
    - 类别 2 求出残差 f22(x) = 1 − f2(x)；
    - 类别 3 求出残差 f33(x) = 0 − f3(x)。
  * 然后第二轮训练，
    - 第一类输入为(x, f11(x))
    - 第二类输入为(x, f22(x))
    - 第三类输入为(x, f33(x))。
  * 继续训练出三棵树，一直迭代 M 轮，每轮构建 3 棵树。当训练完毕以后，新来一个样本 x1，我们需要预测该样本的类别的时候，便可使用 softmax 计算每个类别的概率。

## 参考资料

深入理解LightGBM https://mp.weixin.qq.com/s/zejkifZnYXAfgTRrkMaEww

决策树（下）——XGBoost、LightGBM（非常详细） - 阿泽的文章 - 知乎 https://zhuanlan.zhihu.com/p/87885678

Lightgbm如何处理类别特征： https://blog.csdn.net/anshuai_aw1/article/details/83275299

LightGBM 直方图优化算法：https://blog.csdn.net/jasonwang_/article/details/80833001

---

# 文件：AI算法\machine-learning\metrics.md

---

# 评测指标面试题

metric主要用来评测机器学习模型的好坏程度,不同的任务应该选择不同的评价指标,分类,回归和排序问题应该选择不同的评价函数. 不同的问题应该不同对待,即使都是分类问题也不应该唯评价函数论,不同问题不同分析.

## 回归(Regression)

### 平均绝对误差(MAE)

平均绝对误差MAE（Mean Absolute Error）又被称为 L1范数损失。

$$
MAE=\frac{1}{n}\sum_{i=1}^{n}|y_i-\hat{y}_i| \tag{1}
$$

MAE虽能较好衡量回归模型的好坏，但是绝对值的存在导致函数不光滑，在某些点上不能求导，可以考虑将绝对值改为残差的平方，这就是均方误差。

### 均方误差(MSE)

均方误差MSE（Mean Squared Error）又被称为 L2范数损失 。

$$
MSE=\frac{1}{n}\sum_{i=1}^{n}(y_i-\hat{y}_i)^2 \tag{2}
$$

由于MSE与我们的目标变量的量纲不一致，为了保证量纲一致性，我们需要对MSE进行开方 。

### 均方根误差(RMSE)

$$
RMSE=\sqrt{\frac{1}{n}\sum_{i=1}^{n}(y_i-\hat{y}_i)^2} \tag{3}
$$

### R2_score

$$
R2_score =1-\frac{\sum^n_{i}\left(y_{i}-\hat{y}\right)^{2} / n}{\sum^n_{i}\left(y_{i}-\bar{y}\right)^{2} / n}=1-\frac{M S E}{\operatorname{Var}}	\tag{4}
$$

$R2{_score}$又称决定系数，表示反应因变量的全部变异能通过数学模型被自变量解释的比例, $R2\_{score}$越大，模型准确率越好。

$y$表示实际值,$\hat{y}$表示预测值,$\bar{y}$表示实际值的均值,$n$表示样本数,$i$表示第$i$个样本。$Var$表示实际值的方差，也就是值的变异情况。

$MSE$表示均方误差，为残差平方和的均值,该部分不能能被数学模型解释的部分,属于不可解释性变异。

因此：

$$
可解释性变异占比 = 1-\frac{不可解释性变异}{整体变异}= 1-\frac{M S E}{\operatorname{Var}} = R2\_score	\tag{5}
$$

## 分类(Classification)

### 准确率和错误率

$$
Acc(y,\hat{y})=\frac{1}{n}\sum_{i=1}^{n}y_i=\hat{y_i} \tag{6}
$$

$$
Error(y, \hat{y})=1-acc(y,\hat{y}) \tag{7}
$$

Acc与Error平等对待每个类别，即每一个样本判对 (0) 和判错 (1) 的代价都是一样的。使用Acc与Error作为衡量指标时，需要考虑样本不均衡问题以及实际业务中好样本与坏样本的重要程度。

### 混淆矩阵

对于二分类问题,可将样例根据其真是类别与学习器预测类别的组合划分为：

```
真正例(true positive, TP):预测为 1，预测正确，即实际 1
假正例(false positive, FP):预测为 1，预测错误，即实际 0
真反例(ture negative, TN):预测为 0，预测正确，即实际 0
假反例(false negative, FN):预测为 0，预测错误，即实际 1
```

则有:TP+FP+TN+FN=样例总数. 分类结果的混淆矩阵(confusion matrix)如下:

![image-20210616223053609](img/Metrics/image-20210616223053609.png)

### 精确率（查准率） Precision

Precision 是分类器预测的正样本中预测正确的比例，取值范围为[0,1]，取值越大，模型预测能力越好。

$$
P=\frac{TP}{TP+FP} \tag{8}
$$

### 召回率（查全率）Recall

Recall 是分类器所预测正确的正样本占所有正样本的比例，取值范围为[0,1]，取值越大，模型预测能力越好。

$$
R=\frac{TP}{TP+FN} \tag{9}
$$

### F1 Score

Precision和Recall 是互相影响的，理想情况下肯定是做到两者都高，但是一般情况下Precision高、Recall 就低， Recall 高、Precision就低。为了均衡两个指标，我们可以采用Precision和Recall的加权调和平均（weighted harmonic mean）来衡量，即F1 Score

$$
\frac{1}{F_1}=\frac{1}{2} \cdot (\frac{1}{P}+\frac{1}{R}) \tag{10}
$$

$$
F_1=\frac{2*P*R}{P+R}  \tag{11}
$$

### ROC

ROC全称是"受试者工作特征"(Receiver Operating Characteristic)曲线. ROC曲线为 FPR 与 TPR 之间的关系曲线，这个组合以 FPR 对 TPR，即是以代价 (costs) 对收益 (benefits)，显然收益越高，代价越低，模型的性能就越好。 其中ROC曲线的横轴是"假正例率"(False Positive Rate, **FPR**), 纵轴是"真正例率"(True Positive Rate, **TPR**), **注意这里不是上文提高的P和R**.

- y 轴为真阳性率（TPR）：在所有的正样本中，分类器预测正确的比例（等于Recall）

$$
TPR=\frac{TP}{TP+FN} \tag{12}
$$

- x 轴为假阳性率（FPR）：在所有的负样本中，**分类器预测错误的比例**

$$
FPR=\frac{FP}{TN+FP} \tag{13}
$$

现实使用中,一般使用有限个测试样例绘制ROC曲线,此时需要有有限个(真正例率,假正例率)坐标对. 绘图过程如下:

1. 给定$m^+$个正例和$m^-$个反例,根据学习器预测结果对样例进行排序,然后将分类阈值设为最大,此时真正例率和假正例率都为0,坐标在(0,0)处,标记一个点.
2. 将分类阈值依次设为每个样本的预测值,即依次将每个样本划分为正例.
3. 假设前一个坐标点是(x,y),若当前为真正例,则对应坐标为$(x,y+\frac{1}{m^+})$, 若是假正例,则对应坐标为$(x+\frac{1}{m^-}, y)$
4. 线段连接相邻的点.

ROC曲线如下图(其中对角线对应于"随机猜测"模型):

![image-20210616214012974](img/Metrics/image-20210616214012974.png)

### AUC

对于二分类问题，预测模型会对每一个样本预测一个得分s或者一个概率p。 然后，可以选取一个阈值t，让得分s>t的样本预测为正，而得分s<t的样本预测为负。 这样一来，根据预测的结果和实际的标签可以把样本分为4类,则有混淆矩阵：

|          | 实际为正 | 实际为负 |
| -------- | -------- | -------- |
| 预测为正 | TP       | FP       |
| 预测为负 | FN       | TN       |

随着阈值t选取的不同，这四类样本的比例各不相同。定义真正例率TPR和假正例率FPR为：

$$
\begin{array}{l}
\mathrm{TPR}=\frac{\mathrm{TP}}{\mathrm{TP}+\mathrm{FN}} \\
\mathrm{FPR}=\frac{\mathrm{FP}}{\mathrm{FP}+\mathrm{TN}}
\end{array} \tag{14}
$$

随着阈值t的变化，TPR和FPR在坐标图上形成一条曲线，这条曲线就是ROC曲线。 显然，如果模型是随机的，模型得分对正负样本没有区分性，那么得分大于t的样本中，正负样本比例和总体的正负样本比例应该基本一致。

实际的模型的ROC曲线则是一条上凸的曲线，介于随机和理想的ROC曲线之间。而ROC曲线下的面积，即为AUC！

![image-20210713205533305](img/metrics/image-20210713205533305.png)

这里的x和y分别对应TPR和FPR，也是ROC曲线的横纵坐标。

$$
\mathrm{AUC}=\int_{t=\infty}^{-\infty} y(t) d x(t) \tag{15}
$$

参考：https://tracholar.github.io/machine-learning/2018/01/26/auc.html

### KS Kolmogorov-Smirnov

KS值是在模型中用于**区分预测正负样本分隔程度**的评价指标，一般应用于金融风控领域。与ROC曲线相似，ROC是以FPR作为横坐标，TPR作为纵坐标，通过改变不同阈值，从而得到ROC曲线。ks曲线为TPR-FPR，ks曲线的最大值通常为ks值。可以理解TPR是收益，FPR是代价，ks值是收益最大。图中绿色线是TPR、蓝色线是FPR。

![image-20210616214714505](img/Metrics/image-20210616214714505.png)

KS的计算步骤如下：

1. 按照模型的结果对每个样本进行打分
2. 所有样本按照评分排序，从小到大分为10组（或20组）
3. 计算每个评分区间的好坏样本数。
4. 计算每个评分区间的累计好样本数占总好账户数比率(good%)和累计坏样本数占总坏样本数比率(bad%)。
5. 计算每个评分区间累计坏样本占比与累计好样本占比差的绝对值（累计bad%-累计good%），然后对这些绝对值取最大值即得此评分模型的K-S值。

### CTR（Click-Through-Rate）

CTR即点击通过率,是互联网广告常用的术语,指网络广告（图片广告/文字广告/关键词广告/排名广告/视频广告等）的点击到达率,即该广告的实际点击次数（严格的来说,可以是到达目标页面的数量）除以广告的展现量(Show content).

$$
ctr=\frac{点击次数}{展示量}　\tag{16}
$$

### CVR    (Conversion Rate)

CVR即转化率。是一个衡量CPA广告效果的指标，简言之就是用户点击广告到成为一个有效激活或者注册甚至付费用户的转化率.

$$
cvr=\frac{点击量}{转化量}　\tag{17}
$$

## 参考

1. 周志华 西瓜书
2. 李航 统计学习方法
3. https://baike.baidu.com/item/CVR/20215345
4. https://baike.baidu.com/item/CTR/10653699?fr=aladdin
5. https://www.cnblogs.com/shenxiaolin/p/9309749.html

---

# 文件：AI算法\machine-learning\NaïveBayes.md

---

# 贝叶斯面试题

## 1.简述朴素贝叶斯算法原理和工作流程

**工作原理**：

* 假设现在有样本$x=(x_1, x_2, x_3, \dots x_n)$待分类项
* 假设样本有$m$个特征$(a_1,a_2,a_3,\dots a_m)$(特征独立)
* 再假设现在有分类目标$Y=\{ y_1，y_2，y_3，\dots ,y_n\}$
* 那么就$\max ({P}({y}_1 | {x}), {P}({y}_2 | {x}), {P}({y}_3 | {x}) ,{P}({y_n} | {x}))$是最终的分类类别。
* 而$P(y_i | x)=\frac{P(x | y_i) * P(y_i)}{ P(x)} $，因为$x$对于每个分类目标来说都一样，所以就是求$\max({P}({x}|{y_i})*{P}({y_i}))$
* $P(x | y _i) * P(y_i)=P(y_i) * \prod(P(a_j| y_i))$，而具体的$P(a_j|y_i)$和$P(y_i)$都是能从训练样本中统计出来
* ${P}({a_j} | {y_i})$表示该类别下该特征$a_j$出现的概率$P(y_i)$表示全部类别中这个这个类别出现的概率,这样就能找到应该属于的类别了

## 2. 条件概率、先验概率、后验概率、联合概率、贝叶斯公式的概念

* 条件概率：
  
  * $P(X|Y)$含义： 表示$Y$发生的条件下$X$发生的概率。
* 先验概率
  
  * **表示事件发生前的预判概率。**这个可以是基于历史数据统计，也可以由背景常识得出，也可以是主观观点得出。一般都是单独事件发生的概率，如 $P(X)$
* 后验概率
  
  * 基于先验概率求得的**反向条件概率**，形式上与条件概率相同(若$P(X|Y)$ 为正向，则$P(Y|X)$ 为反向)
* 联合概率：
* 事件$X$与事件$Y$同时发生的概率。
* 贝叶斯公式
  
  * $$
    P(Y|X) = \frac{P(X|Y) P(Y)}{P(X)}  \\
    $$
  * $P(Y)$ 叫做**先验概率**：事件$X$发生之前，我们根据以往经验和分析对事件$Y$发生的一个概率的判断
  * $P(Y|X)$ 叫做**后验概率**：事件$X$发生之后，我们对事件$Y$发生的一个概率的重新评估
  * $P(Y,X)$叫做**联合概率**：事件$X$与事件$Y$同时发生的概率。
  * 先验概率和后验概率是相对的。如果以后还有新的信息引入，更新了现在所谓的后验概率，得到了新的概率值，那么这个新的概率值被称为后验概率。

## 3.为什么朴素贝叶斯如此“朴素”？

因为它**假定所有的特征在数据集中的作用是同样重要和独立的**。正如我们所知，这个假设在现实世界中是很不真实的，因此，说朴素贝叶斯真的很“朴素”。用贝叶斯公式表达如下：

$$
P(Y|X_1, X_2) = \frac{P(X_1|Y) P(X_2|Y) P(Y)}{P(X_1)P(X_2)}
$$

**而在很多情况下，所有变量几乎不可能满足两两之间的条件。**

朴素贝叶斯模型(Naive Bayesian Model)的朴素(Naive)的含义是**“很简单很天真”**地假设样本特征彼此独立.这个假设现实中基本上不存在，但特征相关性很小的实际情况还是很多的，所以这个模型仍然能够工作得很好。

## 4.什么是贝叶斯决策理论？

贝叶斯决策理论是主观贝叶斯派归纳理论的重要组成部分。贝叶斯决策就是在不完全情报下，对部分未知的状态用主观概率估计，然后用贝叶斯公式对发生概率进行修正，最后再利用期望值和修正概率做出最优决策(选择概率最大的类别)。
贝叶斯决策理论方法是统计模型决策中的一个基本方法，其**基本思想**是：

* 已知类条件概率密度参数表达式和先验概率
* 利用贝叶斯公式转换成后验概率
* 根据后验概率大小进行决策分类

## 5.朴素贝叶斯算法的前提假设是什么？

* 特征之间相互独立
* 每个特征同等重要

## 6.为什么属性独立性假设在实际情况中很难成立，但朴素贝叶斯仍能取得较好的效果?

* 对于分类任务来说，只要各类别的条件概率排序正确、无需精准概率值即可导致正确分类；
* 如果属性间依赖对所有类别影响相同，或依赖关系的影响能相互抵消，则属性条件独立性假设在降低计算开销的同时不会对性能产生负面影响。

## 7.什么是朴素贝叶斯中的零概率问题？如何解决？

**零概率问题**：在计算实例的概率时，如果某个量$x$，在观察样本库(训练集)中没有出现过，会导致整个实例的概率结果是0。

**解决办法**：若$P(x)$为零则无法计算。为了解决零概率的问题，法国数学家拉普拉斯最早提出用加1的方法估计没有出现过的现象的概率，所以加法平滑也叫做**拉普拉斯平滑**。

**举个栗子**：假设在文本分类中，有3个类，$C1、C2、C3$，在指定的训练样本中，某个词语$K1$，在各个类中观测计数分别为0，990，10，$K1$的概率为0，0.99，0.01，对这三个量使用拉普拉斯平滑的计算方法如下：

```
1/1003=0.001，
991/1003=0.988，
11/1003=0.011
在实际的使用中也经常使用加 lambda(1≥lambda≥0)来代替简单加1。如果对N个计数都加上lambda，这时分母也要记得加上N*lambda。
```

将朴素贝叶斯中的所有概率计算**应用拉普拉斯平滑即可以解决零概率问题**。

## 8.朴素贝叶斯中概率计算的下溢问题如何解决？

**下溢问题**：在朴素贝叶斯的计算过程中，需要对特定分类中各个特征出现的**概率进行连乘，小数相乘，越乘越小，这样就造成了下溢出**。
为了解决这个问题，对乘积结果取自然对数。通过求对数可以避免下溢出或者浮点数舍入导致的错误。

$$
\prod_{i=x}^{n} p\left(x_{i} | y_{j}\right)
$$

**解决办法**：对其**取对数**：

$$
\log \prod_{i=1}^{n} p\left(x_{i} | y_{j}\right)
$$

$$
=\sum_{i=1}^{n} \log p\left(x_{i} | y_{j}\right)
$$

将小数的乘法操作转化为取对数后的加法操作，规避了变为零的风险同时并不影响分类结果。

## 9.当数据的属性是连续型变量时，朴素贝叶斯算法如何处理？

当朴素贝叶斯算法数据的属性为连续型变量时，有两种方法可以计算属性的类条件概率。

* 第一种方法：把一个连续的属性离散化，然后用相应的离散区间替换连续属性值。但这种方法不好控制离散区间划分的粒度。如果粒度太细，就会因为每个区间内训练记录太少而不能对$P(X|Y)$
  做出可靠的估计，如果粒度太粗，那么有些区间就会有来自不同类的记录，因此失去了正确的决策边界。
* 第二种方法：假设连续变量服从某种概率分布，然后使用训练数据估计分布的参数，例如可以使用高斯分布来表示连续属性的类条件概率分布。
  * 高斯分布有两个参数，均值$\mu$和方差$\sigma 2$，对于每个类$y_i$，属性$X_i$的类条件概率等于：

$$
P\left(X_{i}=x_{i} | Y=y_{j}\right)=\frac{1}{\sqrt{2 \Pi} \sigma_{i j}^{2}} e^{\frac{\left(x_{i}-\mu_{j}\right)^{2}}{2 \sigma_{i}^{2}}}
$$

$\mu_{i j}$：类$y_j$的所有训练记录关于$X_i$的样本均值估计

$\sigma_{i j}^{2}$：类$y_j$的所有训练记录关于$X$的样本方差

通过高斯分布估计出类条件概率。

## 10.朴素贝叶斯有哪几种常用的分类模型？

朴素贝叶斯的三个常用模型：高斯、多项式、伯努利

* 高斯模型：
  
  * 处理包含连续型变量的数据，使用高斯分布概率密度来计算类的条件概率密度
* 多项式模型：
  
  * 其中$\alpha$为拉普拉斯平滑，加和的是属性出现的总次数，比如文本分类问题里面，不光看词语是否在文本中出现，也得看出现的次数。如果总词数为$n$，出现词数为$m$的话，说起来有点像掷骰子$n$次出现$m$次这个词的场景。
    
    $$
    P\left(x_{i} | y_{k}\right)=\frac{N_{y k_{1}}+\alpha}{N_{y_{k}}+\alpha n}
    $$
  * 多项式模型适用于离散特征情况，在文本领域应用广泛， 其基本思想是：**我们将重复的词语视为其出现多次**。
* 伯努利模型：
  
  * 伯努利模型特征的取值为布尔型，即出现为true没有出现为false，在文本分类中，就是一个单词有没有在一个文档中出现。
  * 伯努利模型适用于离散特征情况，它将重复的词语都视为只出现一次。
    
    $$
    P( '代开'， '发票'， '发票'， '我' | S) = P('代开' | S)   P( '发票' | S) P('我' | S)
    $$
    
    我们看到，”发票“出现了两次，但是我们只将其算作一次。我们看到，”发票“出现了两次，但是我们只将其算作一次。

## 11.为什么说朴素贝叶斯是高偏差低方差？

在统计学习框架下，大家刻画模型复杂度的时候，有这么个观点，认为$Error=Bias +Variance$。

* $Error$反映的是整个模型的准确度，
* $Bias$反映的是模型在样本上的输出与真实值之间的误差，即模型本身的精准度，
* $Variance$反映的是模型每一次输出结果与模型输出期望(平均值)之间的误差，即模型的稳定性，数据是否集中。
* 对于复杂模型，充分拟合了部分数据，使得他们的偏差较小，而由于对部分数据的过度拟合，对于部分数据预测效果不好，整体来看可能引起方差较大。
* 对于朴素贝叶斯了。它简单的假设了各个数据之间是无关的，是一个被严重简化了的模型，简单模型与复杂模型相反，大部分场合偏差部分大于方差部分，也就是说高偏差而低方差。

## 12.朴素贝叶斯为什么适合增量计算？

因为朴素贝叶斯在训练过程中实际只需要计算出各个类别的概率和各个特征的类条件概率，这些概率值可以快速的根据增量数据进行更新，无需重新全量训练，所以其十分适合增量计算，该特性可以使用在超出内存的大量数据计算和按小时级等获取的数据计算中。

## 13.高度相关的特征对朴素贝叶斯有什么影响？

假设有两个特征高度相关，相当于该特征在模型中发挥了两次作用(计算两次条件概率)，使得朴素贝叶斯获得的结果向该特征所希望的方向进行了偏移，影响了最终结果的准确性，所以朴素贝叶斯算法应先处理特征，把相关特征去掉。

## 14.朴素贝叶斯的应用场景有哪些？

* **文本分类/垃圾文本过滤/情感判别**：
  这大概是朴素贝叶斯应用最多的地方了，即使在现在这种分类器层出不穷的年代，在文本分类场景中，朴素贝叶斯依旧坚挺地占据着一席之地。因为多分类很简单，同时在文本数据中，分布独立这个假设基本是成立的。而垃圾文本过滤(比如垃圾邮件识别)和情感分析(微博上的褒贬情绪)用朴素贝叶斯也通常能取得很好的效果。
* **多分类实时预测**：
  对于文本相关的多分类实时预测，它因为上面提到的优点，被广泛应用，简单又高效。
* **推荐系统**：
  朴素贝叶斯和协同过滤是一对好搭档，协同过滤是强相关性，但是泛化能力略弱，朴素贝叶斯和协同过滤一起，能增强推荐的覆盖度和效果。

## 15.朴素贝叶斯有什么优缺点？

* 优点：
  * 对数据的训练快，分类也快
  * 对缺失数据不太敏感，算法也比较简单
  * 对小规模的数据表现很好，能个处理多分类任务，适合增量式训练，尤其是数据量超出内存时，可以一批批的去增量训练
* 缺点：
  * 对输入数据的表达形式很敏感
  * 由于朴素贝叶斯的“朴素”特点，所以会带来一些准确率上的损失。
  * 需要计算先验概率，分类决策存在错误率。

## 16.朴素贝叶斯与 LR 区别？

- **朴素贝叶斯是生成模型**，根据已有样本进行贝叶斯估计学习出先验概率 $P(Y)$ 和条件概率 $P(X|Y)$，进而求出联合分布概率 $P(X,Y)$，最后利用贝叶斯定理求解$P(Y|X)$， 而**LR是判别模型**，根据极大化对数似然函数直接求出条件概率 $P(Y|X)$
- 朴素贝叶斯是基于很强的**条件独立假设**(在已知分类Y的条件下，各个特征变量取值是相互独立的)，而 LR 则对此没有要求
- 朴素贝叶斯适用于数据集少的情景，而LR适用于大规模数据集。

## 17. 贝叶斯优化算法(参数调优)

* 网格搜索和随机搜索：在测试一个新点时，会忽略前一个点的信息；
* 贝叶斯优化算法：充分利用了之前的信息。贝叶斯优化算法通过对目标函数形式进行学习，找到使目标函数向全局最优值提升的参数。
* 学习目标函数形式的方法：
  
  * 首先根据先验分布，假设一个搜集函数；
  * 每一次使用新的采样点来测试目标函数时，利用这个信息来更新目标函数的先验分布
  * 算法测试由后验分布给出的全局最值最可能出现的位置的点。

对于贝叶斯优化算法，有一个需要注意的地方，一旦找到了一个局部最优值，它会在该区域不断采样，所以很容易陷入局部最优值。为了弥补这个缺陷，贝叶斯优化算法会在探索和利用之间找到一个平衡点，“探索”就是在还未取样的区域获取采样点；而“利用”则是根据后验分布在最可能出现全局最值的区域进行采样。

## 18.朴素贝叶斯分类器对异常值敏感吗?

朴素贝叶斯是一种**对异常值不敏感**的分类器，保留数据中的异常值，常常可以保持贝叶斯算法的整体精度，如果对原始数据进行降噪训练，分类器可能会因为失去部分异常值的信息而导致泛化能力下降。

## 19.朴素贝叶斯算法对缺失值敏感吗？

朴素贝叶斯是一种**对缺失值不敏感**的分类器，朴素贝叶斯算法能够处理缺失的数据，在算法的建模时和预测时数据的属性都是单独处理的。因此**如果一个数据实例缺失了一个属性的数值，在建模时将被忽略**，不影响类条件概率的计算，在预测时，计算数据实例是否属于某类的概率时也将忽略缺失属性，不影响最终结果。

## 20. 一句话总结贝叶斯算法

**贝叶斯分类器直接用贝叶斯公式解决分类问题**。假设样本的特征向量为$x$，类别标签为$y$，根据贝叶斯公式，样本属于每个类的条件概率（后验概率）为：

$$
p(y | \mathbf{x})=\frac{p(\mathbf{x} | y) p(y)}{p(\mathbf{x})}
$$

分母$p(x)$对所有类都是相同的，**分类的规则是将样本归到后验概率最大的那个类**，不需要计算准确的概率值，只需要知道属于哪个类的概率最大即可，这样可以忽略掉分母。分类器的判别函数为：

$$
\arg \max _{y} p(\mathrm{x} | y) p(y)
$$

在实现贝叶斯分类器时，**需要知道每个类的条件概率分布$p(x|y)$即先验概率**。一般假设样本服从正态分布。训练时确定先验概率分布的参数，一般用最大似然估计，即最大化对数似然函数。

**贝叶斯分类器是一种生成模型，可以处理多分类问题，是一种非线性模型。**

## 21.朴素贝叶斯与LR的区别？（经典问题）

朴素贝叶斯是生成模型，而LR为判别模型.朴素贝叶斯：已知样本求出先验概率与条件概率，进而计算后验概率。**优点：样本容量增加时，收敛更快；隐变量存在时也可适用。缺点：时间长；需要样本多；浪费计算资源**.     **Logistic回归**：不关心样本中类别的比例及类别下出现特征的概率，它直接给出预测模型的式子。设每个特征都有一个权重，训练样本数据更新权重w，得出最终表达式。**优点：直接预测往往准确率更高；简化问题；可以反应数据的分布情况，类别的差异特征；适用于较多类别的识别。缺点：收敛慢；不适用于有隐变量的情况。**    > + 朴素贝叶斯是基于很强的条件独立假设（在已知分类Y的条件下，各个特征变量取值是相互独立的），而LR则对此没有要求。    > + 朴素贝叶斯适用于数据集少的情景，而LR适用于大规模数据集。

---

# 文件：AI算法\machine-learning\Prophet.md

---

## Prophet面试题

## 1. 简要介绍Prophet

常见的时间序列分解方法：

将时间序列分成季节项$S_t$，趋势项$T_t$，剩余项$R_t$，即对所有的$t≥0$

$$
y_{t}=S_{t}+T_{t}+R_{t}
$$

$$
y_{t}=S_{t} \times T_{t} \times R_{t}
$$

$$
\ln y_{t}=\ln S_{t}+\ln T_{t}+\ln R_{t}
$$

fbprophet 的在此基础上，添加了节日项。

$$
y(t)=g(t)+s(t)+h(t)+\epsilon_{t}
$$

## 2. 趋势项模型

* **基于逻辑回归**
  
  sigmoid 函数为
  
  $$
  \sigma(x)=1 /\left(1+e^{-x}\right)
  $$
  
  prophet在逻辑回归的基础上添加了随时间变化的参数，那么逻辑回归就可以改写成：
  
  $$
  f(x)=\frac{C(t)}{\left(1+e^{-k(t)(x-m(t))}\right)}
  $$
  
  这里的 $C$ 称为曲线的最大渐近值， $k$ 表示曲线的增长率，$m$  表示曲线的中点。当 $$
  C=1, k=1, m=0
  
  $$
  时，恰好就是大家常见的 sigmoid 函数的形式。
  $$
* **基于分段线性函数**
  
  $$
  g(t)=\frac{C(t)}{1+\exp \left(-\left(k+\boldsymbol{a}(t)^{t} \boldsymbol{\delta}\right) \cdot\left(t-\left(m+\boldsymbol{a}(t)^{T} \boldsymbol{\gamma}\right)\right.\right.}
  $$
  
  $k$表示变化量
  
  $a_{j}(t)$表示指示函数：
  
  $$
  a_{j}(t)=\left\{\begin{array}{l}1, \text { if } t \geq s_{j} \\ 0, \text { otherwise }\end{array}\right.
  $$
  
  $\delta_{j}$表示在时间戳$s_{j}$上的增长率的变化量
  
  $\gamma_{j}$确定线段边界
  
  $$
  \gamma_{j}=\left(s_{j}-m-\sum_{\ell<j} \gamma_{\ell}\right) \cdot\left(1-\frac{k+\sum_{\ell<j} \delta_{\ell}}{k+\sum_{\ell \leq j} \delta_{\ell}}\right)
  $$
  
  其中：
  
  $$
  \boldsymbol{a}(t)=\left(a_{1}(t), \cdots, a_{S}(t)\right)^{T}, \boldsymbol{\delta}=\left(\delta_{1}, \cdots, \delta_{S}\right)^{T}, \boldsymbol{\gamma}=\left({\gamma}_{1}, \cdots, \gamma_{S}\right)^{T}
  $$

## 3. 变点的选择

在 Prophet 算法中，需要给出变点的位置，个数，以及增长的变化率：

- changepoint_range
  
  changepoint_range 指的是百分比，需要在前 changepoint_range 那么长的时间序列中设置变点
- n_changepoint
  
  n_changepoint 表示变点的个数，在默认的函数中是 n_changepoint = 25
- changepoint_prior_scale。
  
  changepoint_prior_scale 表示变点增长率的分布情况
  
  $$
  \delta_{j} \sim \operatorname{Laplace}(0, \tau)
  $$
  
  $\mathcal{T}$就是 change_point_scale

## 4. 对未来的预估

对于已知的时间序列，可以手动设置s个变点

对于预测的数据模型使用Poisson分布找到新增的变点，然后与已知的变点进行拼接

## 5. 季节性趋势

时间序列通常会随着天，周，月，年等季节性的变化而呈现季节性的变化，也称为周期性的变化

prophet算法使用傅立叶级数来模拟时间序列的周期性

$P$表示时间序列的周期， $P = 365.25$表示以年为周期，$P = 7$表示以周为周期。它的傅立叶级数的形式都是：

$$
s(t)=\sum_{n=1}^{N}\left(a_{n} \cos \left(\frac{2 \pi n t}{P}\right)+b_{n} \sin \left(\frac{2 \pi n t}{P}\right)\right)
$$

## 6. 节假日效应（holidays and events）

除了周末，同样有很多节假日，而且不同的国家有着不同的假期，不同的节假日可以看成相互独立的模型，并且可以为不同的节假日设置不同的前后窗口值，表示该节假日会影响前后一段时间的时间序列。

$$
h(t)=Z(t) \boldsymbol{\kappa}=\sum_{i=1}^{L} \kappa_{i} \cdot 1_{\left\{t \in D_{i}\right\}}
$$

其中：$Z(t)=\left(1_{\left\{t \in D_{1}\right\}}, \cdots, 1_{\left\{t \in D_{L}\right\}}\right), \boldsymbol{\kappa}=\left(\kappa_{1}, \cdots, \kappa_{L}\right)^{T}$，$\boldsymbol{\kappa} \sim \operatorname{Normal}\left(0, v^{2}\right)$

并且该正态分布是受到$v$ = holidays_prior_scale 这个指标影响的。默认值是 10，当值越大时，表示节假日对模型的影响越大；当值越小时，表示节假日对模型的效果越小

## 7. 参数

在 Prophet 中，用户一般可以设置以下四种参数：

1. Capacity：在增量函数是逻辑回归函数的时候，需要设置的容量值。
2. Change Points：可以通过 n_changepoints 和 changepoint_range 来进行等距的变点设置，也可以通过人工设置的方式来指定时间序列的变点。
3. 季节性和节假日：可以根据实际的业务需求来指定相应的节假日。
4. 光滑参数：
   
   $\tau$ = changepoint_prior_scale 可以用来控制趋势的灵活度
   
   $\sigma$ = seasonality_prior_scale 用来控制季节项的灵活度，
   
   $v$ =  holidays prior scale 用来控制节假日的灵活度。

## 参考资料

https://zhuanlan.zhihu.com/p/52330017

---

# 文件：AI算法\machine-learning\RandomForest.md

---

# 随机森林面试题

## 1. 简单介绍随机森林

一种基于树模型的Bagging的优化版本，一棵树的生成肯定还是不如多棵树，因此就有了随机森林，解决决策树泛化能力弱的特点。

**多次随机取样，多次随机取属性，选取最优分割点，构建多个(CART)分类器，投票表决**

**算法流程：**

* 输入为样本集$D=\{(x，y_1)，(x_2，y_2) \dots (x_m，y_m)\}$，弱分类器迭代次数$T$。
* 输出为最终的强分类器$f(x)$
* 对于$t=1，2 \dots T$
  
  * 对训练集进行第$t$次随机采样，共采集$m$次，得到包含$m$个样本的采样集Dt
  * 用采样集$D_t$训练第$t$个决策树模型$G_t(x)$，在训练决策树模型的节点的时候，在节点上所有的样本特征中选择一部分样本特征，在这些随机选择的部分样本特征中选择一个最优的特征来做决策树的左右子树划分
* 如果是分类算法预测，则$T$个弱学习器投出最多票数的类别或者类别之一为最终类别。如果是回归算法，$T$个弱学习器得到的回归结果进行算术平均得到的值为最终的模型输出。

## 2. 随机森林的随机性体现在哪里？

**多次有放回的随机取样，多次随机选择特征**

## 3. 随机森林为什么不容易过拟合？

* 随机森林中的每一颗树都是过拟合的，拟合到非常小的细节上
* 随机森林通过引入随机性，使每一颗树拟合的细节不同
* 所有树组合在一起，过拟合的部分就会自动被消除掉。

因此随机森林出现过拟合的概率相对低。

## 4. 为什么不用全样本训练？

全样本训练忽视了局部样本的规律（各个决策树趋于相同），对于模型的泛化能力是有害的，使随机森林算法在样本层面失去了随机性。

## 5. 为什么要随机特征？

随机特征保证基分类器的多样性（差异性），最终集成的泛化性能可通过个体学习器之间的差异度而进一步提升，从而提高泛化能力和抗噪能力。

## 6. RF与 GBDT 的区别？

* 随机森林将多棵决策树的结果进行投票后得到最终的结果，对不同的树的训练结果也没有做进一步的优化提升，将其称为**Bagging算法。**
* GBDT用到的是**Boosting算法**，在迭代的每一步构建弱学习器弥补原有模型的不足。GBDT中的Gradient Boost就是通过每次迭代的时候构建一个沿梯度下降最快的方向的学习器。

## 7. RF为什么比Bagging效率高？

因为在个体决策树的构建过程中，Bagging使用的是“确定型”决策树，bagging在选择划分属性时要对每棵树是对所有特征进行考察；而随机森林仅仅考虑一个特征子集。

## 8. 你已经建了一个有10000棵树的随机森林模型。在得到0.00的训练误差后，你非常高兴。但是，验证错误是34.23。到底是怎么回事？你还没有训练好你的模型吗？

- 模型过拟合十分严重
- 新的测试集与训练集的数据分布不一致

## 9. 如何使用随机森林对特征重要性进行评估？

**袋外数据(OOB)**： 大约有1/3的训练实例没有参与第k棵树的生成，它们称为第$k$棵树的袋外数据样本。

在随机森林中某个特征$X$的重要性的计算方法如下：

* 对于随机森林中的每一颗决策树，使用相应的OOB(袋外数据)来计算它的袋外数据误差，记为$err_{OOB1}$。
* 随机地对袋外数据OOB所有样本的特征$X$加入噪声干扰(就可以随机的改变样本在特征X处的值)，再次计算它的袋外数据误差，记为$err_{OOB2}$。
* 假设随机森林中有$N$棵树，那么对于特征$X$的重要性为$(err_{OOB2}-err_{OOB1}/N)$，之所以可以用这个表达式来作为相应特征的重要性的度量值是因为：若给某个特征随机加入噪声之后，袋外的准确率大幅度降低，则说明这个特征对于样本的分类结果影响很大，也就是说它的重要程度比较高。

## 10. 随机森林算法训练时主要需要调整哪些参数？

* **n_estimators:**随机森林建立子树的数量。
  较多的子树一般可以让模型有更好的性能，但同时让你的代码变慢。需要选择最佳的随机森林子树数量
* **max_features：**随机森林允许单个决策树使用特征的最大数量。
  增加max_features一般能提高模型的性能，因为在每个节点上，我们有更多的选择可以考虑。然而，这未必完全是对的，因为它降低了单个树的多样性，而这正是随机森林独特的优点。但是，可以肯定，你通过增加max_features会降低算法的速度。因此，你需要适当的平衡和选择最佳max_features。
* **max_depth：** 决策树最大深度
  
  默认决策树在建立子树的时候不会限制子树的深度
* **min_samples_split：**内部节点再划分所需最小样本数
  内部节点再划分所需最小样本数，如果某节点的样本数少于min_samples_split，则不会继续再尝试选择最优特征来进行划分。
* **min_samples_leaf：** 叶子节点最少样本
  
  这个值限制了叶子节点最少的样本数，如果某叶子节点数目小于样本数，则会和兄弟节点一起被剪枝。
* **max_leaf_nodes：** 最大叶子节点数
  
  通过限制最大叶子节点数，可以防止过拟合，默认是"None”，即不限制最大的叶子节点数。如果加了限制，算法会建立在最大叶子节点数内最优的决策树。
* **min_impurity_split：** 节点划分最小不纯度
  这个值限制了决策树的增长，如果某节点的不纯度（基于基尼系数，均方差）小于这个阈值，则该节点不再生成子节点。即为叶子节点。一般不推荐改动默认值1e-7。

## 11. 随机森林的优缺点

- 优点
  
  - 训练可以高度并行化，对于大数据时代的大样本训练速度有优势。个人觉得这是的最主要的优点。
  - 由于可以随机选择决策树节点划分特征，这样在样本特征维度很高的时候，仍然能高效的训练模型。
  - 在训练后，可以给出各个特征对于输出的重要性
  - 由于采用了随机采样，训练出的模型的方差小，泛化能力强。
  - 相对于Boosting系列的Adaboost和GBDT， RF实现比较简单。
  - 对部分特征缺失不敏感，如果有很大一部分的特征遗失，仍可以维持准确度。
- 缺点
  
  - 在某些噪音比较大的样本集上，RF模型容易陷入过拟合。
  - 取值划分比较多的特征容易对RF的决策产生更大的影响，从而影响拟合的模型的效果。

## 12. 简述一下Adaboost原理

Adaboost算法利用同一种基分类器（弱分类器），基于分类器的错误率分配不同的权重参数，最后累加加权的预测结果作为输出。

* Adaboost算法流程：
  * 样本赋予权重，得到第一个分类器。
  * 计算该分类器的错误率，根据错误率赋予分类器权重（注意这里是**分类器的权重**）。
  * 增加分错样本的权重，减小分对样本的权重（注意这里是**样本的权重**）。
  * 然后再用**新的样本权重**训练数据，得到新的分类器。
  * 多次迭代，直到分类器错误率为0或者整体弱分类器错误为0，或者到达迭代次数。
  * 将所有弱分类器的结果加权求和，得到一个较为准确的分类结果。错误率低的分类器获得更高的决定系数，从而在对数据进行预测时起关键作用。

## 13. AdaBoost的优点和缺点

* 优点
  * Adaboost提供一种框架，在框架内可以使用各种方法构建子分类器。可以使用简单的弱分类器，不用对特征进行筛选，也不存在过拟合的现象。
  * Adaboost算法不需要弱分类器的先验知识，最后得到的强分类器的分类精度依赖于所有弱分类器。无论是应用于人造数据还是真实数据，Adaboost都能显著的提高学习精度。
  * Adaboost算法不需要预先知道弱分类器的错误率上限，且最后得到的强分类器的分类精度依赖于所有弱分类器的分类精度，可以深挖分类器的能力。
  * Adaboost可以根据弱分类器的反馈，自适应地调整假定的错误率，执行的效率高。
  * Adaboost对同一个训练样本集训练不同的弱分类器，按照一定的方法把这些弱分类器集合起来，构造一个分类能力很强的强分类器，即“三个臭皮匠赛过一个诸葛亮"”。
* 缺点
  * 在Adaboost训练过程中，Adaboost会使得难于分类样本的权值呈指数增长，训练将会过于偏向这类困难的样本，导致Adaboost算法易受噪声干扰。
  * Adaboost依赖于弱分类器，而弱分类器的训练时间往往很长。

## 14. Adaboost对噪声敏感吗？

在Adaboost训练过程中，Adaboost会使得难于分类样本的权值呈指数增长，训练将会过于偏向这类困难的样本，导致Adaboost算法易受噪声干扰。

## 15. Adaboost和随机森林算法的异同点

随机森林和Adaboost算法都可以用来分类，它们都是优秀的基于决策树的组合算法。

* 相同之处
  * 二者都是Bootstrap自助法选取样本。
  * 二者都是要训练很多棵决策树。
* 不同之处
  * Adaboost是基于Boosting的算法，随机森林是基于Bagging的算法。
  * Adaboost后面树的训练，其在变量抽样选取的时候，对于上一棵树分错的样本，抽中的概率会加大。
  * 随机森林在训练每一棵树的时候，随机挑选了部分特征作为拆分特征，而不是所有的特征都去作为拆分特征。
  * 在预测新数据时，Adaboost中所有的树加权投票来决定因变量的预测值，每棵树的权重和错误率有关；随机森林按照所有树中少数服从多数树的分类值来决定因变量的预测值（或者求取树预测的平均值）。

---

# 文件：AI算法\machine-learning\SVM.md

---

# SVM面试题

## 1. SVM直观解释

SVM，Support Vector Machine，它是一种二分类模型，其基本模型定义为特征空间上的间隔最大的线性分类器，间隔最大使它有别于感知机；其还包括**核技巧**，这使它成为实质上的非线性分类器。其学习策略就是间隔最大化，可形式化为一个求解凸二次规划的问题。其学习算法就是求解凸二次规划的最优化算法。

这里涉及了几个概念，**二分类模型**，**线性分类器**，**间隔最大化**，**凸二次规划问题**。

- 二分类模型：给定的各个样本数据分别属于两个类之一，而目标是确定新数据点将归属到哪个类中。
- 线性分类器：分割样本点的分类器是一个超平面，这也就要求样本线性可分，这是hard-margin SVM的要求，对于后来的soft-margin SVM，放低为近似线性可分，再到后来的核技巧，要求映射到高维空间后要近似线性可分。
- 线性可分：$D0$和$D1$是$n$维欧氏空间中的两个点集（点的集合）。如果存在 $n$维向量 $w$和实数$b$，使得所有属于$D0$的点 xi 都有 $wx_i+b>0$，而对于所有属于$D1$的点 $x_j$则有 $wx_j+b<0$。则我们称$D0$和$D1$线性可分。
- 间隔最大化：首先要知道SVM中有函数间隔和几何间隔，函数间隔刻画样本点到超平面的相对距离，几何间隔刻画的是样本点到超平面的绝对距离，SVM的直观目的就是找到最小函数距离的样本点，然后最大化它的几何间隔。
- 凸二次规划：目标函数是二次的，约束条件是线性的。

## 2. 核心公式

- 线性可分训练集：$T=\left\{\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right), \ldots,\left(x_{n}, y_{n}\right)\right\}$
- 学习得到的超平面：$w^{* T} x+b^{*}=0$
- 相应的分类决策函数：$f(x)=\operatorname{sign}\left(w^{* T} x+b^{*}\right)$
- SVM基本思想：间隔最大化，不仅要讲正负类样本分开，而且对最难分的点（离超平面最近的点）也要有足够大的确信度将他们分开。

![在这里插入图片描述](img/SVM/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1NTQ3Mjgx,size_16,color_FFFFFF,t_70.png)![在这里插入图片描述](img/SVM/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1NTQ3Mjgx,size_16,color_FFFFFF,t_70-20211031095607863-5645369.png)

**函数间隔**

给定一个超平面$（w, b）$，定义该超平面关于样本点 $(x_i,y_i )$ 的函数间隔为：$\widehat{\gamma}_{i}=y_{i}\left(w^{T} x_{i}+b\right)$
定义该超平面关于训练集$T$的函数间隔为：$\widehat{\gamma}=\min _{i=1,2, \ldots, N} \widehat{\gamma}_{i}$

**几何间隔**

给定一个超平面$（w, b）$，定义该超平面关于样本点 $(x_i,y_i )$ 的几何间隔为：$\gamma_{i}=y_{i}\left(\frac{w^{T}}{\|w\|} x_{i}+\frac{b}{\|w\|}\right)$
定义该超平面关于训练集$T$的几何间隔为：$\gamma=\min _{i=1,2, \ldots, N} \gamma_{i}$

**函数间隔与几何间隔的关系**

$\begin{array}{c}{\gamma_{i}=\frac{\hat{\gamma}_{i}}{\|w\|}, i=1,2, \ldots, N} \\ {\gamma=\frac{\hat{\gamma}}{\|w\|}}\end{array}$

**间隔最大化**

求得一个几何间隔最大的分离超平面，可以表示为如下的最优化问题：
$\begin{array}{c}{\max _{w, b} \gamma} \\ {\text {s.t.} y_{i}\left(\frac{w^{T}}{\|w\|} x_{i}+\frac{b}{\|w\|}\right) \geq \gamma, i=1,2, \ldots, N}\end{array}$

考虑函数间隔与几何间隔的关系式，改写为：

$\begin{array}{c}{\max _{w, b} \frac{\hat{\gamma}}{\|w\|}} \\ {\text {s.t. } y_{i}\left(w^{T} x_{i}+b\right) \geq \hat{\gamma}, i=1,2, \ldots, N}\end{array}$

等价与下式

$\begin{array}{c}{\max _{w, b} \frac{1}{\|w\|}} \\ {\text {s.t. } 1-y_{i}\left(w^{T} x_{i}+b\right) \leq 0, i=1,2, \ldots, N}\end{array}$

注意到最大化$\frac{1}{\|w\|}$ 和最小化$\frac{1}{2}\|w\|^{2}$是等价的，故最优化问题可转化为：

$\begin{array}{c}{\min _{w, b} \frac{1}{2}\|w\|^{2}} \\ {\text {s.t. } 1-y_{i}\left(w^{T} x_{i}+b\right) \leq 0, i=1,2, \ldots, N}\end{array}$

构造Lagrange函数：
$\begin{aligned} L(w, b, \alpha)=& \frac{1}{2}\|w\|^{2}+\sum_{i=1}^{N} \alpha_{i}\left[1-y_{i}\left(w^{T} x_{i}+b\right)\right] \\ \alpha_{i} & \geq 0, i=1,2, \ldots, N \end{aligned}$

令$\theta_{\alpha}(w, b)=\max _{\alpha_{i} \geq 0} L(w, b, \alpha)$

则有$\theta_{\alpha}(w, b)=\left\{\begin{array}{c}{\frac{1}{2}\|w\|^{2},当全部约束满足} \\ {+\infty，当存在约束不满足}\end{array}\right.$

故原问题等价于
$\min _{w, b} \theta_{\alpha}(w, b)=\min _{w, b} \max _{\alpha_{i} \geq 0} L(w, b, \alpha)$

**对偶算法**

根据拉格朗日对偶性，上式的对偶问题为：
$\min _{w, b} \theta_{\alpha}(w, b)= \max _{\alpha_{i} \geq 0}\min _{w, b} L(w, b, \alpha)$

（1）求$\min _{w, b} L(w, b, \alpha)$

$\nabla_{w} L(w, b, \alpha)=w-\sum_{i=1}^{N} \alpha_{i} y_{i} x_{i}=0$

$\nabla_{b} L(w, b, \alpha)=-\sum_{i=1}^{N} \alpha_{i} y_{i}=0$

得

$w=\sum_{i=1}^{N} \alpha_{i} y_{i} x_{i}$

$\sum_{i=1}^{N} \alpha_{i} y_{i}=0$

将以上两式代入拉格朗日函数中消去，得
$\begin{aligned} L(w, b, \alpha) &=-\frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left\langle x_{i}, x_{j}\right\rangle+\sum_{i=1}^{\mathrm{N}} \alpha_{i} \end{aligned}$

（2）求$\min _{w, b} L(w, b, \alpha)$求对$\alpha$的极大，即是对偶问题

$\begin{aligned} \max _{\alpha} &-\frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left\langle x_{i}, x_{j}\right\rangle+\sum_{i=1}^{\mathrm{N}} \alpha_{i} \\ \text {s.t.} & \sum_{i=1}^{N} \alpha_{i} y_{i}=0 \\ \alpha_{i} & \geq 0, i=1,2, \ldots, N \end{aligned}$

将极大改为极小，得

${\min _{\alpha} \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left\langle x_{i}, x_{j}\right\rangle-\sum_{i=1}^{\mathrm{N}} \alpha_{i}}$

$\sum_{i=1}^{N} \alpha_{i} y_{i}=0$

$\alpha_{i} \geq 0, i=1,2, \ldots, N$

原问题的对偶问题：
$\begin{aligned} \min _{\alpha} & \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left\langle x_{i}, x_{j}\right\rangle-\sum_{i=1}^{\mathrm{N}} \alpha_{i} \\ \text {s.t.} & \sum_{i=1}^{N} \alpha_{i} y_{i}=0  \\ & \alpha_{i} \geq 0, i=1,2, \ldots, N \end{aligned}$

求解方法：
（1）由于该问题为凸优化问题，故可直接求解。
（2）由于该问题与其原问题等价，其原问题满足Slater定理，故该问题的解与KKT条件为充分必要的关系，故只需找到一组解满足KKT条件，即找到了问题的解（充分性）。

关于对偶问题的解$\alpha^{*}=\left(\alpha_{1}^{*}, \alpha_{2}^{*}, \ldots, \alpha_{N}^{*}\right)$，是由SMO算法解出来的，这个结合加入松弛变量的情况再讲。

解出对偶问题的解$\alpha^{*}=\left(\alpha_{1}^{*}, \alpha_{2}^{*}, \ldots, \alpha_{N}^{*}\right)$后，怎么求原问题的解$w^{*}, b^{*}$？

由KKT条件可知，原问题和对偶问题均取到最优值的解$\left(w^{*}, b^{*}, \alpha^{*}\right)$需满足以下四个要求：

1. 对原始变量梯度为0：
   $\nabla_{w} L\left(w^{*}, b^{*}, \alpha^{*}\right)=w^{*}-\sum_{i=1}^{N} \alpha_{i}^{*} y_{i} x_{i}=0$
   $\nabla_{b} L\left(w^{*}, b^{*}, \alpha^{*}\right)=-\sum_{i=1}^{N} \alpha_{i}^{*} y_{i}=0$
2. 原问题可行：
   $1-y_{i}\left(w^{* T} x_{i}+b^{*}\right) \leq 0, i=1,2, \ldots, N$
3. 不等式约束乘子非负:
   $\alpha_{i}^{*} \geq 0, i=1,2, \ldots, N$
4. 对偶互补松弛：
   $\alpha_{i}^{*}\left[1-y_{i}\left(w^{* T} x_{i}+b^{*}\right)\right]=0, i=1,2, \dots, N$

由于1中
$\nabla_{w} L\left(w^{*}, b^{*}, \alpha^{*}\right)=w^{*}-\sum_{i=1}^{N} \alpha_{i}^{*} y_{i} x_{i}=0$

得到
$w^{*}=\sum_{i=1}^{N} \alpha_{i}^{*} y_{i} x_{i}$
这样$w^{*}$就求出来了

用反证法我们可以得到至少有一个$\alpha_{i}^{*}>0$，假设所有的$\alpha_{i}^{*}=0$，由$w^{*}-\sum_{i=1}^{N} \alpha_{i}^{*} y_{i} x_{i}=0$知道，$w^{*}=0$，而$w^{*}=0$显然不是原问题的解，我们要零解一点意义都没有。

接下来，求$b^{*}$
取$\alpha_{i}^{*}$ 的一个分量满足$\alpha_{i}^{*}>0$ ，则有对应的由4中的 $\alpha_{i}^{*}\left[1-y_{i}\left(w^{* T} x_{i}+b^{*}\right)\right]=0, i=1,2, \dots, N$，有$1-y_{j}\left(w^{* T} x_{j}+b^{*}\right)=0$

代入$w^{*}$和样本点$(x_j,y_j)$，求出
$b^{*}=y_{j}-\sum_{i=1}^{N} \alpha_{i}^{*} y_{i}\left\langle x_{i}, x_{j}\right\rangle$

这样超平面的两个参数$(w^{*},b^{*})$就都求出来了
$w^{*}=\sum_{i=1}^{N} \alpha_{i}^{*} y_{i} x_{i}$
$b^{*}=y_{j}-\sum_{i=1}^{N} \alpha_{i}^{*} y_{i}\left\langle x_{i}, x_{j}\right\rangle$

至于为什么SVM叫支持向量机，因为我们发现只有$\alpha_{i}^{*}>0$时，对应的样本$(x_i,y_i)$才会对最终超平面的结果产生影响，此时$1-y_{i}\left(w^{* T} x_{i}+b^{*}\right)=0$， 也就是函数间隔为1，我们称这类样本为支持向量，所以这个模型被叫做支持向量机。支持向量的个数一般很少，所以支持向量机只有很少的“重要的”训练样本决定。

**核技巧**

基本思想：找一个映射Φ（一般为高维映射），将样本点特征x映射到新的特征空间Φ(x)，使其在新的特征空间中线性可分（或近似线性可分），然后利用之前的SVM算法在新的特征空间中对样本进行分类。
![在这里插入图片描述](img/SVM/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1NTQ3Mjgx,size_16,color_FFFFFF,t_70-20211031095618159.png)
流程：
输入训练集$T=\left\{\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right), \ldots,\left(x_{n}, y_{n}\right)\right\}$其中$x_{i} \in R^{n}, y_{i} \in\{-1,+1\}$
（1）选择合适的映射函数Φ，将训练集??映射为
$T=\left\{\left(\Phi\left(x_{1}\right), y_{1}\right),\left(\Phi\left(x_{2}\right), y_{2}\right), \ldots,\left(\Phi\left(x_{n}\right), y_{n}\right)\right\}$
（2）选择惩罚参数C，构造并求解约束最优化问题（原问题的对偶问题）
$\min_{\alpha} \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left\langle\Phi\left(x_{i}\right), \Phi\left(x_{j}\right)\right\rangle-\sum_{i=1}^{\mathrm{N}} \alpha_{i}$
$\begin{aligned} \text { s.t. } & \sum_{i=1}^{N} \alpha_{i} y_{i}=0 \\ & 0 \leq \alpha_{i} \leq C, i=1,2, \ldots, N \end{aligned}$
求得最优解$\alpha^{*}=\left(\alpha_{1}^{*}, \alpha_{2}^{*}, \ldots, \alpha_{N}^{*}\right)^{T}$
（3）计算$W^{*}, b^{*}$:
$w^{*}=\sum_{i=1}^{N} \alpha_{i}^{*} y_{i} \Phi\left(x_{i}\right)$
选择$a^{*}$的一个分量满足$0<\alpha_{i}^{*}<C$，计算
$b^{*}=y_{j}-\sum_{i=1}^{N} \alpha_{i}^{*} y_{i}\left\langle\Phi\left(x_{i}\right), \Phi\left(x_{j}\right)\right\rangle$
（4）求得分离超平面和分类决策函数：
$w^{* T} \Phi(x)+b^{*}=0$
$f(x)=\operatorname{sign}\left(w^{* T} \Phi(x)+b^{*}\right)=\operatorname{sign}\left(\sum_{i=1}^{N} \alpha_{i}^{*} y_{i}\left\langle\Phi(x), \Phi\left(x_{i}\right)\right\rangle+ b^{*}\right)$

该算法的问题：
（1）合适的映射函数??太难找，几乎找不到
（2）假设找到了映射函数??，由于将数据映射到高维，在高维空间中做运算，计算量太大（维数灾难）

改进：
考虑到算法中如果不需写出分离超平面，即不需写出$w^{?}$，而是直接用$f(x)=\operatorname{sign}\left(w^{* T} \Phi(x)+b^{*}\right)=\operatorname{sign}\left(\alpha_{i}^{*} y_{i}\left\langle\Phi(x), \Phi\left(x_{j}\right)\right\rangle+ b^{*}\right)$来做预测，同样可以给出分类边界以及达到预测目的。这样的话，算法中需要用到样本的地方全部以内积形式出现，如果我们能够找到一种函数，能够在低维空间中直接算出高维内积，并且该函数对应着某个映射，即解决了以上两个问题。

核函数的本质：用相似度函数重新定义内积运算。

什么样的函数可以作为核函数？
核函数对应的Gram矩阵为半正定矩阵。

常用的核函数:

1. 线性核函数（linear kernel）$K(x, z)=x^{T} z$
2. 多项式核函数（polynomial kernel function）$K(x, z)=\left(\gamma x^{T} z+r\right)^{p}$
3. 高斯核函数（ Gaussian kernel function ） $K(x, z)=\exp \left(-\gamma\|x-z\|^{2}\right)$

## 3. SVM 为什么采用间隔最大化

当训练数据线性可分时，存在无穷个分离超平面可以将两类数据正确分开。感知机利用误分类最小策略，求得分离超平面，不过此时的解有无穷多个。线性可分支持向量机利用间隔最大化求得最优分离超平面，这时，解是唯一的。另一方面，此时的分隔超平面所产生的分类结果是**最鲁棒的**，对未知实例的泛化能力最强。可以借此机会阐述一下几何间隔以及函数间隔的关系。

## 4. 为什么要将求解 SVM 的原始问题转换为其对偶问题

- 对偶问题往往更易求解，当我们寻找约束存在时的最优点的时候，约束的存在虽然减小了需要搜寻的范围，但是却使问题变得更加复杂。为了使问题变得易于处理，我们的方法是把目标函数和约束全部融入一个新的函数，即拉格朗日函数，再通过这个函数来寻找最优点。
- 可以自然引入核函数，进而推广到非线性分类问题。

## 5. 为什么 SVM 要引入核函数

当样本在原始空间**线性不可分时**，可**将样本从原始空间映射到一个更高维的特征空间，使得样本在这个特征空间内线性可分**。而引入这样的映射后，所要求解的对偶问题的求解中，无需求解真正的映射函数，而只需要知道其核函数。核函数的定义：$K(x,y)=<ϕ(x),ϕ(y)>$，即在特征空间的内积等于它们在原始样本空间中通过核函数 $K $计算的结果。一方面数据变成了高维空间中线性可分的数据，另一方面不需要求解具体的映射函数，只需要给定具体的核函数即可，这样使得求解的难度大大降低。

## 6. 为什么SVM对缺失数据敏感

- SVM 没有处理缺失值的策略
- SVM的效果和支持向量点有关，缺失值可能影响支持向量点的分布

## 7. SVM 核函数之间的区别

SVM 核函数一般选择**线性核**和**高斯核(RBF 核)**。

线性核：主要用于线性可分的情形，参数少，速度快，对于一般数据，分类效果已经很理想了。

RBF 核：主要用于线性不可分的情形，参数多，分类结果非常依赖于参数。

如果 Feature 的数量很大，跟样本数量差不多，这时候选用线性核的 SVM。

如果 Feature 的数量比较小，样本数量一般，不算大也不算小，选用高斯核的 SVM。

## 8. LR和SVM的联系与区别

- 联系：
  
  - LR和SVM都可以处理分类问题，且一般都用于处理线性二分类问题
  - 两个方法都可以增加不同的正则化项，如l1、l2等等。所以在很多实验中，两种算法的结果是很接近的。
- 区别：
  
  - LR是参数模型，SVM是非参数模型。
  - 从目标函数来看，区别在于逻辑回归采用的是交叉熵损失函数，SVM采用的是合页损失函数，这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重。
  - SVM的处理方法是只考虑支持向量点，也就是和分类最相关的少数点，去学习分类器。而逻辑回归通过非线性映射，大大减小了离分类平面较远的点的权重，相对提升了与分类最相关的数据点的权重。
  - 逻辑回归相对来说模型更简单，好理解，特别是大规模线性分类时比较方便。而SVM的理解和优化相对来说复杂一些，SVM转化为对偶问题后,分类只需要计算与少数几个支持向量的距离,这个在进行复杂核函数计算时优势很明显,能够大大简化模型和计算。

## 9. SVM的原理是什么？

SVM是一种二类分类模型，其主要思想为找到空间中的一个更够将所有数据样本划开的超平面，并且使得数据集中所有数据到这个超平面的距离最短。它的基本模型是在特征空间中寻找间隔最大化的分离超平面的线性分类器。（间隔最大使它有别于感知机）。需要求解能够正确划分训练数据集并且几何间隔最大的分离超平面。对于线性可分的数据集来说，这样的超平面有无穷多个（即感知机），但是几何间隔最大的分离超平面却是唯一的。

## 10. SVM如何处理多分类问题？

**一对多**：就是对每个类都训练出一个分类器，设定为目标类为一类，其余类为另外一类。这样针对k个类可以训练出k个分类器，当有一个新的样本来的时候，用这k个分类器来测试，那个分类器的概率高，那么这个样本就属于哪一类。
**一对一**：任意两个类训练出一个分类器，如果有k类，一共训练出$C(2,k)$ 个分类器，这样当有一个新的样本要来的时候，用这$C(2,k) $个分类器来测试，每当被判定属于某一类的时候，该类就加一，最后票数最多的类别被认定为该样本的类。

## 参考文献

- 关于SMO算法 https://blog.csdn.net/luoshixian099/article/details/51227754#commentBox
- 李航《统计学习方法》
- 面试真题 https://zhuanlan.zhihu.com/p/43827793   https://zhuanlan.zhihu.com/p/57947723

---

# 文件：AI算法\machine-learning\TreeEmbedding.md

---

# 树模型集成学习

集成学习主要有两个思想，分别是bagging和boosting。树模型的集成模型都是使用树作为基模型，最常用的cart树，常见的集成模型有RandomForest、GBDT、Xgboost、Lightgbm、Catboost。

# 概要介绍

## RandomForest

随机森林(Random Forest,RF)是Bagging的一个扩展变体。RF在以决策树为基学习器构建Bagging集成的基础上，进一步在决策树的训练过程中引入了随机属性选择。既然模型叫做随机森林，森林我们可以理解为是多棵树的集合就是森林，随机主要有两个点进行有放回的采样，

1. 每次建树特征个数随机选择
2. 每次建树样本个数随机选择

随机森林中基学习器的多样性不仅来自样本扰动，还来自属性扰动，这就使得最终集成得泛化性能可通过个体学习器之间差异度得增加而进一步提升。使得模型更加鲁棒。

## GBDT

GBDT使用的是加法模型和前向分布算法，而AdaBoost算法是前向分布加法算法的特例，前向分布算法是加法模型，当基函数为基本分类器时，该加法模型等价于Adaboost的最终分类器。
GBDT也是迭代，使用了前向分布算法，但是弱学习器限定了只能使用CART回归树模型，同时迭代思路和Adaboost也有所不同。在GBDT的迭代中，假设我们前一轮迭代得到的强学习器是, 损失函数是, 我们本轮迭代的目标是找到一个CART回归树模型的弱学习器，让本轮的损失函数最小。也就是说，本轮迭代找到决策树，要让样本的损失尽量变得更小。GBDT本轮迭代只需拟合当前模型的残差。

## Xgboost

Xgboost是gbdt的改进或者说是梯度提升树的一种，Xgb可以说是工程上的最佳实践模型，简单的说xgb=gbdt+二阶梯度信息+随机特征和样本选择+特征百分位值加速+空值特征自动划分。还有必要的正则项和最优特征选择时的并行计算等。

## Lightgbm

首先，GBDT是一个非常流行的机器学习算法，另外基于GBDT实现的XGBoost也被广泛使用。但是当面对高纬度和大数据量时，其效率和可扩展性很难满足要求。主要的原因是对于每个特征，我们需要浏览所有的数据去计算每个可能分裂点的信息增益，真是非常耗时的。基于此，提出了两大技术：Gradient-based One-Side Sampling (GOSS) and Exclusive Feature Bundling (EFB).

##　catboost
CatBoost = Category + Boosting.
2017年7月21日，俄罗斯Yandex开源CatBoost，亮点是在模型中可直接使用Categorical特征并减少了tuning的参数。

# 核心公式

1. gbdt的前向分布公式
   
   $$
   f_m(x)=f_{m-1}(x)+\beta_m b(x;\gamma_m) \tag{1}
   $$
2. gbdt的第m轮的扶梯度公式
   
   $$
   \left[
       \frac{\partial L(y,f(x_i))}{\partial f(x_i)}
   \right]_{f(x)=f_{m-1}(x)} \tag{2}
   $$
3. gbdt格式化损失函数
   
   $$
   (y,f_m(x))=L(y,f_{m-1}(x)+\beta_m b(x;\gamma_m)) \tag{3}
   $$
4. 泰勒展开式
   若函数f（x）在包含x0的某个闭区间[a,b]上具有n阶导数，且在开区间（a,b）上具有（n+1）阶导数，则对闭区间[a,b]上任意一点x，成立下式：
   
   $$
   f(x)=f(x_0)+f'(x_0)(x-x_0)+\frac{f''(x0)}{2!}(x-x_0)^2+ ... + \frac{f^{(n)}(x_0)}{n!}(x-x_0)^n+R_n(x) \tag{4}
   $$
   
   $$
   f(x+\Delta x)=f(x)+f'(x)\Delta x + \frac{1}{2!}f''(x)\Delta x^2+...+\frac{1}{n!}f^{(n)}(x)\Delta x^n+R_n(x) \tag{5}
   $$
   
   其中，$R_n(x)$是$(x-x_0)^n的高阶无穷小.$
5. xgboost的目标公式(t轮迭代)
   
   $$
   obj^{(t)}=\sum_{i=1}^{n}l(y_i,\hat{y}_i^t)+\sum_{i=1}^{t}\Omega(f_i) \tag{6}
   $$
   
   $$
   =\sum_{i=1}^{n}l(y,\hat y_{i}^{(t-1)}+f_t(x_i))+\Omega(f_t)+constant \tag{7}
   $$
6. xgboost损失函数的泰勒二阶展开
   
   $$
   ^{(t)} \eqsim \sum_{i=1}^{n}[l(y_i,\hat y ^{(t-1)})+g_i f_t(x_i) + \frac{1}{2}h_i f_t^2(x_i)]+\Omega(f_t) \tag{8}
   $$
   
   其中，$l(y_i,\hat y ^{(t-1)})$是常数，$g_i=\partial_{\hat{y}^{(t-1)}}l(y_i, \hat{y}^{(t-1)})$, $h_i=\partial_{\hat{y}^{(t-1)}}^2l(y_i, \hat{y}^{(t-1)})$. 常数对目标函数的优化不相关，于是可以将目标函数转化为如下:
   
   $$
   ^{(t)} = \sum_{i=1}^{n}[g_i f_t(x_i) + \frac{1}{2}h_i f_t^2(x_i)]+\Omega(f_t) \tag{9}
   $$
   
   $$
   \sum_{i=1}^{n}[g_i f_t(x_i) + \frac{1}{2}h_i f_t^2(x_i)]+\lambda T+\frac{1}{2}\sum_{j=1}^{T}\omega_j^2  \tag{10}
   $$
   
   $$
   \sum_{j=1}^{T}[(\sum_{i \in I_j}g_i) \omega_j + \frac{1}{2}(\sum_{i \in I_j}h_i) \omega_j^2] + \lambda T + \frac{1}{2}\sum_{i=1}^{T} \omega_j^2  \tag{11}
   $$
   
   $$
   \sum_{i=1}^{n}[g_i f_t(x_i) + \frac{1}{2}h_i f_t^2(x_i)]+\lambda T+\frac{1}{2}\sum_{j=1}^{T}\omega_j^2  \tag{12}
   $$
   
   $$
   \sum_{j=1}^{T}[(\sum_{i \in I_j}g_i) \omega_j + \frac{1}{2}(\sum_{i \in I_j}h_i+\lambda) \omega_j^2] + \lambda T  \tag{13}
   $$
   
   求上式最小化的参数，对$\omega$求导数并另其等于0，得到下式:
   
   $$
   frac{\partial l^{(t)}}{\partial \omega_j}=0 \tag{14}
   $$
   
   $$
   sum_{i \in I_j}+(\sum_{i \in I_j}h_i + \lambda) \omega_j=0 \tag{15}
   $$
   
   $$
   omega_j^*=-\frac{\sum_{i \in I_j}g_i}{\sum_{i \in I_j}h_i + \lambda} \tag{16}
   $$
   
   将上式带入损失函数，得到最小损失：
   
   $$
   hat{l}^{(t)}(q)=-\frac{1}{2}\sum_{j=1}^{T}\frac{(\sum_{i \in I_j}g_i)^2}{\sum_{i \in I_j}h_i+ \lambda}+\gamma T \tag{17}
   $$
   
   根据公式(17)可以作为特征分裂的指标.计算公式如下(这个值越大越好):
   
   $$
   _{split}=\frac{1}{2}
   \left[
       \frac{（\sum_{i \in I_L}g_i)^2}{\sum_{i \in I_L}h_i+\lambda} + 
       \frac{（\sum_{i \in I_R}g_i)^2}{\sum_{i \in I_R}h_i+\lambda} - 
       \frac{（\sum_{i \in I}g_i)^2}{\sum_{i \in I}h_i+\lambda}
       \right ] - \lambda \tag{18}
   $$

# 算法十问

1. 随机森林为什么能够更鲁棒？

> 由于随机森林使用了使用了行采样和列采样技术，是的每棵树不容易过拟合；并且是基于树的集成算法，由于使用了采用数据是的每棵树的差别较大，在进行embedding的时候可以更好的降低模型的方差，整体而言是的RF是一个鲁棒的模型。

2. RF分类和回归问题如何预测y值？

> RF是一个加权平均的模型，是进行分类问题的时候，使用的个k个树的投票策略，多数服从少数。在回归的使用是使用的k个树的平均。可以看出来rf的训练和预测过程都可以进行并行处理。

3. 相同数据量，训练RF和gbdt谁可以更快？谁对异常值不敏感？

> gbdt是前向加法模型，由于第i棵树需要用到前i-1树的残差，所有在再整个建立过程是串行处理的，RF整体是bagging算法的一种，是k个树的加权平均，k棵树可以并行处理，因此可能得到更快的速度。需要指出在gbdt的原始算法中没有使用行列的随机采样，相反rf使用了随机采样。
> 由于gbdt当前的误差会延续给下一棵树，而RF每次都是独立的随机采样，随机森林对异常值不敏感，GBDT对异常值非常敏感。

4. 解释一个什么是gb，什么是dt，即为什么叫做gbdt？

> gbdt(Gradient Boosting Decision Tree),dt是指Decision Tree表示使用决策树作为基学习器，使用的cart树，gb表示梯度提升，因为在传统的gbdt中在第i轮的迭代中，使用前i-1的梯度作为当前残差进行拟合。

5. gbdt为什么用负梯度代表残差？

> 上文公式(3)是gbdt的损失函数，对公式(3)进行在$f_{m-1}(x)处进行$泰勒的一阶展开:
> 
> $$
> L(y,f_m(x))=L(y,f_{m-1}(x)+\beta_m b(x;\gamma_m))
> $$
> 
> $$
> =L(y,f_{m-1}(x))+\frac{\partial L(y, f_{m-1}(x))}{\partial f_{m-1}(x)}(f_{m}(x)-f_{m-1}(x))
> $$
> 
> $$
> =L(y,f_{m-1}(x))+\frac{\partial L(y, f_{m-1}(x))}{\partial f_{m-1}(x)}(\beta_m b(x;\gamma_m)) \tag{19}
> $$
> 
> 从我们的目标是损失函数最小化，使公式(19)最小化，由于$L(y,f_{m-1}(x))$是个常数，所以我们的损失函数最小化可以转化为:
> 
> $$
> argmin_{(\beta_m,\gamma_m)}=min \frac{\partial L(y, f_{m-1}(x))}{\partial f_{m-1}(x)}(\beta_m b(x;\gamma_m)) \tag{20}
> $$
> 
> 将上述式子的两项都看做是向量，为了是相乘之后最小，一定是向量之间的异号,因此得到:
> 
> $$
> (\beta_m b(x;\gamma_m)) = - \frac{\partial L(y, f_{m-1}(x))}{\partial f_{m-1}(x)} \tag{21}
> $$
> 
> 从公式(20)可以看出第m棵树使用前m-1的负梯度作为残差，所有每次都是拟合的负梯度.

6. gbdt是训练过程如何选择特征？

> gbdt使用基学习器是CART树，CART树是二叉树，每次使用yes or no进行特征选择，数值连续特征使用的最小均方误差，离散值使用的gini指数。在每次划分特征的时候会遍历所有可能的划分点找到最有的特征分裂点，这是用为什么gbdt会比rf慢的主要原因之一。

7. gbdt应用在多分类问题？

> + gbdt 每轮的训练是在上一轮的训练的残差基础之上进行训练的。这里的残差就是当前模型的负梯度值 。这个要求每轮迭代的时候，弱分类器的输出的结果相减是有意义的。残差相减是有意义的。
> + 对于多分类任务，GDBT的做法是采用一对多的策略也就是说，对每个类别训练M个分类器。假设有K个类别，那么训练完之后总共有M*K颗树。
> + 两层循环的顺序不能改变。也就是说，K个类别都拟合完第一颗树之后才开始拟合第二颗树，不允许先把某一个类别的M颗树学习完，再学习另外一个类别。
>   <img src='../assert/mult_gbdt.png'/>

8. RF和GBDT的区别？

> GBDT是采用boosing方法，降低偏差；RF采用的是baggging方法，降低方差。其中GBDT中的核心是通过用分类器（如CART、RF）拟合损失函数梯度，而损失函数的定义就决定了在子区域内各个步长，其中就是期望输出与分类器预测输出的查，即bias；而RF的核心就是自采样（样本随机）和属性随机（所有样本中随机选择K个子样本选择最优属性来划分），样本数相同下的不同训练集产生的各个分类器，即数据的扰动导致模型学习性能的变化，即variance。

9. Xgboost相对gbdt做了哪些改进？

> + 传统GBDT以CART作为基分类器，xgboost还支持线性分类器，这个时候xgboost相当于带L1和L2正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）。
> + 传统GBDT在优化时只用到一阶导数信息，xgboost则对代价函数进行了二阶泰勒展开，同时用到了一阶和二阶导数。顺便提一下，xgboost工具支持自定义代价函数，只要函数可一阶和二阶求导。
> + xgboost在代价函数里加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、每个叶子节点上输出的score的L2模的平方和。从Bias-variance tradeoff角度来讲，正则项降低了模型的variance，使学习出来的模型更加简单，防止过拟合，这也是xgboost优于传统GBDT的一个特性。
> + 列抽样（column subsampling）。xgboost借鉴了随机森林的做法，支持列抽样，不仅能降低过拟合，还能减少计算，这也是xgboost异于传统gbdt的一个特性。
>   对缺失值的处理。对于特征的值有缺失的样本，xgboost可以自动学习出它的分裂方向。
> + xgboost工具支持并行。boosting不是一种串行的结构吗?怎么并行的？注意xgboost的并行不是tree粒度的并行，xgboost也是一次迭代完才能进行下一次迭代的（第t次迭代的代价函数里包含了前面t-1次迭代的预测值）。xgboost的并行是在特征粒度上的。我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），xgboost在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量。这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。
> + 可并行的近似直方图算法。树节点在进行分裂时，我们需要计算每个特征的每个分割点对应的增益，即用贪心法枚举所有可能的分割点。当数据无法一次载入内存或者在分布式情况下，贪心算法效率就会变得很低，所以xgboost还提出了一种可并行的近似直方图算法，用于高效地生成候选的分割点。

10. xgb如何在计算特征时加速的？

> + xgboost工具支持并行。boosting不是一种串行的结构吗?怎么并行的？注意xgboost的并行不是tree粒度的并行，xgboost也是一次迭代完才能进行下一次迭代的（第t次迭代的代价函数里包含了前面t-1次迭代的预测值）。xgboost的并行是在特征粒度上的。我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），xgboost在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量。这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。
> + 可并行的近似直方图算法。树节点在进行分裂时，我们需要计算每个特征的每个分割点对应的增益，即用贪心法枚举所有可能的分割点。当数据无法一次载入内存或者在分布式情况下，贪心算法效率就会变得很低，所以xgboost还提出了一种可并行的近似直方图算法，用于高效地生成候选的分割点。

11. xgb为什么使用二阶梯度信息，为什么不使用三阶或者更高梯度信息？

> xgb之所以使用二阶梯度信息，是因为从泰勒展开式来看，gbdt使用的一阶梯度的泰勒展开式，丢失了很多的信息，使用二阶可以使损失函数更加准确。从泰勒展开的角度来看展开的次数越多越能更精准的表示损失函数的值，但是如果我们使用二阶梯度就要要求损失函数二阶可导，如果使用n阶展开就要求损失函数n阶可导，但是有很多损失函数不是n阶可导的，比如均方误差，因此使用二阶梯度信息是一个泰勒展开和损失函数选择的折中。

12. lgb相对xgb做了哪些改进？

> + 直方图算法，LightGBM提供一种数据类型的封装相对Numpy,Pandas,Array等数据对象而言节省了内存的使用，原因在于他只需要保存离散的直方图，LightGBM里默认的训练决策树时使用直方图算法，XGBoost里现在也提供了这一选项，不过默认的方法是对特征预排序，直方图算法是一种牺牲了一定的切分准确性而换取训练速度以及节省内存空间消耗的算法.
> + 在训练决策树计算切分点的增益时，预排序需要对每个样本的切分位置计算，所以时间复杂度是O(#data)而LightGBM则是计算将样本离散化为直方图后的直方图切割位置的增益即可，时间复杂度为O(#bins),时间效率上大大提高了(初始构造直方图是需要一次O(#data)的时间复杂度，不过这里只涉及到加和操作).
> + 直方图做差进一步提高效率，计算某一节点的叶节点的直方图可以通过将该节点的直方图与另一子节点的直方图做差得到，所以每次分裂只需计算分裂后样本数较少的子节点的直方图然后通过做差的方式获得另一个子节点的直方图，进一步提高效率
> + 节省内存,将连续数据离散化为直方图的形式，对于数据量较小的情形可以使用小型的数据类型来保存训练数据
>   不必像预排序一样保留额外的对特征值进行预排序的信息
>   减少了并行训练的通信代价.
> + 稀疏特征优化、直接支持类别特征、网络通信优化

13. 比较一下catboost、lgb和xgb？
    XGBoost、LightGBM和CatBoost都是目前经典的SOTA（state of the art）Boosting算法，都可以归类到梯度提升决策树算法系列。三个模型都是以决策树为支撑的集成学习框架，其中XGBoost是对原始版本的GBDT算法的改进，而LightGBM和CatBoost则是在XGBoost基础上做了进一步的优化，在精度和速度上都有各自的优点。

> + 三个模型树的构造方式有所不同，XGBoost使用按层生长（level-wise）的决策树构建策略，LightGBM则是使用按叶子生长（leaf-wise）的构建策略，而CatBoost使用了对称树结构，其决策树都是完全二叉树。
> + 对于类别特征的处理。XGBoost本身不具备自动处理类别特征的能力，对于数据中的类别特征，需要我们手动处理变换成数值后才能输入到模型中；LightGBM中则需要指定类别特征名称，算法即可对其自动进行处理；CatBoost以处理类别特征而闻名，通过目标变量统计等特征编码方式也能实现类别特征的高效处理。

14. 如果将所有数据复制一倍放入训练数据集，RF和GBDT分别有什么表现？

> RF可能出现过拟合? GBDT没有任何改变?(请思考)

15. gbdt如何防止过拟合？由于gbdt是前向加法模型，前面的树往往起到决定性的作用，如何改进这个问题？

> 一般使用缩减因子对每棵树进行降权，可以使用带有dropout的GBDT算法，dart树，随机丢弃生成的决策树，然后再从剩下的决策树集中迭代优化提升树。
> GBDT与Boosting区别较大，它的每一次计算都是为了减少上一次的残差，而为了消除残差，可以在残差减小的梯度方向上建立模型;
> 在GradientBoost中，每个新的模型的建立是为了使得之前的模型的残差往梯度下降的方法。

16. RF/GBDT/XGB/lightGBM ?

> <img src='../assert/gbdt-1.png'>
> <img src='../assert/gbdt-2.png'>
> <img src='../assert/gbdt-3.png'>
> <img src='../assert/gbdt-4.png'>

# 面试真题

1. RF和GBDT能够并行吗?
2. 写一个gbdt的损失函数?
3. 为什么要拟合负梯度?
4. xgboost如何进行参数更新的?
5. xgboost为什么使用二阶梯度信息?
6. gbdt对异常值敏感吗?为什么?

# 参考

1. https://www.cnblogs.com/fujian-code/p/9018114.html
2. https://blog.csdn.net/u010398493/article/details/77587749
3. https://www.jianshu.com/p/49ab87122562
4. https://blog.csdn.net/qq_22238533/article/details/79199605

---

# 文件：AI算法\machine-learning\w2v+tf-idf.md

---

## Word2Vector

### 1.什么是词嵌入模型？

把词映射为实数域向量的技术也叫词嵌⼊

### 2.介绍一下Word2Vec

谷歌2013年提出的Word2Vec是目前最常用的词嵌入模型之一。Word2Vec实际是一种**浅层的神经网络模型**，它有两种网络结构，分别是**连续词袋**（CBOW）和**跳字**(Skip-Gram)模型。

### 3.介绍CBOW

CBOW，全称Continuous Bag-of-Word，中文叫做连续词袋模型：**以上下文来预测当前词** $w_t$ 。CBOW模型的目的是预测 $P(w_t| w_{t-k}, \cdots, w_{t-1}, w_{t+1}, \cdots, w_{t+k}) $

![img](https://pic4.zhimg.com/v2-27f3e577618f84c0026968d273d823ef_b.jpg)

#### 前向传播过程

- 输入层: 输入C个单词$x$： $x_{1k}, \cdots, x_{Ck} $，并且每个 $x$ 都是用 **One-hot** 编码表示，每一个 $x$ 的维度为 V（词表长度）。
- 输入层到隐层
  
  - 首先，共享矩阵为 $W_{V \times N}$ ，**V表示词表长度**，W的每一行表示的就是一个N维的向量（训练结束后，W的每一行就表示一个词的词向量）。
  - 然后，我们把所有**输入的词转$x$化为对应词向量**，然后**取平均值**，这样我们就得到了隐层输出值 ( 注意，隐层中无激活函数，也就是说这里是线性组合)。 其中，隐层输出 $h$ 是一个N维的向量 。
  
  $$
  h = \frac{1}{C} W^T(x_1 + x_2 + \cdots + x_c)
  $$
- 隐层到输出层：隐层的输出为N维向量 $h$ ， 隐层到输出层的权重矩阵为  $W'_{N \times V}$ 。然后，通过矩阵运算我们得到一个 $V \times 1 $ 维向量
  
  $$
  u = W'^{T} * h
  $$

其中，向量 $u$  的第 $i$  行表示词汇表中第 $i$  个词的可能性，然后我们的目的就是取可能性最高的那个词。因此，在最后的输出层是一个softmax 层获取分数最高的词，那么就有我们的最终输出：

$$
P(w_j| context)  =y_i =  \frac{exp({u_j})}{\sum_{k \in V} exp({u_k})}
$$

#### 损失函数

我们假定 $j^*$ 是真实单词在词汇表中的下标，那么根据极大似然法，则目标函数定义如下：

$$
E = -log \, p(W_O |W_I) = -log \, \frac{exp({u_j})}{\sum_{k \in V} exp({u_k})} =  log  \sum_{k \in V} exp(u_{k})  -u_j
$$

### 4.Skip-gram模型

Skip-Gram的基本思想是：**通过当前词 $w_t$ 预测其上下文 $w_{t-i}, \cdots , w_{t+i}$** ，模型如下图所示：

![img](https://pic2.zhimg.com/v2-42ef75691c18a03cfda4fa85a8409e19_b.jpg)

#### 前向传播过程

- 输入层：   输入的是一个单词，其表示形式为 **One-hot** ，我们将其表示为V维向量 $x_k$ ，其中 $V$ 为词表大小。然后，通过词向量矩阵 $W_{V \times N}$ 我们得到一个N维向量
  
  $$
  h = W^T * x_k = v^{T}_{w_I}
  $$
- 隐层： 而隐层中没有激活函数，也就是说输入=输出，因此隐藏的输出也是 $h$ 。
- 隐层到输出层：
  
  - 首先，因为要输出C个单词，因此我们此时的输出有C个分布： $y_1, \cdots y_C $，且每个分布都是独立的，我们需要单独计算， 其中 $y_i$  表示窗口的第 $i$  个单词的分布。
  - 其次， 因为矩阵 $W'_{N \times V}$ 是共享的，因此我们得到的 $V \times 1$ 维向量 $u$ 其实是相同的，也就是有 $u_{c,j} = u_j$ ，这里 $u$ 的每一行同 CBOW 中一样，表示的也是评分。
  - 最后，每个分布都经过一个 softmax 层，不同于 CBOW，我们此处产生的是第 $i$ 个单词的分布（共有C个单词），如下：
  
  $$
  P(w_{i,j}| context)  =y_i =  \frac{exp({u_j})}{\sum_{k \in V} exp({u_k})}
  $$

#### 损失函数

假设 $j^*$ 是真实单词在词汇表中的下标，那么根据极大似然法，则目标函数定义如下：

$$
\begin{split} E &= - log \, p(w_1, w_2, \cdots, w_C | w_I)   \\ &= - log \prod_{c=1}^C P(w_c|w_i) \\ &= - log  \prod_{c=1}^{C} \frac{exp(u_{c, j})}{\sum_{k=1}^{V} exp(u_{c,k}) } \\ &= - \sum_{c=1}^C u_{j^*_c} + C \cdot log \sum_{k=1}^{V} exp(u_k) \end{split}
$$

### 5.Word2Vec与LDA的区别

- LDA
  
  LDA是利用文档中**单词的共现关系**来对单词按**主题聚类**，也可以理解为对“**文档-单词**”矩阵进行**分解**，得到“**文档-主题**”和“**主题-单词**”两个**概率分布**。
- Word2Vec
  
  Word2Vec是利用**上下文-单词**“矩阵进行学习，其中上下文由周围的几个单词组成，由此得到的词向量表示更多地融入了上下文共现的特征。也就是说，如果两个单词所对应的word2vec向量相似度较高，那么它们很可能经常在同样的上下文中出现。
- LDA模型是一种基于**概率图模型**的**生成式模型**，其似然函数可以写成若干条件概率连乘的形式，其中包括需要推测的隐含变量（即主题）；
- 而Word2Vec模型一般表达为**神经网络**的形式，似然函数定义在网络的输出之上，需要通过学习网络的权重以得到单词的稠密向量表示。

### 6.Word2Vec存在的问题是什么？

- 对每个local context window单独训练，没有利用包 含在global co-currence矩阵中的统计信息。
- 对多义词无法很好的表示和处理，因为使用了唯一的词向量

## Tf-idf

### 1.介绍一下Tf-idf

**一个词语在一篇文章中出现次数越多, 同时在所有文档中出现次数越少, 越能够代表该文章.**

- **TF:** Term Frequency, 表示词频。 一个给定的词在该文章中出现的次数。
  
  $$
  TF = \frac{\text{某个词在文章中的出现次数}}{\text{文章的总词数}}  \\
  $$
- **IDF:** Inverse Document Frequency, 表示逆文档频率。如果包含词条 t 的文档越少, IDF越大，则说明词条具有很好的类别区分能力。

$$
IDF = log(\frac{语料库的文档总数}{包含该词的文档数+1})  \\
$$

- **TF-IDF：**某一特定文件内的高词语频率，以及该词语在整个文件集合中的低文件频率，可以产生出高权重的TF-IDF。因此，**TF-IDF倾向于过滤掉常见的词语**，保留重要的词语
  $$
  \text{TF-IDF} = TF \times IDF
  $$

**举例说明**

假设现在有一篇文章， 文章中包含 10000 个词组， 其中，"贵州" 出现100次，"的" 出现500次，那么我们可以计算得到这几个词的 TF(词频) 值：

$$
TF(贵州) = 100 / 10000 = 0.01 \\
TF(的) = 500 / 10000 = 0.05
$$

现在语料库中有 1000 篇文章， 其中，包含 "贵州" 的有 99 篇， 包含 "的" 的有 899 篇， 则它们的 IDF 值计算为：

$$
IDF(贵州) = log(1000 / (99+1)) = 1.000 \\
IDF(的) = log(1000 / (899+1)) = 0.046
$$

### 2. Tf-idf的优缺点

- 优点：简单快速，而且容易理解。
- 缺点：有时候用词频来衡量文章中的一个词的重要性**不够全面**，有时候重要的词出现的可能不够多，而且这种计算无法体现位置信息，**无法体现词在上下文的重要性**。

## 参考资料

白面机器学习

https://github.com/scutan90/DeepLearning-500-questions

https://github.com/NLP-LOVE/ML-NLP

https://github.com/songyingxin/NLPer-Interview

---

# 文件：AI算法\machine-learning\XGBoost.md

---

# XGBoost面试题

## 1. RF和GBDT的区别

**相同点：**

- 都是由多棵树组成，最终的结果都是由多棵树一起决定。

**不同点：**

- **集成学习**：$RF$属于$Bagging$思想，而$GBDT$是$Boosting$思想
- **偏差-方差权衡**：$RF$不断的降低模型的方差，而$GBDT$不断的降低模型的偏差
- **并行性**：$RF$的树可以并行生成，而$GBDT$只能顺序生成(需要等上一棵树完全生成)
- **最终结果**：$RF$最终是多棵树进行多数表决（回归问题是取平均），而$GBDT$是加权融合
- **数据敏感性**：$RF$对异常值不敏感，而$GBDT$对异常值比较敏感
- **泛化能力**：$RF$不易过拟合，而$GBDT$容易过拟合

## 2. 比较LR和GBDT，说说什么情景下GBDT不如LR

先说说$LR$和$GBDT$的区别：

- $LR$是线性模型，可解释性强，很容易并行化，但学习能力有限，需要大量的人工特征工程
- $GBDT$是非线性模型，具有天然的特征组合优势，特征表达能力强，但是树与树之间无法并行训练，而且树模型很容易过拟合；

当在**高维稀疏特征的场景下，$LR$的效果一般会比$GBDT$好**。原因如下：

先看一个例子：

> 假设一个二分类问题，label为0和1，特征有100维，如果有1w个样本，但其中只要10个正样本1，而这些样本的特征 f1的值为全为1，而其余9990条样本的f1特征都为0(在高维稀疏的情况下这种情况很常见)。 我们都知道在这种情况下，树模型很容易优化出一个使用f1特征作为重要分裂节点的树，因为这个结点直接能够将训练数据划分的很好，但是当测试的时候，却会发现效果很差，因为这个特征f1只是刚好偶然间跟y拟合到了这个规律，这也是我们常说的过拟合。

因为现在的模型普遍都会带着正则项，**而 $LR$ 等线性模型的正则项是对权重的惩罚**，也就是 $w_1$一旦过大，惩罚就会很大，进一步压缩 $w_1$的值，使他不至于过大。但是，树模型则不一样，**树模型的惩罚项通常为叶子节点数和深度**等，而我们都知道，对于上面这种`case`，树只需要一个节点就可以完美分割9990和10个样本，一个结点，最终产生的惩罚项极其之小。

这也就是为什么在高维稀疏特征的时候，线性模型会比非线性模型好的原因了：**带正则化的线性模型比较不容易对稀疏特征过拟合。**

## 3. 简单介绍一下XGBoost

$XGBoost$是大规模、分布式的通用梯度提升(GBDT)库，它在GB框架下实现了GBDT和一些广义线性ML算法。$XGBoost$是一种集成学习算法，属于3类常用的集成方法($Bagging$，$Boosting$，$Stacking$)中的$Boosting$算法类别。它是一个加法模型，每次迭代都学习一棵CART树来拟合之前 t-1 棵树的预测结果与训练样本真实值的残差。

$XGBoost$对$GBDT$进行了一系列优化，比如损失函数进行了二阶泰勒展开、目标函数加入正则项、支持并行、默认缺失值处理等，在可扩展性和训练速度上有了巨大的提升，但其核心思想没有大的变化。

## 4. XGBoost与GBDT有什么不同

- **基分类器**：$XGBoost$的基分类器不仅支持$CART$决策树，还支持线性分类器，此时$XGBoost$相当于带$L1$和$L2$正则化项的$LR$回归（分类问题）或者线性回归（回归问题）。
- **导数信息**：$XGBoost$对损失函数做了二阶泰勒展开，可以更为精准的逼近真实的损失函数，$GBDT$只用了一阶导数信息，并且$XGBoost$还支持自定义损失函数，只要损失函数一阶、二阶可导。
- **正则项**：$XGBoost$的目标函数加了正则项， 相当于预剪枝，使得学习出来的模型更加不容易过拟合。
- **列抽样**：$XGBoost$支持列采样，与随机森林类似，用于防止过拟合。
- **缺失值处理**：对树中的每个非叶子结点，$XGBoost$可以自动学习出它的默认分裂方向。如果某个样本该特征值缺失，会将其划入默认分支。
- **并行化**：注意不是树维度的并行，而是特征维度的并行。$XGBoost$预先将每个特征按特征值排好序，存储为块结构，分裂结点时可以采用多线程并行查找每个特征的最佳分割点，极大提升训练速度。
- **可扩展性**：损失函数支持自定义，只需要新的损失函数二阶可导。

## 5. XGBoost为什么可以并行训练

- **不是说每棵树可以并行训练**，$XGBoost$本质上仍然采用$Boosting$思想，每棵树训练前需要等前面的树训练完成才能开始训练。
- **而是特征维度的并行**：在训练之前，每个特征按特征值对样本进行预排序，并存储为`block`结构，在后面查找特征分割点时可以重复使用，而且特征已经被存储为一个个`block`结构，那么在寻找每个特征的最佳分割点时，可以利用多线程对每个`block`并行计算。
- 注意xgboost的并行不是tree粒度的并行，xgboost也是一次迭代完才能进行下一次迭代的（第t次迭代的代价函数里包含了前面t-1次迭代的预测值）。xgboost的并行是在特征粒度上的。我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），xgboost在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量。这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。

## 6. XGBoost为什么快？

- **分块并行**：训练前每个特征按特征值进行排序并存储为`block`结构，后面查找特征分割点时重复使用，并且支持并行查找每个特征的分割点
- **`block` 处理优化**：`block`预先放入内存；`block`按列进行解压缩；将`block`划分到不同硬盘来提高吞吐
- **候选分位点**：每个特征采用常数个分位点作为候选分割点
- **CPU cache 命中优化**： 使用缓存预取的方法，对每个线程分配一个连续的`buffer`，读取每个`block`中样本的梯度信息并存入连续的`buffer`中。

## 7. XGBoost中如何处理过拟合的情况？

- **目标函数中增加了正则项**：使用叶子结点的数目和叶子结点权重的$L2$模的平方，控制树的复杂度。
- **设置目标函数的增益阈值**：如果分裂后目标函数的增益小于该阈值，则不分裂。
- **设置最小样本权重和的阈值**：当引入一次分裂后，重新计算新生成的左、右两个叶子结点的样本权重和。如果任一个叶子结点的样本权重低于某一个阈值（最小样本权重和），也会放弃此次分裂。
- **设置树的最大深度**：$XGBoost$ 先从顶到底建立树直到最大深度，再从底到顶反向检查是否有不满足分裂条件的结点，进行剪枝。
- **shrinkage**: 可以叫学习率或步长，为了给后面的训练留出更多的学习空间
- **子采样**：每轮计算可以不使用全部样本，使算法更加保守
- **列抽样**：训练的时候只用一部分特征（不考虑剩余的block块即可）

* 调参：
  * 第一类参数：用于直接控制模型的复杂度。包括`max_depth，min_child_weight，gamma` 等参数
  * 第二类参数：用于增加随机性，从而使得模型在训练时对于噪音不敏感。包括`subsample，colsample_by树`
  * 还有就是直接减小`learning rate`，但需要同时增加`estimator` 参数。

## 8. XGBoost如何处理缺失值？

$XGBoost$模型的一个优点就是允许特征存在缺失值。对缺失值的处理方式如下：

- $XGBoost$**在构建树的节点过程中只考虑非缺失值的数据遍历**，而为每个节点增加了一个缺省方向，当样本相应的特征值缺失时，可以被归类到缺省方向上，最优的缺省方向可以从数据中学到。至于如何学到缺省值的分支，其实很简单，分别枚举特征缺省的样本归为左右分支后的增益，选择增益最大的枚举项即为最优缺省方向。
- 如果在**训练中没有缺失值而在预测中出现缺失，那么会自动将缺失值的划分方向放到右子结点**。

* **树模型对缺失值的敏感度低，大部分时候可以在数据缺失时时使用**。
  
  原因就是：**一棵树中每个结点在分裂时，寻找的是某个特征的最佳分裂点（特征值）**，完全可以不考虑存在特征值缺失的样本，也就是说，如果某些样本缺失的特征值缺失，对寻找最佳分割点的影响不是很大。

## 9. XGBoost如何处理不平衡数据？

* 设置`scale_pos_weight`来平衡正样本和负样本的权重。例如，当正负样本比例为1:10时，`scale_pos_weight`可以取10；
* 你不能重新平衡数据集(会破坏数据的真实分布)的情况下，应该设置`max_delta_step`为一个有限数字来帮助收敛（基模型为$LR$时有效）。

## 10. XGBoost如何选择最佳分裂点？

* 训练前预先将特征**对特征值进行排序**，存储为`block`结构，以便在结点分裂时可以重复使用
* 采用**特征并行**的方法利用多个线程分别计算每个特征的最佳分割点，根据每次分裂后产生的增益，**最终选择增益最大的那个特征的特征值**作为最佳分裂点。
* $XGBoost$使用**直方图近似算法**，对特征排序后仅选择常数个候选分裂位置作为候选分裂点，极大提升了结点分裂时的计算效率。

## 11. XGBoost的Scalable性如何体现？

- **基分类器的scalability**：弱分类器可以支持$CART$决策树，也可以支持$LR$。
- **目标函数的scalability**：支持自定义loss function，只需要其一阶、二阶可导。有这个特性是因为泰勒二阶展开，得到通用的目标函数形式。
- **学习方法的scalability**：`block`结构支持并行化，支持 Out-of-core计算。

## 12. XGBoost如何评价特征的重要性？

常用的三种方法来评判模型中特征的重要程度：

- `freq` ： 频率是表示特定特征在模型树中发生分裂的相对次数的百分比
- `gain` ： 增益意味着相应的特征对通过对模型中的每个树采取每个特征的贡献而计算出的模型的相对贡献。与其他特征相比，此度量值的较高值意味着它对于生成预测更为重要。
- `cover` ：覆盖度量指的是与此功能相关的观测的相对数量。例如，如果您有100个观察值，4个特征和3棵树，并且假设特征1分别用于决定树1，树2和树3中10个，5个和2个观察值的叶节点;那么该度量将计算此功能的覆盖范围为$10 + 5 + 2 = 17$个观测值。这将针对所有4项功能进行计算，并将以17个百分比表示所有功能的覆盖指标。

**$XGBoost$是根据`gain`来做重要性判断的。**

## 14. XGBoost的优缺点

* 优点
  
  * **精度更高：** $GBDT$ 只用到一阶泰勒展开，而 $XGBoost$ 对损失函数进行了二阶泰勒展开。$XGBoost$ 引入二阶导一方面是为了增加精度，另一方面也是为了能够自定义损失函数，二阶泰勒展开可以近似大量损失函数；
  * **灵活性更强：** $GBDT$ 以 $CART$ 作为基分类器，$XGBoost$ 不仅支持 $CART$ 还支持线性分类器，使用线性分类器的 $XGBoost$ 相当于带 和 正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）。此外，$XGBoost$ 工具支持自定义损失函数，只需函数支持一阶和二阶求导；
  * **正则化：** $XGBoost$ 在目标函数中加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、叶子节点权重的 范式。正则项降低了模型的方差，使学习出来的模型更加简单，有助于防止过拟合，这也是$XGBoost$优于传统$GBDT$的一个特性。
  * **Shrinkage（缩减）：** 相当于学习速率。$XGBoost$ 在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间。传统$GBDT$的实现也有学习速率；
  * **列抽样：** $XGBoost$ 借鉴了随机森林的做法，支持列抽样，不仅能降低过拟合，还能减少计算。这也是$XGBoost$异于传统$GBDT$的一个特性；
  * **缺失值处理：** 对于特征的值有缺失的样本，$XGBoost$ 采用的稀疏感知算法可以自动学习出它的分裂方向；
  * **$XGBoost$工具支持并行：** $Boosting$不是一种串行的结构吗?怎么并行的？注意$XGBoost$的并行不是树粒度的并行，$XGBoost$也是一次迭代完才能进行下一次迭代的（第次迭代的代价函数里包含了前面次迭代的预测值）。$XGBoost$的并行是在特征粒度上的。我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），$XGBoost$在训练之前，预先对数据进行了排序，然后保存为`block`结构，后面的迭代中重复地使用这个结构，大大减小计算量。这个`block`结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。
  * **可并行的近似算法：** 树节点在进行分裂时，我们需要计算每个特征的每个分割点对应的增益，即用贪心法枚举所有可能的分割点。当数据无法一次载入内存或者在分布式情况下，贪心算法效率就会变得很低，所以$XGBoost$还提出了一种可并行的近似算法，用于高效地生成候选的分割点。
* 缺点
  
  * 虽然利用预排序和近似算法可以降低寻找最佳分裂点的计算量，但**在节点分裂过程中仍需要遍历数据集**；
  * **预排序过程的空间复杂度过高**，不仅需要存储特征值，还需要存储特征对应样本的梯度统计值的索引，相当于**消耗了两倍的内存**。

## 15. XGBoost和LightGBM的区别

* 树生长策略
  
  * XGB采用`level-wise`的分裂策略：XGB对每一层所有节点做无差别分裂，但是可能有些节点增益非常小，对结果影响不大，带来不必要的开销。
  * LGB采用`leaf-wise`的分裂策略：Leaf-wise是在所有叶子节点中选取分裂收益最大的节点进行的，但是很容易出现过拟合问题，所以需要对最大深度做限制 。

- 分割点查找算法
  
  - XGB使用特征预排序算法，LGB使用基于直方图的切分点算法，其优势如下：
    - 减少内存占用，比如离散为256个bin时，只需要用8位整形就可以保存一个样本被映射为哪个bin(这个bin可以说就是转换后的特征)，对比预排序的exact greedy算法来说（用int_32来存储索引+ 用float_32保存特征值），可以节省7/8的空间。
    - 计算效率提高，预排序的Exact greedy对每个特征都需要遍历一遍数据，并计算增益。而直方图算法在建立完直方图后，只需要对每个特征遍历直方图即可。
  - LGB还可以使用直方图做差加速，一个节点的直方图可以通过父节点的直方图减去兄弟节点的直方图得到，从而加速计算
- 直方图算法
  
  - XGB 在每一层都动态构建直方图， 因为XGB的直方图算法不是针对某个特定的feature，而是所有feature共享一个直方图(每个样本的权重是二阶导)，所以每一层都要重新构建直方图。
  - LGB中对每个特征都有一个直方图，所以构建一次直方图就够了
- 支持离散变量
  
  - XGB无法直接输入类别型变量因此需要事先对类别型变量进行编码（例如独热编码），
  - LGB可以直接处理类别型变量。
- 缓存命中率
  
  - XGB用`block`结构的一个缺点是取梯度的时候，是通过索引来获取的，而这些梯度的获取顺序是按照特征的大小顺序的，这将导致非连续的内存访问，可能使得CPU cache缓存命中率低，从而影响算法效率。
  - LGB是基于直方图分裂特征的，梯度信息都存储在一个个bin中，所以访问梯度是连续的，缓存命中率高。
- 并行策略-特征并行
  
  - XGB每个`worker`节点中仅有部分的列数据，也就是垂直切分，每个`worker`寻找局部最佳切分点，`worker`之间相互通信，然后在具有最佳切分点的`worker`上进行节点分裂，再由这个节点广播一下被切分到左右节点的样本索引号，其他`worker`才能开始分裂。
  - LGB特征并行的前提是每个`worker`留有一份完整的数据集，但是每个`worker`仅在特征子集上进行最佳切分点的寻找；`worker`之间需要相互通信，通过比对损失来确定最佳切分点；然后将这个最佳切分点的位置进行全局广播，每个`worker`进行切分即可。
- 并行策略-数据并行
  
  - LGB中先对数据水平切分，每个`worker`上的数据先建立起局部的直方图，然后合并成全局的直方图，采用直方图相减的方式，先计算样本量少的节点的样本索引，然后直接相减得到另一子节点的样本索引，这个直方图算法使得`worker`间的通信成本降低一倍，因为只用通信以此样本量少的节点
  - XGB中的数据并行也是水平切分，然后单个`worker`建立局部直方图，再合并为全局，不同在于根据全局直方图进行各个`worker`上的节点分裂时会单独计算子节点的样本索引

## 16. GBDT的拟合值残差为什么用负梯度代替，而不是直接拟合残差

![image-20210921182812155](img/XGBoost/image-20210921182812155.png)

1. 使用残差拟合只是考虑到损失函数为平方损失的特殊情况，负梯度是更加广义上的拟合项，更具普适性。
2. 负梯度永远是函数下降最快的方向，自然也是GBDT目标函数下降最快的方向。注意并不是拟合梯度，只是用梯度去拟合。GBDT本来中的G代表Grandient，本来就是用梯度拟合；
3. 用残差去拟合，只是目标函数是均方误差的一种特殊情况，这个特殊情况跟CART拟合残差一模一样，使得看起来就拟合残差合情合理。
4. 代价函数除了loss还有正则项，正则中有参数和变量，很多情况下只拟合残差，loss变小但是正则变大，代价函数不一定就小，这时候就要用梯度啦，梯度的本质也是一种方向导数，综合了各个方向（参数）的变化，选择了一个总是最优（下降最快）的方向；
5. 最后目标函数可表达为由梯度构成，所以说成是拟合梯度，也好像不是不行

## 17. XGBoost使用二阶泰勒展开的目的和优势

- XGBoost是以MSE为基础推导出来的，在MSE的情况下，XGBoost的目标函数展开就是一阶项+二阶项的形式，而其他类似Logloss这样的目标函数不能表示成这样形式，为了后续推导的统一，所以将目标函数进行二阶泰勒展开，就可以直接定义损失函数了，只要二阶可导即可，增强了模型的扩展性
- 二阶信息能够让梯度收敛的更快，拟牛顿法比SGD收敛更快，一阶信息描述梯度变化方向，二阶信息可以描述梯度变化方向是如何变化的。

## 参考

1. https://blog.csdn.net/u010665216/article/details/78532619
2. https://blog.csdn.net/jamexfx/article/details/93780308
3. https://mp.weixin.qq.com/s/a4v9n_hUgxNyKSQ3RgDMLA
4. https://www.cnblogs.com/pinard/p/6140514.html
5. https://www.zhihu.com/question/63560633/answer/569536833

---

# 文件：AI算法\machine-learning\机器学习基础面试题.md

---

1. ⽼板给了你⼀个关于癌症检测的数据集，你构建了⼆分类器然后计算了准确率为 98%， 你是否对这个模型很满意？为什么？如果还不算理想，接下来该怎么做？
   
   首先模型主要是找出患有癌症的患者，模型关注的实际是坏样本。其次一般来说癌症的数据集中坏样本比较少，正负样本不平衡。
   
   准确率指的是分类正确的样本占总样本个数的比率
   
   $$
   \text { Accuracy }=\frac{n_{\text {correct }}}{n_{\text {total }}}
   $$
   
   其中$n_{correct}$为正确分类样本的个数，$n_{total}$为总样本分类的个数。
   
   当好样本(未患病)的样本数占99%时，模型把所有的样本全部预测为好样本也可以获得99%的准确率，所以当正负样本非常不平衡时，准确率往往会偏向占比大的类别，因此这个模型使用准确率作为模型的评估方式并不合适。
   
   鉴于模型关注的实际是坏样本，建议使用召回率(Recall )作为模型的评估函数。
   
   Recall 是分类器所预测正确的正样本占所有正样本的比例，取值范围为[0,1]，取值越大，模型预测能力越好。
   
   $$
   R=\frac{TP}{TP+FN}
   $$
   
   其次，使用类别不平衡的解决方案：
   
   常见的处理数据不平衡的方法有：重采样、Tomek links、SMOTE、NearMiss等
   
   除此之外：还可以使用模型处理：使用多种树模型算法，使用多种重采样的训练集，对少数样本预测错误增大惩罚，避免使用Accuracy，可以用confusion matrix，precision，recall，f1-score，AUC，ROC等指标。
2. 怎么判断⼀个训练好的模型是否过拟合？ 如果判断成了过拟合，那通过什么办法 可以解决过拟合问题？
   
   模型在验证集合上和训练集合上表现都很好，而在测试集合上变现很差。
   
   解决过拟合的办法：
   
   - 特征降维
   - 添加正则化，降低模型的复杂度
   - Dropout
   - Early stopping
   - 交叉验证
   - 决策树剪枝
   - 选择合适的网络结构
3. 对于线性回归，我们可以使⽤ Closed-Form Solution, 因为可以直接把导数设置 为 0，并求出参数。在这个 Closed-Form ⾥涉及到了求逆矩阵的过程，什么时候不能求出其逆矩阵？这时候如何处理？
   
   - 什么是闭式解(Closed-Form Solution)
     
     解析解(Analytical solution) 就是根据严格的公式推导，给出任意的自变量就可以求出其因变量，也就是问题的解，然后可以利用这些公式计算相应的问题。所谓的解析解是一种包含分式、三角函数、指数、对数甚至无限级数等基本函数的解的形式。用来求得解析解的方法称为解析法(Analytical techniques)，解析法即是常见的微积分技巧，例如分离变量法等。解析解是一个封闭形式(Closed-form) 的函数，因此对任一自变量，我们皆可将其带入解析函数求得正确的因变量。因此，解析解也被称为封闭解(Closed-form solution)。
     
     数值解(Numerical solution) 是采用某种计算方法，如有限元法， 数值逼近法，插值法等得到的解。别人只能利用数值计算的结果，而不能随意给出自变量并求出计算值。当无法藉由微积分技巧求得解析解时，这时便只能利用数值分析的方式来求得其数值解了。在数值分析的过程中，首先会将原方程加以简化，以利于后来的数值分析。例如，会先将微分符号改为差分（微分的离散形式）符号等，然后再用传统的代数方法将原方程改写成另一种方便求解的形式。这时的求解步骤就是将一自变量带入，求得因变量的近似解，因此利用此方法所求得的因变量为一个个离散的数值，不像解析解为一连续的分布，而且因为经过上述简化的操作，其正确性也不如解析法可靠。
     
     简而言之，解析解就是给出解的具体函数形式，从解的表达式中就可以算出任何对应值；数值解就是用数值方法求出近似解，给出一系列对应的自变量和解。
     
     参考：https://blog.csdn.net/weicao1990/article/details/90742414
   - 什么时候不能求出其逆矩阵
     
     满秩矩阵或者方阵才有逆矩阵，当一个矩阵不满秩，在对角线上存在为0的特征值，求逆的时候$\frac{1}{0}$无法计算从而不可逆，那我们给它加上一个单位矩阵，这样它就不为0了，
     
     求解的时候加上单位矩阵其实就是对线性回归引入正则化的过程
     
     参考：https://zhuanlan.zhihu.com/p/44612139
4. 关于正则，我们⼀般采⽤ L2 或者 L1, 这两个正则之间有什么区别？ 什么时候需要⽤ L2， 什么时候需要⽤ L1?
   
   L1正则化（也叫Lasso回归）是在目标函数中加上与系数的绝对值相关的项，而L2正则化（也叫岭回归）则是在目标函数中加上与系数的平方相关的项。
   
   Lasso 和岭回归系数估计是由椭圆和约束函数域的第一个交点给出的。因为岭回归的约束函数域没有尖角，所以这个交点一般不会产生在一个坐标轴上，也就是说岭回归的系数估计全都是非零的。然而，Lasso 约束函数域在每个轴上都有尖角，因此椭圆经常和约束函数域相交。发生这种情况时，其中一个系数就会等于 0。
   
   L2正则化会使参数的绝对值变小，增强模型的稳定性（不会因为数据变化而产生很大的震荡）；而L1正则化会使一些参数为零,可以实现特征稀疏, 增强模型解释性。
   
   参考：https://blog.csdn.net/zouxy09/article/details/24971995/
5. 正则项是否是凸函数？请给出证明过程。
   
   相关概念：凸集，凸函数
   
   ![image-20210703211855105](img/机器学习基础面试题/image-20210703211855105.png)
   
   ![image-20210703212000568](img/机器学习基础面试题/image-20210703212000568.png)
   
   因此证明正则项是否是凸函数，需要证明：
   
   1. $f(\boldsymbol{x})$  在 $D$ 上二阶连续可微
   2. $f(\boldsymbol{x}) $的Hessian(海塞)矩阵在 $D$上是半正定
   3. 半正定矩阵的判定定理之一：若实对称矩阵的所有顺序主子式均为非负，则该矩阵为半 正定矩阵。
      
      参考：https://www.bilibili.com/video/BV1Mh411e7VU?p=2
6. 什么叫 ElasticNet? 它主要⽤来解决什么问题？ 具体如何去优化？
   
   弹性回归是岭回归和lasso回归的混合技术，它同时使用 L2 和 L1 正则化。当有多个相关的特征时，弹性网络是有用的。lasso回归很可能随机选择其中一个，而弹性回归很可能都会选择。
   
   $$
   \hat{\beta}=\underset{\beta}{\operatorname{argmin}}\left(\|y-X \beta\|^{2}+\lambda_{2}\|\beta\|^{2}+\lambda_{1}\|\beta\|_{1}\right)
   $$
   
   - 在高度相关变量的情况下，它支持群体效应。
   - 它对所选变量的数目没有限制
   - 它具有两个收缩因子 λ1 和 λ2。
   
   参考：https://www.zhihu.com/search?type=content&q=ElasticNet
7. 基于 Coordinate Descent 算法给出 LASSO 的优化推导过程。
   
   参考：https://www.cnblogs.com/zzqingwenn/p/10874522.html
8. 请推导逻辑回归模型： ⽬标函数的构建，最优解的求解过程（SGD）需要详细写出。
9. 在数据线性可分的情况下，为什么逻辑回归模型的参数会变得⽆穷⼤？怎么避免？
10. 逻辑回归是线性还是⾮线性模型？ 为什么？ 请给出推导过程。

logistic回归属于线性模型还是非线性模型？ - 泰克尼客的回答 - 知乎 https://www.zhihu.com/question/30726036/answer/532532312

11. 我们在使⽤逻辑回归模型的时候，通常把连续性变量切分成离散型变量，为什么？ 有什么好处？
12. 朴素⻉贝叶斯应为叫 Naïve Bayes, 请说出朴素⻉贝叶斯模型的构建过程以及预测过程， 并说出为什么叫“naive”?
13. 什么叫⽣成模型，什么叫判别模型？ 朴素⻉贝叶斯，逻辑回归，HMM，语⾔模型 中哪⼀个是⽣成模型，哪⼀个是判别模型？
    
    **生成式模型**先对数据的联合分布  进行建模，然后再通过贝叶斯公式计算样本属于各类别的后验概率 。
    
    **判别式模型**直接进行条件概率建模，由数据直接学习决策函数  或条件概率分布 作为预测的模型。**判别方法不关心背后的数据分布，关心的是对于给定的输入，应该预测什么样的输出。**
    
    * **特点**
      
      **生成式模型**的特点在于，其可以从统计的角度表示数据的分布情况，能**反映同类数据本身的相似度**，不关心各类的边界在哪；
      
      而**判别式模型**直接学习的是条件概率分布，所以其**不能反映训练数据本身的特性**，其目的在于**寻找不同类别之间的最优分界面**，反映异类数据之间的差异。
      
      由生成模型可以得到判别模型，但由判别模型得不到生成模型。
      
      当存在隐变量（当我们找不到引起某一现象的原因的时候，我们就把这个在起作用但是无法确定的因素，叫“隐变量”） 时，仍可以利用生成方法学习，此时判别方法不能用。因为生成式模型同时对 x 和 y 建立概率模型，所以如果 x 中有出现没有观察到的量或者只有部分 y 的时候，就可以很自然地使用 EM 算法 来进行处理。极端情况下，在完全没有 y 信息的情况下，GM 仍然是可以工作的——无监督学习可以看成是 GM 的一种。
    
    |                | 优点                                                                                                                                                                                                                                                                                                                                                              | 缺点                                                                                                                                                                                                                                                                         | 代表算法                                                               |
| -------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------- |
| **生成式模型** | 1.  由于统计了数据的分布情况，所以其实际带的信息要比判别模型丰富，对于研究单类问题来说也比判别模型灵活性强；2. 模型可以通过增量学习得到（增量学习是指一个学习系统能不断地从新样本中学习新的知识，并能保存大部分以前已经学习到的知识。）； 3. 收敛速度更快，当样本容量增加的时，生成模型可以更快的收敛于真实模型； 4. 隐变量存在时，也可以使用。 | 1. 学习和计算过程比较复杂，由于学习了更多的样本信息，所以计算量大，如果我们只是做分类，就浪费了这部分的计算量；2. 准确率较差； 3. 往往需要对特征进行假设，比如朴素贝叶斯中需要假设特征间独立同分布，所以如果所选特征不满足这个条件，将极大影响生成式模型的性能。 | 朴素贝叶斯、贝叶斯网络、隐马尔可夫模型、隐马尔可夫模型                 |
| **判别式模型** | 1. 由于关注的是数据的边界，所以能清晰的分辨出多类或某一类与其他类之间的差异，所以准确率相对较高；2. 计算量较小，需要的样本数量也较小；                                                                                                                                                                                                                      | 1. 不能反映训练数据本身的特性；2. 收敛速度较慢                                                                                                                                                                                                                         | k 近邻法、决策树、逻辑斯谛回归模型、最大熵模型、支持向量机、条件随机场 |
    
    参考 ：https://cloud.tencent.com/developer/article/1544597
14. 决策树与随机森林的区别是什么？ 如果让你选择，你会使⽤哪个模型，为什么？
15. 请介绍 k-means 算法的流程， 写出 k-means 模型的⽬标函数。K-means 求解 过程跟 EM 算法之间有什么关系？ K-MEANS ⽬标函数是否是 convex?
16. 什么叫 EM 算法？有哪些经典模型的求解过程会⽤到 EM 算法？
17. EM 算法是否⼀定会收敛？EM 算法给出的全局最优还是局部最优？
18. 请解释什么叫 MLE，什么叫 MAP? 请说明它们之间的区别。 在数据量⽆穷多的 时候，是否 MAP 趋近于 MLE 估计？
19. 请解释什么叫召回率，精确率，F1 Measure，ROC, AUC? 什么时候需要⽤到这 些？
20. 数据集拥有⾮常多的特征，但样本个数有限，所以计划做特征选择，有哪些⽅法 可以⽤来做特征选择呢？
21. 随机森林和 Gradient Boosting Tree 之间的区别是什么？
22. 在构建决策树模型的时候，我们⼀般不会构建到底，也就做⼀些剪枝的操作，为 什么？ 然⽽，在构建随机森林的时候剪枝的操作不像决策树⾥那么重要，为什么？
23. 什么样的数据是不均衡数据（imbalance data）？ 需要怎么样的处理？
24. 什么是 kernel trick? 它有什么好处？并写出 RBF kernel, Gaussian Kernel 的公 式。
25. 什么 Mercer’s Theorem， 阐述⼀下具体的细节。
26. 使⽤⾮线性 Kernel 有哪些优缺点？重点介绍⼀下效率上的缺点，并说明为什么会 产⽣效率上的缺点？
27. SVM 是 margin-based classifier, 试着推导 SVM，并说明什么是 KKT 条件。
28. 如果不考虑 kernel， 逻辑回归和 SVM 的区别是什么？
29. 在随机梯度下降法⾥怎么有效地选择学习率?有哪些常⻅见的动态改变学习率的策略？
30. 深度学习是什么？ 它跟所谓的传统的学习模型有什么本质的区别？从模型的 Capacity, Hierarchical Representation 的⻆角度举例说明。
31. PCA 的原理是什么？ 推导⼀下 PCA 的过程。
32. 什么叫 PAC 理论？ 它主要⽤来解决什么问题？
33. 解释⼀下矩阵分解算法以及怎么⽤到推荐系统⾥，并利⽤梯度下降法来推导矩阵 分解过程。
34. 模型参数和超参数的区别是什么？
    
    参考：https://mp.weixin.qq.com/s/Nwd0Dm2_D1eY3n4z_Fw1FA
35. 什么叫因变量，以及因变量模型？
36. 超参数的选择⽅法有哪些？⾄少列出 4 种以上来说明，并说出其优缺点。
37. 什么是 XGBoost 模型？说明⼀下其技术细节。
38. 怎么把 K-means 算法应⽤到⼤规模的数据上？ 有什么 Scalable 的⽅法？(hint:mini-batch, triangle inequality)
39. K-means 算法与 GMM 之间有什么关系？
40. 在深度学习模型⾥，有哪些技术可以⽤来避免过拟合现象？
41. CNN ⾥⾯ POOLING 的作⽤是什么？ 卷积的作⽤是什么？
42. 在分类问题⾥，最后⼀层通常使⽤ softmax，请写 softmax 函数。
43. 描述⼀下 SGD, Adagrad, Adam 算法之间的区别，什么时候使⽤ SGD？ Adagrad？ Adam 算法？
44. 简 单 描 述 ⼀ 下 什 么 是 Variational Autoencoder(VAE), 什 么 是 Generative Adversial Network(GAN)
45. Dropout 和 Bagging 模型的关系是什么？ 为什么 Dropout 可以起到避免过拟合 的作⽤？
46. 对于拥有两层隐含层的神经⽹络（MLP）, 请⼿动推导其 BP 算法的细节。
47. 使⽤ KNN 会遇到⼀些效率上的问题，请说明如何使⽤ LSH（latent semantic hashing）来做近似操作？
48. 在朴素⻉贝叶斯和语⾔模型中我们通常会使⽤ smoothing 技术，请简述⼏个常⻅见 的 smoothing ⽅法以及它们优缺点。
    
    参考：https://mp.weixin.qq.com/s?__biz=MzA3NjIyOTk4OA==&mid=2247484173&idx=1&sn=02f133259224c86dd5f573f226e7f0a7&scene=19#wechat_redirect

---

# 文件：AI算法\machine-learning\梯度下降.md

---

# 梯度下降法面试题

## 1. 机器学习中为什么需要梯度下降

梯度下降的作用：

* 梯度下降是迭代法的一种，可以用于**求解最小二乘问题**。
* 在求解损失函数的最小值时，可以通过梯度下降法来一步步的迭代求解，**得到最小化的损失函数和模型参数值。**
* 如果我们需要求解损失函数的最大值，可通过梯度上升法来迭代。**梯度下降法和梯度上升法可相互转换**。

## 2. 梯度下降法缺点

**缺点**：

* 靠近极小值时收敛速度减慢。
* 直线搜索时可能会产生一些问题。
* 可能会“之字形”地下降。

**注意**：

* 梯度是一个向量，即**有方向有大小**。
* 梯度的方向是**最大方向导数的方向**。
* 梯度的值是**最大方向导数的值**。

## 3. 梯度下降法直观理解

假如最开始，我们在一座大山上的某处位置，因为到处都是陌生的，不知道下山的路，所以只能摸索着根据直觉，走一步算一步，在此过程中，每走到一个位置的时候，都会求解当前位置的梯度，沿着梯度的负方向，也就是当前最陡峭的位置向下走一步，然后继续求解当前位置梯度，向这一步所在位置沿着最陡峭最易下山的位置走一步。不断循环求梯度，就这样一步步地走下去，一直走到我们觉得已经到了山脚。当然这样走下去，有可能我们不能走到山脚，而是到了某一个局部的山势低处。
由此，从上面的解释可以看出，梯度下降不一定能够找到全局的最优解，有可能是一个局部的最优解。当然，如果损失函数是凸函数，梯度下降法得到的解就一定是全局最优解。

## 4. 梯度下降核心思想归纳

* 确定优化模型的假设函数及损失函数。
* 初始化参数，随机选取取值范围内的任意数；

- 迭代操作：
  - 计算当前梯度
  - 修改新的变量
  - 计算朝最陡的下坡方向走一步
  - 判断是否需要终止，如否，**梯度更新**
- 得到全局最优解或者接近全局最优解。

## 5. 如何对梯度下降法进行调优

实际使用梯度下降法时，各项参数指标不能一步就达到理想状态，对梯度下降法调优主要体现在以下几个方面：

(1)**算法迭代步长$\alpha$选择。**
在算法参数初始化时，有时根据经验将步长初始化为1。实际取值取决于数据样本。可以从大到小，多取一些值，分别运行算法看迭代效果，如果损失函数在变小，则取值有效。如果取值无效，说明要增大步长。但步长太大，有时会导致迭代速度过快，错过最优解。步长太小，迭代速度慢，算法运行时间长。

(2)**参数的初始值选择。**
初始值不同，获得的最小值也有可能不同，梯度下降有可能得到的是局部最小值。如果损失函数是凸函数，则一定是最优解。由于有局部最优解的风险，需要多次用不同初始值运行算法，关键损失函数的最小值，选择损失函数最小化的初值。

(3)**标准化处理。**
由于样本不同，特征取值范围也不同，导致迭代速度慢。为了减少特征取值的影响，可对特征数据标准化，使新期望为0，新方差为1，可节省算法运行时间。

## 6. 随机梯度和批量梯度区别

```
随机梯度下降(SDG)和批量梯度下降(BDG)是两种主要梯度下降法，其目的是增加某些限制来加速运算求解。
下面通过介绍两种梯度下降法的求解思路，对其进行比较。
假设函数为：
```

$$
h_\theta (x_0,x_1,...,x_3) = \theta_0 x_0 + \theta_1 x_1 + ... + \theta_n x_n
$$

损失函数为：

$$
J(\theta_0, \theta_1, ... , \theta_n) = 
			\frac{1}{2m} \sum^{m}_{j=0}(h_\theta (x^{j}_0
	,x^{j}_1,...,x^{j}_n)-y^j)^2
$$

其中，$m$为样本个数，$j$为参数个数。

1、 **批量梯度下降的求解思路如下：**
a) 得到每个$\theta$对应的梯度：

$$
\frac{\partial}{\partial \theta_i}J({\theta}_0,{\theta}_1,...,{\theta}_n)=\frac{1}{m}\sum^{m}_{j=0}(h_\theta (x^{j}_0
	,x^{j}_1,...,x^{j}_n)-y^j)x^{j}_i
$$

b) 由于是求最小化风险函数，所以按每个参数 $\theta$ 的梯度负方向更新 $ \theta_i $ ：

$$
\theta_i=\theta_i - \frac{1}{m} \sum^{m}_{j=0}(h_\theta (x^{j}_0
	,x^{j}_1,...,x^{j}_n)-y^j)x^{j}_i
$$

c) 从上式可以注意到，它得到的虽然是一个全局最优解，但每迭代一步，都要用到训练集所有的数据，如果样本数据很大，这种方法迭代速度就很慢。
相比而言，随机梯度下降可避免这种问题。

2、**随机梯度下降的求解思路如下：**
a) 相比批量梯度下降对应所有的训练样本，随机梯度下降法中损失函数对应的是训练集中每个样本的粒度。
损失函数可以写成如下这种形式，

$$
J(\theta_0, \theta_1, ... , \theta_n) = 
			\frac{1}{m} \sum^{m}_{j=0}(y^j - h_\theta (x^{j}_0
			,x^{j}_1,...,x^{j}_n))^2 = 
			\frac{1}{m} \sum^{m}_{j=0} cost(\theta,(x^j,y^j))
$$

b)对每个参数 $ \theta$ 按梯度方向更新 $ \theta$：

$$
\theta_i = \theta_i + (y^j - h_\theta (x^{j}_0, x^{j}_1, ... ,x^{j}_n))
$$

c) 随机梯度下降是通过每个样本来迭代更新一次。
随机梯度下降伴随的一个问题是噪音较批量梯度下降要多，使得随机梯度下降并不是每次迭代都向着整体最优化方向。

**小结：**
随机梯度下降法、批量梯度下降法相对来说都比较极端，简单对比如下：

|     方法     | 特点                                                                                                                                                                                                                                          |
| :----------: | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 批量梯度下降 | a)采用所有数据来梯度下降。b)批量梯度下降法在样本量很大的时候，训练速度慢。                                                                                                                                                               |
| 随机梯度下降 | a)随机梯度下降用一个样本来梯度下降。b)训练速度很快。c)随机梯度下降法仅仅用一个样本决定梯度方向，导致解有可能不是全局最优。d)收敛速度来说，随机梯度下降法一次迭代一个样本，导致迭代方向变化很大，不能很快的收敛到局部最优解。 |

下面介绍能结合两种方法优点的小批量梯度下降法。

3、 **小批量(Mini-Batch)梯度下降的求解思路如下**
对于总数为$m$个样本的数据，根据样本的数据，选取其中的$n(1< n< m)$个子样本来迭代。其参数$\theta$按梯度方向更新$\theta_i$公式如下：

$$
\theta_i = \theta_i - \alpha \sum^{t+n-1}_{j=t}
		( h_\theta (x^{j}_{0}, x^{j}_{1}, ... , x^{j}_{n} ) - y^j ) x^{j}_{i}
$$

## 7.  各种梯度下降法性能比较

```
下表简单对比随机梯度下降(SGD)、批量梯度下降(BGD)、小批量梯度下降(Mini-batch GD)、和Online GD的区别：
```

|                |    BGD    |   SGD   | Mini-batch GD |   Online GD   |
| :------------: | :--------: | :------: | :-----------: | :------------: |
|     训练集     |    固定    |   固定   |     固定     |    实时更新    |
| 单次迭代样本数 | 整个训练集 | 单个样本 | 训练集的子集 | 根据具体算法定 |
|   算法复杂度   |     高     |    低    |     一般     |       低       |
|     时效性     |     低     |   一般   |     一般     |       高       |
|     收敛性     |    稳定    |  不稳定  |    较稳定    |     不稳定     |

```
Online GD于Mini-batch GD/SGD的区别在于，所有训练数据只用一次，然后丢弃。这样做的优点在于可预测最终模型的变化趋势。
```

```
Online GD在互联网领域用的较多，比如搜索广告的点击率(CTR)预估模型，网民的点击行为会随着时间改变。用普通的BGD算法(每天更新一次)一方面耗时较长(需要对所有历史数据重新训练)；另一方面，无法及时反馈用户的点击行为迁移。而Online GD算法可以实时的依据网民的点击行为进行迁移。
```

## 8. 推导多元函数梯度下降法的迭代公式。

根据多元函数泰勒公式，如果忽略一次以上的项，函数在$\mathbf{x}$点处可以展开为

$$
f(\mathbf{x}+\Delta \mathbf{x})=f(\mathbf{x})+(\nabla f(\mathbf{x}))^{\mathrm{T}} \Delta \mathbf{x}+o(\|\mathbf{\Delta} \mathbf{x}\|)
$$

对上式变形，函数的增量与自变量增量、函数梯度的关系为

$$
f(\mathbf{x}+\Delta \mathbf{x})-f(\mathbf{x})=(\nabla f(\mathbf{x}))^{\mathrm{T}} \Delta \mathbf{x}+o(\|\Delta \mathbf{x}\|)
$$

如果令$\Delta \mathbf{x}=-\nabla f(\mathbf{x})$则有

$$
f(\mathbf{x}+\Delta \mathbf{x})-f(\mathbf{x}) \approx-(\nabla f(\mathbf{x}))^{\mathrm{T}} \nabla f(\mathbf{x}) \leq 0
$$

即函数值减小。即有

$$
f(\mathbf{x}+\Delta \mathbf{x}) \leq f(\mathbf{x})
$$

梯度下降法每次的迭代增量为

$$
\Delta \mathbf{x}=-\alpha \nabla f(\mathbf{x})
$$

其中$\alpha$为人工设定的接近于的正数，称为步长或学习率。其作用是保证$\mathbf{x}+\Delta \mathbf{x}$在$\mathbf{x}$的
邻域内，从而可以忽略泰勒公式中的$o(\|\Delta \mathbf{x}\|)$项。

使用该增量则有

$$
(\nabla f(\mathbf{x}))^{\mathrm{T}} \Delta \mathbf{x}=-\alpha(\nabla f(\mathbf{x}))^{\mathrm{T}}(\nabla f(\mathbf{x})) \leq 0
$$

函数值下降。从初始点$\mathbf{x}_{0}$开始，反复使用如下**迭代公式**

$$
\mathbf{x}_{k+1}=\mathbf{x}_{k}-\alpha \nabla f\left(\mathbf{x}_{k}\right)
$$

只要没有到达梯度为0的点，函数值会沿序列$\mathbf{x}_{k}$递减，最终收敛到梯度为0 的点。从$\mathbf{x}_{0}$
出发，用**迭代公式**进行迭代，会形成一个函数值递减的序列$\left\{\mathbf{x}_{i}\right\}$

$$
f\left(\mathbf{x}_{0}\right) \geq f\left(\mathbf{x}_{1}\right) \geq f\left(\mathbf{x}_{2}\right) \geq \ldots \geq f\left(\mathbf{x}_{k}\right)
$$

## 9.  梯度下降法如何判断是否收敛？

迭代终止的条件是函数的梯度值为0(实际实现时是接近于0 即可)，此时认为已经达
到极值点。可以通过判定梯度的二范数是否充分接近于0 而实现。

## 10. 梯度下降法为什么要在迭代公式中使用步长系数？

其作用是保证$\mathbf{x}+\Delta \mathbf{x}$在$\mathbf{x}$的邻域内，即控制增量的步长，从而可以忽略泰勒公式中的
$o(\|\Delta \mathbf{x}\|)$项。否则不能保证每次迭代时函数值下降。

## 11. 梯度下降法和牛顿法能保证找到函数的极小值点吗，为什么？

不能，可能收敛到鞍点，不是极值点。

## 12. 解释一元函数极值判别法则。

假设$x_0$为函数的驻点，可分为以下三种情况。
case1：在该点处的二阶导数大于0，则为函数的极小值点；
case2：在该点处的二阶导数小于0，则为极大值点；
case3：在该点处的二阶导数等于0，则情况不定，可能是极值点，也可能不是极值点。

## 13. 解释多元函数极值判别法则。

假设多元函数在点M的梯度为0 ，即M 是函数的驻点。其Hessian 矩阵有如下几种情
况。
case1：Hessian 矩阵正定，函数在该点有极小值。
case2：Hessian 矩阵负定，函数在该点有极大值。
case3：Hessian 矩阵不定，则不是极值点，称为鞍点。
Hessian 矩阵正定类似于一元函数的二阶导数大于0，负定则类似于一元函数的二阶导
数小于0。

## 14. 什么是鞍点？

**Hessian 矩阵不定的点称为鞍点**，它不是函数的极值点。

## 15. 解释什么是局部极小值，什么是全局极小值。

* 全局极小值
  * 假设$\mathbf{x}^{*}$是一个可行解，如果对可行域内所有点$\mathbf{x}$都有$f\left(\mathbf{x}^{*}\right) \leq f(\mathbf{x})$，则
    称$\mathbf{x}^{*}$为全局极小值。
* 局部极小值
  * 对于可行解$\mathbf{x}^{*}$，如果存在其$\delta$邻域，使得该邻域内的所有点即所有满足
    $\left\|\mathbf{x}-\mathbf{x}^{*}\right\| \leq \delta$的点$\mathbf{x}$，都有$f\left(x^{*}\right) \leq f(x)$，则称$\mathbf{x}^{*}$为局部极小值。

## 16. 推导多元函数牛顿法的迭代公式。

根据费马定理，函数在点$\mathbf{x}$ 处取得极值的必要条件是梯度为0

$$
\nabla f(\mathbf{x})=\mathbf{0}
$$

对于一般的函数，直接求解此方程组存在困难。对目标函数在$\mathbf{x}_{0}$ 处作二阶泰勒展开

$$
f(\mathbf{x})=f\left(\mathbf{x}_{0}\right)+\nabla f\left(\mathbf{x}_{0}\right)^{\mathrm{T}}\left(\mathbf{x}-\mathbf{x}_{0}\right)+\frac{1}{2}\left(\mathbf{x}-\mathbf{x}_{0}\right)^{\mathrm{T}} \nabla^{2} f\left(\mathbf{x}_{0}\right)\left(\mathbf{x}-\mathbf{x}_{0}\right)+o\left(\left\|\mathbf{k}-\mathbf{x}_{0}\right\|^{2}\right)
$$

忽略二次以上的项，将目标函数近似成二次函数，等式两边同时对$\mathbf{x}$求梯度，可得

$$
\nabla f(\mathbf{x}) \approx \nabla f\left(\mathbf{x}_{0}\right)+\nabla^{2} f\left(\mathbf{x}_{0}\right)\left(\mathbf{x}-\mathbf{x}_{0}\right)
$$

其中 $\nabla^{2} f\left(\mathbf{x}_{0}\right)$为在$\mathbf{x}_{0}$ 处的Hessian 矩阵。令函数的梯度为0 ，有

$$
\nabla f\left(\mathbf{x}_{0}\right)+\nabla^{2} f\left(\mathbf{x}_{0}\right)\left(\mathbf{x}-\mathbf{x}_{0}\right)=\mathbf{0}
$$

解这个线性方程组可以得到

$$
\tag{1}\mathbf{x}=\mathbf{x}_{0}-\left(\nabla^{2} f\left(\mathbf{x}_{0}\right)\right)^{-1} \nabla f\left(\mathbf{x}_{0}\right)
$$

如果将梯度向量简写为$\mathbf{g}$ ，Hessian 矩阵简记为$\mathbf{H}$ ，式(1)可以简写为

$$
\tag{2}\mathbf{x}=\mathbf{x}_{0}-\mathbf{H}^{-1} \mathbf{g}
$$

在泰勒公式中忽略了高阶项将函数做了近似，因此这个解不一定是目标函数的驻点，需要反复用式(2) 进行迭代。从初始点$\mathbf{x}_{0}$处开始，计算函数在当前点处的Hessian 矩阵和梯度向量，然后用下面的公式进行迭代

$$
\tag{3} \mathbf{x}_{k+1}=\mathbf{x}_{k}-\alpha \mathbf{H}_{k}^{-1} \mathbf{g}_{k}
$$

直至收敛到驻点处。迭代终止的条件是梯度的模接近于0 ，或达到指定的迭代次数。其中$\alpha$是人工设置的学习率。需要学习率的原因与梯度下降法相同，是为了保证能够忽略泰勒公式中的高阶无穷小项。

## 参考资料：

深度学习500问： https://github.com/scutan90/DeepLearning-500-questions

机器学习与深度学习习题集答案-1：https://mp.weixin.qq.com/s/4kWUE8ml_o6iF0F1TREyiA

---

# 文件：AI算法\machine-learning\线性回归+逻辑回归.md

---

# 线性回归于逻辑回归面试题

## 1. 简单介绍一下线性回归。

**线性回归(Linear Regression)是利用称为线性回归方程的最小平方函数对一个或多个自变量和因变量之间关系进行建模的一种回归分析。**这种函数是一个或多个称为回归系数的模型参数的线性组合。只有一个自变量的情况称为简单回归,大于一个自变量情况的叫做多元回归。

- 线性：两个变量之间的关系**是**一次函数关系的——图象**是直线**，叫做线性。
- 非线性：两个变量之间的关系**不是**一次函数关系的——图象**不是直线**，叫做非线性。
- 回归：人们在测量事物的时候因为客观条件所限，求得的都是测量值，而不是事物真实的值，为了能够得到真实值，无限次的进行测量，最后通过这些测量数据计算**回归到真实值**，这就是回归的由来。

线性回归就是利用的样本$D=(\mathrm{x}_i, \mathrm{y}_i)
$，$ \mathrm{i}=1,2,3 \ldots \mathrm{N}, \mathrm{x}_i$是特征数据，可能是一个，也可能是多个，通过有监督的学习，学习到由$x$到$y$的映射$h$，利用该映射关系对未知的数据进行预估，因为$y$为连续值，所以是回归问题。

## 2. 线性回归的假设函数是什么形式？

线性回归的假设函数（$\theta_{0}$表示截距项，$ x_{0} = 1$，方便矩阵表达）：

$$
f(x)=\theta_{0} x_{0}+\theta_{1} x_{1}+\theta_{2} x_{2} \ldots+\theta_{n} x_{n}  = \theta ^TX
$$

其中$\theta,X$都是列向量

## 3. 线性回归的损失函数是什么形式？

一般使用**最小二乘法**，真实值$y_{i}$，预测值$h_θ(x)$，则误差平方为$\left(y_{i}-h_{\theta}\left(x_{i}\right)\right)^{2}$找到合适的参数，使得误差平方和最小

$$
MSE: J\left(\theta_{0}, \theta_{1}\right)=\frac{1}{2 m} \sum_{i=1}^{m}\left(y_{i}-h_{\theta}\left(x_{i}\right)\right)^{2}
$$

其中共有$m$个样本点，乘以1/2是为了方便计算。

参考资料：https://www.cnblogs.com/xym4869/p/11309134.html

## 4. 简述岭回归与Lasso回归以及使用场景。

* 目的：
  
  * 解决线性回归出现的过拟合的请况。
  * 解决在通过正规方程方法求解$\theta$的过程中出现的$X^TX$不可逆的请况。
* 本质：
  
  * 约束(限制)要优化的参数

这两种回归均通过在损失函数中引入**正则化项**来达到目的：

**线性回归的损失函数：**

$$
J(\theta)=\frac{1}{2 m} \sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right)^{2}
$$

* **岭回归**
  * 损失函数：

$$
J(\theta)=\frac{1}{2 m} \sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right)^{2}+\lambda \sum_{j=1}^{n} \theta_{j}^{2}
$$

* **Lasso回归**
  * 损失函数

$$
J(\theta)=\frac{1}{2 m} \sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right)^{2}+\lambda \sum_{j=1}^{n} |\theta_{j}|
$$

* 补充
  
  * **ElasticNet 回归**： 线性回归 + L1正则化 + L2 正则化。
    
    * ElasticNet在我们发现用Lasso回归太过(太多特征被稀疏为0),而岭回归也正则化的不够(回归系数衰减太慢)的时候，可以考虑使用ElasticNet回归来综合，得到比较好的结果。
    * 损失函数
      
      $$
      J(\theta)=\frac{1}{2} \sum_{i}^{m}\left(y^{(i)}-\theta^{T} x^{(i)}\right)^{2}+\lambda\left(\rho \sum_{j}^{n}\left|\theta_{j}\right|+(1-\rho) \sum_{j}^{n} \theta_{j}^{2}\right)
      $$
  * **LWR( 局部加权)回归**：
    
    * 局部加权线性回归是在线性回归的基础上对每一个测试样本（训练的时候就是每一个训练样本）在其已有的样本进行一个加权拟合，**权重的确定**可以通过一个核来计算，常用的有**高斯核**（离测试样本越近，权重越大，反之越小），这样对每一个测试样本就得到了不一样的权重向量，所以最后得出的拟合曲线不再是线性的了，这样就增加的模型的复杂度来更好的拟合非线性数据。
    * 损失函数
      
      $$
      J(\theta)=\frac{1}{2} \sum_{i=1}^{m} w^{(i)}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right)^{2}
      $$

## 5. 线性回归要求因变量服从正态分布吗？

**线性回归的假设前提是噪声服从正态分布，即因变量服从正态分布。但实际上难以达到，因变量服从正态分布时模型拟合效果更好。**

参考资料： http://www.julyedu.com/question/big/kp_id/23/ques_id/2914

## 6. 简单介绍一下逻辑回归

逻辑回归主要用来解决**分类**问题，线性回归的结果$Y$带入一个非线性变换的**Sigmoid函数**中，得到$[0,1]$之间取值范围的数$S$，$S$可以把它看成是一个概率值，如果我们设置概率阈值为0.5，那么$S$大于0.5可以看成是正样本，小于0.5看成是负样本，就可以进行分类了。

## 7. 简单介绍一下Sigmoid函数

sigmoid函数取值范围为(0,1)，它可以将一个实数映射到(0,1)的区间，可以用来做二分类。
函数公式如下：

$$
S(t)=\frac{1}{1+e^{-t}}
$$

函数中$t$无论取什么值，其结果都在$[0,{1}]$的区间内，我们假设分类的**阈值**是0.5，那么超过0.5的归为1分类，低于0.5的归为0分类，阈值是可以自己设定的。

下来我们把线性回归的输出 $\theta^T X+b$ 带入 $t$ 中就得到了逻辑回归的**假设函数**：

$$
H(\theta, b)=\frac{1}{1+e^{(\theta^T X+b)}}
$$

结果也可以理解为概率，换句话说概率大于0.5的属于1分类，概率小于0.5的属于0分类，这就达到了分类的目的。

## 8. 逻辑回归的损失函数是什么

逻辑回归的损失函数是**交叉熵损失函数**：

$$
\operatorname{cost}\left(h_{\theta}(x), y\right)=\left\{\begin{aligned}  h_{\theta}(x)  & \qquad  y=1 \\ 1-h_{\theta}(x)  & \qquad  y=0 \end{aligned}\right.
$$

**两式合并**得到**概率分布表达式**：

$$
(P(y|x,\theta ) = h_{\theta}(x)^y(1-h_{\theta}(x))^{1-y})
$$

**对数似然函数最大化**得到**似然函数的代数表达式**为 ：

$$
L(\theta) = \prod\limits_{i=1}^{m}(h_{\theta}(x^{(i)}))^{y^{(i)}}(1-h_{\theta}(x^{(i)}))^{1-y^{(i)}}
$$

**取反**得到**损失函数表达式——(负对数损失函数)** ：

$$
J(\theta) = -lnL(\theta) = -\sum\limits_{i=1}^{m}(y^{(i)}log(h_{\theta}(x^{(i)}))+ (1-y^{(i)})log(1-h_{\theta}(x^{(i)})))
$$

## 9.逻辑回归如何进行多分类？

多分类问题一般将二分类推广到多分类的方式有三种，一对一，一对多，多对多。

* 一对一：
  
  * 将$N$个类别两两配对，产生$N(N-1)/2$个二分类任务，测试阶段新样本同时交给所有的分类器，最终结果通过投票产生。
* 一对多：
  
  * 每一次将一个例作为正例，其他的作为反例，训练$N$个分类器，测试时如果只有一个分类器预测为正类，则对应类别为最终结果，如果有多个，则一般选择置信度最大的。从分类器角度一对一更多，但是每一次都只用了2个类别，因此当类别数很多的时候一对一开销通常更小(只要训练复杂度高于$O(N)$即可得到此结果)。
* 多对多：
  
  * 若干各类作为正类，若干个类作为反类。注意正反类必须特殊的设计。

## 10.逻辑回归的优缺点

* 优点
  
  * LR的可解释性强、可控度高、训练速度快
* 缺点
  
  * 对模型中自变量多重共线性较为敏感
    
    例如两个高度相关自变量同时放入模型，可能导致较弱的一个自变量回归符号不符合预期，符号被扭转。需要利用因子分析或者变量聚类分析等手段来选择代表性的自变量，以减少候选变量之间的相关性；
  * 预测结果呈$S$型，因此从$log(odds)$向概率转化的过程是非线性的，在两端随着$log(odds)$值的变化，概率变化很小，边际值太小，slope太小，而中间概率的变化很大，很敏感。 导致很多区间的变量变化对目标概率的影响没有区分度，无法确定阀值。

## 11. 逻辑斯特回归为什么要对特征进行离散化。

* 逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合； 离散特征的增加和减少都很容易，易于模型的快速迭代；
* 稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展；
* 方便交叉与特征组合：离散化后可以进行特征交叉，由$M+N$个变量变为$M*N$个变量，进一步引入非线性，提升表达能力；
* 简化模型：特征离散化以后，起到了简化了逻辑回归模型的作用，降低了模型过拟合的风险。
* 稳定性：特征离散化后，模型会更稳定，比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人；
* 离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄>30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰。
  
  参考资料：https://blog.csdn.net/qq1195365047/article/details/88638809

## 12.  线性回归与逻辑回归的区别

* 线性回归主要解决回归任务，逻辑回归主要解决分类问题。
* 线性回归的输出一半是连续的，逻辑回归的输出一般是离散的。
* 逻辑回归的输入是线性回归的输出，将Sigmoid函数作用于线性回归的输出得到输出结果。

- 线性回归的损失函数是$MSE$,逻辑回归中，采用的是负对数损失函数
  
  参考资料：https://blog.csdn.net/ddydavie/article/details/82668141

## 13. 为什么逻辑回归比线性回归要好？

逻辑回归和线性回归首先都是广义的线性回归，其次模型的优化目标函数是最小二乘，而逻辑回归则是似然函数，逻辑回归在线性回归的基础上，在特征到结果的映射中加入了一层sigmoid函数（非线性）映射，即先把特征线性求和，然后使用sigmoid函数来预测。另外线性回归在整个实数域范围内进行预测，敏感度一致，而分类范围，需要在0,1间的一种回归模型，因而对于这类问题来说，逻辑回归的鲁棒性比线性回归的要好。

参考资料：https://www.deeplearn.me/1788.html

## 14. 逻辑回归有哪些应用

- CTR预估/推荐系统的learning to rank/各种分类场景。
- 某搜索引擎厂的广告CTR预估基线版是LR。
- 某电商搜索排序/广告CTR预估基线版是LR。
- 某电商的购物搭配推荐用了大量LR。
- 某现在一天广告赚1000w+的新闻app排序基线是LR。

## 15. 如果label= {-1, +1}，给出LR的损失函数？

假设label={-1,+1},则

$$
p(y=1|x)=h_{\omega}(x)
$$

$$
p(y=-1 | x) = 1 - h_{\omega} (x)
$$

对于sigmoid函数，有以下特性，

$$
h(-x) = 1 - h(x)
$$

$$
p(y|x) = h_\omega(yx)
$$

同样，我们使用MLE作估计，

$$
\begin{aligned}
L(\omega)&=  \prod_{i=1}^{m} p(y_i | x_i; \omega)  \\
&=  \prod_{i=1}^{m} h_\omega(y_i x_i)\\
&= \prod_{i=1}^{m} \frac{1}{1+e^{-y_iwx_i}}
\end{aligned}
$$

对上式取对数及负值，得到损失为：

$$
\begin{aligned}
-\log L(\omega)&= -\log \prod_{i=1}^{m} p(y_i | x_i; \omega)  \\
&=  -\sum_{i=1}^{m} \log p(y_i | x_i; \omega)  \\
&=  -\sum_{i=1}^{m} \log \frac{1}{1+e^{-y_iwx_i}}\\
&=  \sum_{i=1}^{m} \log(1+e^{-y_iwx_i})\\
\end{aligned}
$$

即对于每一个样本，损失函数为：

$$
L(\omega)=\log(1+e^{-y_iwx_i})
$$

## 16. 逻辑回归在训练的过程当中，如果有很多的特征高度相关或者说有一个特征重复了100遍，会造成怎样的影响？

如果在损失函数最终收敛的情况下，有很多特征高度相关也不会影响分类器的效果。对特征本身来说的话，假设只有一个特征，在不考虑采样的情况下，你现在将它重复100遍。训练以后完以后，数据还是这么多，但是这个特征本身重复了100遍，实质上将原来的特征分成了100份，每一个特征都是原来特征权重值的百分之一。如果在随机采样的情况下，其实训练收敛完以后，还是可以认为这100个特征和原来那一个特征扮演的效果一样，只是可能中间很多特征的值正负相消了。

## 17. LR为什么使用sigmoid函数？

- 线性模型的输出都是在$[-∞,+∞]$之间的，而Sigmoid能够把它映射到$[0,1]$之间。正好这个是概率的范围。
- Sigmoid是连续光滑的。
- 根据Sigmoid函数，最后推导下来逻辑回归其实就是最大熵模型，根据最大似然估计得到的模型的损失函数就是logloss。这让整个逻辑回归都有理可据。
- Sigmoid也让逻辑回归的损失函数成为凸函数，这也是很好的性质。
- 逻辑回归的损失函数是二元分类的良好代理函数，这个也是Sigmoid的功劳。

参考资料：http://sofasofa.io/forum_main_post.php?postid=1004244

## 18. LR如何进行并行计算？

参考资料：http://blog.sina.com.cn/s/blog_6cb8e53d0101oetv.html

## 19.LR和SVM有什么不同吗

+ LR和SVM都可以处理分类问题，且一般都用于处理线性二分类问题（在改进的情况下可以处理多分类问题）
+ 两个方法都可以增加不同的正则化项，如l1、l2等等。所以在很多实验中，两种算法的结果是很接近的。
  区别：
+ LR是参数模型，SVM是非参数模型。
+ 从目标函数来看，区别在于逻辑回归采用的是交叉熵损失函数，SVM采用的是hinge loss，这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重。
+ SVM的处理方法是只考虑support vectors，也就是和分类最相关的少数点，去学习分类器。而逻辑回归通过非线性映射，大大减小了离分类平面较远的点的权重，相对提升了与分类最相关的数据点的权重。
+ 逻辑回归相对来说模型更简单，好理解，特别是大规模线性分类时比较方便。而SVM的理解和优化相对来说复杂一些，SVM转化为对偶问题后,分类只需要计算与少数几个支持向量的距离,这个在进行复杂核函数计算时优势很明显,能够大大简化模型和计算。
+ LR能做的 SVM能做，但可能在准确率上有问题，SVM能做的LR有的做不了。

---

# 文件：AI算法\machine-learning\过拟合与欠拟合.md

---

# 过拟合欠拟合面试题

## 1. 如何理解高方差与低偏差?

模型的预测误差可以分解为三个部分: 偏差(bias)， 方差(variance) 和噪声(noise).

**偏差**

- 偏差度量了模型的期望预测与真实结果的偏离程度， 即刻画了学习算法本身的拟合能力。偏差则表现为在特定分布上的适应能力，偏差越大越偏离真实值。

**方差**

- 方差度量了同样大小的训练集的变动所导致的学习性能的变化， 即刻画了数据扰动所造成的影响。方差越大，说明数据分布越分散。

**噪声**

- 噪声表达了在当前任务上任何模型所能达到的期望泛化误差的下界， 即刻画了学习问题本身的难度 。
  
  下图为**偏差和方差示意图**。

![image-20210927150625116](img/过拟合与欠拟合/image-20210927150625116.png)

**泛化误差、偏差、方差和模型复杂度的关系**（图片来源百面机器学习）

<img src="img/过拟合与欠拟合/JrBui5yzA3IGgk7.png" alt="image-20210423092851506"  />

参考资料：https://blog.csdn.net/simple_the_best/article/details/71167786

## 2. 什么是过拟合和欠拟合，为什么会出现这个现象

过拟合指的是在训练数据集上表现良好，而在未知数据上表现差。如图所示：

![img](img/过拟合与欠拟合/247f6539-1c10-75ac-84f8-02d238699dfd.jpg)

欠拟合指的是模型没有很好地学习到数据特征，不能够很好地拟合数据，在训练数据和未知数据上表现都很差。

过拟合的原因在于：

- 参数太多，模型复杂度过高；
- 建模样本选取有误，导致选取的样本数据不足以代表预定的分类规则；
- 样本噪音干扰过大，使得机器将部分噪音认为是特征从而扰乱了预设的分类规则；
- 假设的模型无法合理存在，或者说是假设成立的条件实际并不成立。

欠拟合的原因在于：

- 特征量过少；
- 模型复杂度过低。

## 3. 怎么解决欠拟合

- 增加新特征，可以考虑加入进特征组合、高次特征，来增大假设空间；
- 添加多项式特征，这个在机器学习算法里面用的很普遍，例如将线性模型通过添加二次项或者三次项使模型泛化能力更强；
- 减少正则化参数，正则化的目的是用来防止过拟合的，但是模型出现了欠拟合，则需要减少正则化参数；
- 使用非线性模型，比如核SVM 、决策树、深度学习等模型；
- 调整模型的容量(capacity)，通俗地，模型的容量是指其拟合各种函数的能力；
- 容量低的模型可能很难拟合训练集。

## 4. 怎么解决过拟合（重点）

- 获取和使用更多的数据（数据集增强）——解决过拟合的根本性方法
- 特征降维:人工选择保留特征的方法对特征进行降维
- 加入正则化，控制模型的复杂度
- Dropout
- Early stopping
- 交叉验证
- 增加噪声

## 5. 为什么参数越小代表模型越简单？

因为参数的稀疏，在一定程度上实现了特征的选择。

越复杂的模型，越是会尝试对所有的样本进行拟合，甚至包括一些异常样本点，这就容易造成在较小的区间里预测值产生较大的波动，这种较大的波动也反映了在这个区间里的导数很大，而只有较大的参数值才能产生较大的导数。因此复杂的模型，其参数值会比较大。 因此参数越少代表模型越简单。

## 6. 为什么L1比L2更容易获得稀疏解？（重点）

![img](img/过拟合与欠拟合/v2-a026e24156e13a1d14c43df26b9bd2a4_720w.jpg)

![img](img/过拟合与欠拟合/v2-f6edae58134c5a26687c3883af48d5d5_720w.jpg)

![img](img/过拟合与欠拟合/v2-3aaa69f70754c469bca5c8e4c3e161db_720w.jpg)

参考链接： https://www.zhihu.com/question/37096933/answer/475278057

## 7. Dropout为什么有助于防止过拟合？（重点）

* 取平均的作用
  
  先回到标准的模型即没有dropout，我们用相同的训练数据去训练5个不同的神经网络，一般会得到5个不同的结果，此时我们可以采用 “5个结果取均值”或者“多数取胜的投票策略”去决定最终结果。例如3个网络判断结果为数字9，那么很有可能真正的结果就是数字9，其它两个网络给出了错误结果。这种“综合起来取平均”的策略通常可以有效防止过拟合问题。因为不同的网络可能产生不同的过拟合，取平均则有可能让一些“相反的”拟合互相抵消。dropout掉不同的隐藏神经元就类似在训练不同的网络，随机删掉一半隐藏神经元导致网络结构已经不同，整个dropout过程就相当于对很多个不同的神经网络取平均。而不同的网络产生不同的过拟合，一些互为“反向”的拟合相互抵消就可以达到整体上减少过拟合。
* 减少神经元之间复杂的共适应关系
  
  因为dropout程序导致两个神经元不一定每次都在一个dropout网络中出现。这样权值的更新不再依赖于有固定关系的隐含节点的共同作用，阻止了某些特征仅仅在其它特定特征下才有效果的情况 。迫使网络去学习更加鲁棒的特征 ，这些特征在其它的神经元的随机子集中也存在。换句话说假如我们的神经网络是在做出某种预测，它不应该对一些特定的线索片段太过敏感，即使丢失特定的线索，它也应该可以从众多其它线索中学习一些共同的特征。从这个角度看dropout就有点像L1，L2正则，减少权重使得网络对丢失特定神经元连接的鲁棒性提高。
* Dropout类似于性别在生物进化中的角色
  
  物种为了生存往往会倾向于适应这种环境，环境突变则会导致物种难以做出及时反应，性别的出现可以繁衍出适应新环境的变种，有效的阻止过拟合，即避免环境改变时物种可能面临的灭绝。

参考链接：https://zhuanlan.zhihu.com/p/38200980

## 8. Dropout在训练和测试时都需要吗？

Dropout在训练时采用，是为了减少神经元对部分上层神经元的依赖，类似将多个不同网络结构的模型集成起来，减少过拟合的风险。而在测试时，应该用整个训练好的模型，因此不需要dropout。

## 9. Dropout如何平衡训练和测试时的差异呢？

Dropout 在训练时以一定的概率使神经元失活，实际上就是让对应神经元的输出为0。假设失活概率为 p ，就是这一层中的每个神经元都有p的概率失活。

例如在三层网络结构中，如果失活概率为0.5，则平均每一次训练有3个神经元失活，所以输出层每个神经元只有3个输入，而实际测试时是不会有dropout的，输出层每个神经元都有6个输入。

**因此在训练时还要对第二层的输出数据除以（1-p）之后再传给输出层神经元，作为神经元失活的补偿，以使得在训练时和测试时每一层输入有大致相同的期望。**

## 10. BN和Dropout共同使用时会出现的问题

BN和Dropout单独使用都能减少过拟合并加速训练速度，但如果一起使用的话并不会产生1+1>2的效果，相反可能会得到比单独使用更差的效果。

参考链接：https://www.zhihu.com/tardis/sogou/art/61725100

## 11. L1 和 L2 正则先验分别服从什么分布

先验就是优化的起跑线， 有先验的好处就是可以在较小的数据集中有良好的泛化性能，当然这是在先验分布是接近真实分布的情况下得到的了，从信息论的角度看，向系统加入了正确先验这个信息，肯定会提高系统的性能。

L1 正则先验分布是 Laplace 分布，L2 正则先验分布是 Gaussian 分布。

Laplace 分布公式为:

$$
f(x)=\frac{1}{2 \lambda} e^{-\frac{|x-\mu|}{\lambda}}
$$

Gaussian 分布公式为:

$$
f(x)=\frac{1}{\sqrt{2 \pi} \sigma} \exp \left(-\frac{(x-\mu)^{2}}{2 \sigma^{2}}\right)
$$

对参数引入高斯正态先验分布相当于L2正则化：

![img](img/过拟合与欠拟合/96b02ee9-c6ab-718f-b358-0a8db26f9cd7.jpg)

对参数引入拉普拉斯先验等价于 L1正则化：

![img](img/过拟合与欠拟合/f4a1eb7a-3ce3-7cea-84a0-d569423b2d01.jpg)

从上面两图可以看出， L2先验趋向零周围， L1先验趋向零本身。

参考链接：https://blog.csdn.net/akenseren/article/details/80427471

---

# 文件：AI算法\NLP\应用场景\NLG.md

---

# NLG

## 知识体系

主要包括 NLG 相关知识点。

## Questions

### 文本生成评估指标 BLUE 的缺点？

- 过于依赖参考翻译，如果译文质量很好但部分字词在参考翻译中没有的话得分会很低。
- 未考虑语法问题。

---

# 文件：AI算法\NLP\应用场景\Similarity.md

---

# 相似度

## 知识体系

主要分为两大类架构体系：表示型和交互型。

## Questions

### 表示型

#### 孪生网络原理？

孪生网络是指包含两个或多个相同子网络的架构。相同是指配置、参数和权重都一模一样。主要用于评价输入之间的相似度。损失函数主要采用 Triplet Loss 或 Contrastive Loss。

---

# 文件：AI算法\NLP\文本表示\Word2Vec详解.md

---

# 1 模型介绍

## 1.1 模型概述

Word2Vec是Google在2013年提出的一个NLP工具，它通过一个浅层的双层神经网络，高效率、高质量地将海量单词向量化。训练得到的词向量满足：

- 相似单词的词向量彼此接近。例如$\text{dis}(\vec V(\text{man}),\vec V(\text{woman})) \ll \text{dis}(\vec V(\text{man}),\vec V(\text{computer}))$
- 保留单词间的线性规则性。例如$\vec V(\text{king})-\vec V(\text{man})+\vec V(\text{woman})\approx \vec V(\text{queen})$

Word2Vec模型的灵感来源于Bengio在2003年提出的NNLM模型（Nerual Network Language Model），该模型使用一个三层前馈神经网络$f(w_k,w_{k-1},w_{k-2},...,w_{k-n+1};\theta)$来拟合一个词序列的条件概率$P(w_k|w_{k-1},w_{k-2},...,w_1)$。第一层是映射层，通过一个共享矩阵，将One-Hot向量转化为词向量，第二层是一个激活函数为tanh的隐含层，第三层是Softmax输出层，将向量映射到$[0,1]$概率空间中。根据条件概率公式与大数定律，使用词频$\frac{\text{Count}(w_k,w_{k-1},w_{k-2},...,w_{k-n+1})}{\text{Count}(w_{k-1},w_{k-2},...,w_{k-n+1})}$来近似地估计真实的条件概率。

<img src="Word2Vec详解.assets/NNLM.png" alt="NNLM" style="zoom: 50%;" />

Bengio发现，我们可以使用映射层的权值作为词向量表征。但是，由于参数空间非常庞大，NNLM模型的训练速度非常慢，在百万级的数据集上需要耗时数周才能得到相对不错的结果，而在千万级甚至更大的数据集上，几乎无法得到结果。

Mikolov发现，NNLM模型可以被拆分成两个步骤：

- 用一个简单的模型训练出一个连续的词向量（映射层）
- 基于词向量表征，训练出一个N-Gram神经网络模型（隐含层+输出层）

而模型的计算瓶颈主要在第二步，特别是输出层的Sigmoid归一化部分。如果我们只是想得到词向量，可以对第二步的神经网络模型进行简化，从而提高模型的训练效率。因此，Mikolov对NNLM模型进行了以下几个部分的修改：

- 舍弃了隐含层。
- NNLM在利用上文词预测目标词时，对上文词的词向量进行了拼接，Word2Vec模型对其直接进行了求和，从而降低了隐含元的维度。
- NNLM在进行Sigmoid归一化时需要遍历整个词汇表，Word2Vec模型提出了Hierarchical Softmax与Negative Sampling两种策略进行优化。
- 依据分布式假设（上下文环境相似的两个词有着相近的语义），将下文单词也纳入训练环境，并提出了两种训练策略，一种是用上下文预测中心词，称为CBOW，另一种是用中心词预测上下文，称为Skip-Gram。

<img src="Word2Vec详解.assets/Word2Vec.png" alt="Word2Vec" style="zoom:40%;" />

## 1.2 CBOW模型

假设我们的语料是**"NLP is so interesting and challenging"**。循环使用每个词作为中心词，来其上下文词来预测中心词。我们通常使用一个指定长度的窗口，根据马尔可夫性质，忽略窗口以外的单词。

|   中心词   |            上下文            |
| :---------: | :--------------------------: |
|     NLP     |            is, so            |
|     is     |     NLP, so, interesting     |
|     so     |  NLP, is, interesting, and  |
| interesting |   is, so, and, challenging   |
|     and     | so, interesting, challenging |
| challenging |       interesting, and       |

我们的目标是通过上下文来预测中心词，也就是给定上下文词，出现该中心词的概率最大。这和完形填空颇有点异曲同工之妙。也即$\max P(\text{NLP|is, so})*P(\text{is|NLP, so, interesting})*\dots$

用公式表示如下：

$$
\begin{align}
\max\limits_{\theta} L(\theta)&=\prod\limits_{w\in D}p(w|C(w)) \\
&=\sum\limits_{w \in D}\log p(w|C(w))
\end{align}
$$

其中$w$指中心词，$C(w)$指上下文词集，$D$指语料库，也即所有中心词的词集。

问题的核心变成了如何构造$\log p(w|C(w))$。我们知道，NNLM模型的瓶颈在Sigmoid归一化上，Mikolov提出了两种改进思路来绕过Sigmoid归一化这一操作。一种思想是将输出改为一个霍夫曼树，每一个单词的概率用其路径上的权重乘积来表示，从而减少高频词的搜索时间；另一种思想是将预测每一个单词的概率，概率最高的单词是中心词改为预测该单词是不是正样本，通过负采样减少负样本数量，从而减少训练时间。

### 1.2.1 Hierarchical Softmax

### 1.2.2 Negative Sampling

基于Hierachical Softmax的模型使用Huffman树代替了传统的线性神经网络，可以提高模型训练的效率。但是，如果训练样本的中心词是一个很生僻的词，那么在Huffman树中仍旧需要进行很复杂的搜索。负采样方法的核心思想是：设计一个分类器， 对于我们需要预测的样本，设为正样本；而对于不是我们需要的样本，设置成负样本。在CBOW模型中，我们需要预测中心词$w$，因此正样本只有$w$，也即$\text{Pos}(w)=\{w\}$，而负样本为除了$w$之外的所有词。对负样本进行**随机采样**，得到$\text{Neg}(w)$，大大简化了模型的计算。

我们首先将$C(w)$输入映射层并求和得到隐含表征$h_w=\sum\limits_{u \in C(w)}\vec v(u)$

从而，

$$
\begin{align}
p(u|C(w))&=
\begin{cases}
\sigma(h_w^T\theta_u), &\mathcal{D}(w,u)=1 \\
1-\sigma(h_w^T\theta_u), &\mathcal{D}(w,u)=0 \\
\end{cases}\\
&=[\sigma(h_w^T\theta_u)]^{\mathcal{D}(w,u)} \cdot [1-\sigma(h_w^T\theta_u)]^{1-\mathcal{D}(w,u)}
\end{align}
$$

从而，

$$
\begin{align}
\max\limits_{\theta} L(\theta)&=\sum\limits_{w \in D}\log p(w|C(w))\\
&=\sum\limits_{w \in D}\log \prod\limits_{u \in D}p(u|C(w)) \\
&\approx\sum\limits_{w \in D}\log \prod\limits_{u \in \text{Pos(w)}\cup \text{Neg(w)} }p(u|C(w))\\
&=\sum\limits_{w \in D}\log\prod\limits_{u \in \text{Pos(w)}\cup \text{Neg(w)}}[\sigma(h_w^T\theta_u)]^{\mathcal{D}(w,u)} \cdot [1-\sigma(h_w^T\theta_u)]^{1-\mathcal{D}(w,u)} \\
&=\sum\limits_{w \in D}\sum\limits_{u \in \text{Pos}(w)\cup \text{Neg}(w)}\mathcal{D}(w,u)\cdot\log \sigma(h_w^T\theta_u)+[1-\mathcal{D}(w,u)]\cdot \log [1-\sigma(h_w^T\theta_u)]\\
&=\sum\limits_{w \in D}\left\{\sum\limits_{u \in \text{Pos}(w)}\log \sigma(h_w^T\theta_u)+\sum\limits_{u \in \text{Neg}(w)}\log [1-\sigma(h_w^T\theta_u)]\right\}
\end{align}
$$

由于上式是一个最大化问题，因此使用随机梯度上升法对问题进行求解。

令$L(w,u,\theta)=\mathcal{D}(w,u)\cdot\log \sigma(h_w^T\theta_u)+[1-\mathcal{D}(w,u)]\cdot \log [1-\sigma(h_w^T\theta_u)]$

则$\frac{\partial L}{\partial\theta_u}=\mathcal{D}(w,u)\cdot[1-\sigma(h_w^T\theta_u)]h_w+[1-\mathcal{D}(w,u)]\cdot \sigma(h_w^T\theta_u)h_w=[\mathcal{D}(w,u)-\sigma(h_w^T\theta_u)]h_w$

因此$\theta_u$的更新公式为：$\theta_u:=\theta_u+\eta[\mathcal{D}(w,u)-\sigma(h_w^T\theta_u)]h_w$

同样地，$\frac{\partial L}{\partial h_w}=[\mathcal{D}(w,u)-\sigma(h_w^T\theta_u)]\theta_u$

上下文词的更新公式为：$v(\tilde{w}):=v(\tilde{w})+\eta\sum\limits_{u \in \text{Pos}(w)\cup \text{Neg}(w)}[\mathcal{D}(w,u)-\sigma(h_w^T\theta_u)]\theta_u$

## 1.3 Skip-Gram模型

仍旧使用上文的语料库**"NLP is so interesting and challenging"**，这次，我们的目标是通过中心词来预测上下文，也就是给定中心词，出现这些上下文词的概率最大。也即$\max P(is|NLP)*P(so|NLP)*P(NLP|is)*P(so|is)*P(interesting|is)*\dots$

用公式表示如下：

$$
\begin{align}
\max\limits_{\theta} L(\theta)&=\prod\limits_{w\in D}\prod\limits_{c \in C(w)}p(c|w) \\
&=\sum\limits_{w \in D}\sum\limits_{c \in C(w)}\log p(c|w)
\end{align}
$$

### 1.3.1 Hierarchical Softmax

### 1.3.2 Negative Sampling

# 2 常见面试问题

**Q1：介绍一下Word2Vec模型。**

> A：两个模型：CBOW/Skip-Gram
> 
> 两种加速方案：Hierarchical Softmax/Negative Sampling

**Q2：Word2Vec模型为什么要定义两套词向量？**

> A：因为每个单词承担了两个角色：中心词和上下文词。通过定义两套词向量，可以将两种角色分开。cs224n中提到是为了更方便地求梯度。参考见：https://www.zhihu.com/answer/706466139

**Q3：Hierarchial Softmax 和 Negative Sampling对比**

> A：基于Huffman树的Hierarchial Softmax 虽然在一定程度上能够提升模型运算效率，但是，如果中心词是生僻词，那么在Huffman树中仍旧需要进行很复杂的搜索$(O(\log N))$。而Negative Sampling通过随机负采样来提升运算效率，其复杂度和设定的负样本数$K$线性相关$(O(K))$，当$K$取较小的常数时，负采样在每⼀步的梯度计算开销都较小。

**Q4：HS为什么用霍夫曼树而不用其他二叉树？**

> 这是因为Huffman树对于高频词会赋予更短的编码，使得高频词离根节点距离更近，从而使得训练速度加快。

**Q5：Word2Vec模型为什么要进行负采样？**

> A：因为负样本的数量很庞大，是$O(|V^2|)$。

**Q6：负采样为什么要用词频来做采样概率？**

> 为这样可以让频率高的词先学习，然后带动其他词的学习。

**Q7：One-hot模型与Word2Vec模型比较？**

> A：One-hot模型的缺点
> 
> - 稀疏 Sparsity
> - 只能表示维度数量的单词 Capacity
> - 无法表示单词的语义 Meaning

**Q8：Word2Vec模型在NNLM模型上做了哪些改进？**

> A：相同点：其本质都可以看作是语言模型；
> 
> 不同点：词向量只不过 NNLM 一个产物，Word2vec 虽然其本质也是语言模型，但是其专注于词向量本身，因此做了许多优化来提高计算效率：
> 
> - 与 NNLM 相比，词向量直接 sum，不再拼接，并舍弃隐层；
> - 考虑到 sofmax 归一化需要遍历整个词汇表，采用 hierarchical softmax 和 negative sampling 进行优化，hierarchical softmax 实质上生成一颗带权路径最小的哈夫曼树，让高频词搜索路劲变小；negative sampling 更为直接，实质上对每一个样本中每一个词都进行负例采样；

**Q9：Word2Vec与LSA对比？**

> A：LSA是基于共现矩阵构建词向量，本质上是基于全局语料进行SVD矩阵分解，计算效率低；
> 
> 而Word2Vec是基于上下文局部语料计算共现概率，计算效率高。

**Q10：Word2Vec的缺点？**

> 忽略了词语的语序；
> 
> 没有考虑一词多义现象

**Q11：怎么从语言模型理解词向量？怎么理解分布式假设？**

> 词向量是语言模型的一个副产物，可以理解为，在语言模型训练的过程中，势必在一定程度上理解了每个单词的含义。而这在计算机的表示下就是词向量。
> 
> 分布式假设指的是相同上下文语境的词有似含义。

**参考资料**

word2vec 中的数学原理详解 https://blog.csdn.net/itplus/article/details/37969519

Word2Vec原理介绍 https://www.cnblogs.com/pinard/p/7160330.html

词向量介绍 https://www.cnblogs.com/sandwichnlp/p/11596848.html

一些关于词向量的问题 https://zhuanlan.zhihu.com/p/56382372

一个在线尝试Word2Vec的小demo https://ronxin.github.io/wevi/

---

# 文件：AI算法\NLP\文本表示\文本结构理解.md

---

# 基于词角度

## 知识体系

基于词角度的文本结构理解主要包括分词、词性标注和命名实体识别。对于文本未切分的语言，分词一般会作为自然语言处理的第一步。即使到了字粒度的 BERT 时代， WWM 效果也要更好一些。从大的方面看有两种不同的分词方式：基于词典和基于序列标注。后者也可用于词性标注和命名实体识别任务。

## Questions

### 常用的分词方法有哪些？

常用分词方法有两种：基于词典的方法和基于序列标注的方法。前者又包括字符串匹配方法和统计语言模型方法；后者包括统计方法和深度学习方法。

### 字符串匹配分词的优缺点是什么？

优点：方法简单可控、速度快；缺点：难以解决歧义及新词问题。

### 结巴分词原理？

基于词典构造有向无环图，计算最大概率路径。新词发现使用 HMM，弥补了 Ngram 难以发现新词的不足。

### HMM 怎么做分词的？

HMM 使用序列标注法进行分词，以 BEMS 标签为例，此为隐状态取值空间。模型需要估计隐状态初始概率、隐状态之间的转移概率和隐状态到观测序列的发射概率。可以使用有监督或无监督学习算法，有监督学习根据标注数据利用极大似然法进行估计，无监督学习使用 Baum-Welch 算法。实际使用时使用维特比算法进行解码，得到最可能的隐状态序列。

### MEMM 是什么？

HMM 有两个基本假设：齐次一阶马尔科夫和观测独立假设。也就是 t 时刻的状态仅仅与前一个状态有关，同时观测序列仅仅取决于它对应的隐状态。这就和实际不符，因为隐状态往往和上下文信息都有关系。于是在 HMM 的基础上引入了 MEMM，即最大熵马尔科夫模型。它打破了 HMM 的观测独立假设，考虑了整个观测序列。HMM 是一种对隐状态序列和观测状态序列联合概率进行建模的生成式模型；MEMM 是直接对标注后的后验概率进行建模的判别式模型。

### 什么是标注偏置问题？如何解决？

在 MEMM 中需要对局部进行归一化，因此隐状态会倾向于转移到那些后续状态可能更少的状态上（以提高整体的后验概率），这就是标注偏置问题。CRF，条件随机场在 MEMM 的基础上进行了全局归一化，解决了标注偏置问题。这其实已经打破了 HMM 的第一个假设（齐次马尔科夫），将有向变成了无向。

### BILSTM-CRF 原理

BiLSTM 是双向 RNN 模型，每一个 Token 对应一个 Label，可以直接用来做序列标注任务。但是 BiLSTM 在 NER 问题上有个问题，因为 NER 的标签之间往往也有关系，比如形容词后面一半会接名词（中文为例），动词后面会接副词，LSTM 没办法获取这部分特征。这时候我们就需要 CRF 层，简单来说，就是加入 Label 之间的关系特征。也就是说，每一个 Label 在预测时都会考虑全局其他的 Label。

### 如何解决序列标注标签不均衡问题？

在 NER 任务中，标签不均衡一般是指要标注的实体较少，大多数标签为 O 的情况，以及部分实体过多，其他实体过少的情况。一般可以有以下几种处理思路：

- 数据增强，主要是词替换（包括同类实体词替换、同义词替换、代词替换等）、随机增删实体词以外的词构建新样本、继续增加新样本、半监督方法等
- 损失函数，给 loss 增加权重惩罚、Dice Loss 等
- 迁移学习，借助预训练模型已经学到的丰富知识

---

# 文件：AI算法\NLP\文本表示\文本表征方式.md

---

# 静态语义表示方法

## 知识体系

主要包括词袋模型 BoW、TFIDF、LDA、Word2vec、Golve、Doc2Vec 等。

## Questions

### 在小数据集中 Skip-Gram 和 CBoW 哪种表现更好？

Skip-Gram 是用一个 Center Word 预测其 Context 里的 Word；而 CBoW 是用 Context 里的所有 Word 去预测一个 Center Word。显然，前者对训练数据的利用更高效（构造的数据集多），因此，对于较小的语料库，Skip-Gram是更好的选择。

### 为什么要使用HS（Hierarchical Softmax ）和负采样（Negative Sampling）？

两个模型的原始做法都是做内积，经过 Softmax 后得到概率，因此复杂度很高。假设我们拥有一个百万量级的词典，每一步训练都需要计算上百万次词向量的内积，显然这是无法容忍的。因此人们提出了两种较为实用的训练技巧，即 HS 和 Negative Sampling。

### 介绍一下HS（Hierarchical Softmax ）

HS 是试图用词频建立一棵哈夫曼树，那么经常出现的词路径会比较短。树的叶子节点表示词，共词典大小多个，而非叶子结点是模型的参数，比词典个数少一个。要预测的词，转化成预测从根节点到该词所在叶子节点的路径，是多个二分类问题。本质是把 N 分类问题变成 log(N) 次二分类

### 介绍一下负采样（Negative Sampling）

把原来的 Softmax 多分类问题，直接转化成一个正例和多个负例的二分类问题。让正例预测 1，负例预测 0，这样子更新局部的参数。.

### 负采样为什么要用词频来做采样概率？

可以让频率高的词先学习，然后带动其他词的学习。

### 负采样有什么作用？

- 可以大大降低计算量，加快模型训练时间
- 保证模型训练效果，因为目标词只跟相近的词有关，没有必要使用全部的单词作为负例来更新它们的权重

### 对比 Skip-Gram 和 CBOW

CBOW 会比 Skip-Gram 训练速度更快，因为前者每次会更新 Context(w) 的词向量，而 Skip-Gram 只更新核心词的词向量。
Skip-Gram 对低频词效果比 CBOW 好，因为 SkipGram 是尝试用当前词去预测上下文，当前词是低频词还是高频词没有区别。但是 CBOW 相当于是完形填空，会选择最常见或者说概率最大的词来补全，因此不太会选择低频词。

### 对比字向量和词向量

字向量可以解决未登录词的问题，以及可以避免分词；词向量包含的语义空间更大，更加丰富，如果语料足够的情况下，词向量是能够学到更多的语义信息。

### 如何衡量 Word2vec 得出的词/字向量的质量？

在实际工程中一般以 Word Embedding 对于实际任务的收益为评价标准，包括词汇类比任务（如 king – queen = man - woman）以及 NLP 中常见的应用任务，比如命名实体识别（NER），关系抽取（RE）等。

### 神经网络框架里的 Embedding 层和 Word Embedding 有什么关系？

Embedding 层就是以 One-Hot 为输入（实际一般输入字或词的 id）、中间层节点为字向量维数的全连接层。而这个全连接层的参数，就是一个 “词向量表”，即 Word Embedding。

### Word2vec 的缺点？

没有考虑词序，因为它假设了词的上下文无关(把概率变为连乘)；没有考虑全局的统计信息。

### LDA 的原理？

LDA 是 pLSA 的贝叶斯版本，pLSA 是使用生成模型建模文章的生成过程，它假定 K 个主题 Z，对于文档集 D 中每个文档 Di 都包含 Ni 个词 W，对每个 Wi，最大化给定文档 Di 生成主题 Zi，再根据 Di 和 Zi 生成 Wi 的概率，最终生成整个文档序列。

LDA 将每篇文章的主题分布和每个主题对应的词分布看成是一种先验分布，即狄利克雷分布。之所以选择该分布，是因为它是多项式分布的共轭先验概率分布，后验分布依然服从狄利克雷分布，方便计算。

具体过程为：首先从超参数为 α 的狄利克雷分布中抽样生成给定文档的主题分布 θ，对于文档中的每一个词，从多项式分布 θ 中抽样生成对应的主题 z，从超参数为 β 的狄利克雷分布中抽样生成给定主题 z 的词分布 φ，从多项式分布 φ 中抽样生成词 w。

LDA 的主题数为超参数，一般使用验证集评估 ppl 或 HDP-LDA。

### Word2vec 和 TF-IDF 在计算相似度时的区别？

- 前者是稠密向量，后者是稀疏向量
- 前者维度低很多，计算更快
- 前者可以表达语义信息，后者不行
- 前者可以通过计算余弦相似度计算两个向量的相似度，后者不行

### 为什么训练得到的字词向量会有如下一些性质，比如向量的夹角余弦、向量的欧氏距离都能在一定程度上反应字词之间的相似性？

因为我们在用语言模型无监督训练时，是开了窗口的，通过前 n 个字预测下一个字的概率，这个 n 就是窗口的大小，同一个窗口内的词语，会有相似的更新，这些更新会累积，而具有相似模式的词语就会把这些相似更新累积到可观的程度。

### Word2vec 与 Glove的异同？

在 Word2vec 中，高频的词共现只是产生了更多的训练数据，并没有携带额外的信息；Glove 加入词的全局共现频率信息。它基于词上下文矩阵的矩阵分解技术，首先构建一个大的单词×上下文共现矩阵，然后学习低维表示，可以视为共现矩阵的重构问题。

- Word2vec 是局部语料训练，特征提取基于滑动窗口；Glove 的滑动窗口是为了构建共现矩阵，统计全部语料在固定窗口内词的共现频次。
- Word2vec 损失函数是带权重的交叉熵；Glove 的损失函数是最小平方损失
- Glove 利用了全局信息，训练时收敛更快

### Word2vec 相比之前的 Word Embedding 方法好在什么地方？

考虑了上下文。

### Doc2vec 原理？

Doc2vec 是训练文档表征的，在输入层增加了一个 Doc 向量。有两种不同的训练方法：Distributed Memory  是给定上下文和段落向量的情况下预测单词的概率。在一个句子或者段落文档训练过程中，段落 ID 保存不变，共享同一个段落向量。Distributed Bag of Words 则在只给定段落向量的情况下预测段落中一组随机单词的概率。使用时固定词向量，随机初始化 Doc 向量，训练几个步骤后得到最终 Doc 向量。

### FastText 相比 Word2vec 有哪些不同？

- FastText 增加了 Ngram 特征，可以更好地解决未登录词及在小数据集上训练的问题
- FastText 是一个工具包，除了可以训练词向量还可以训练有监督的文本分类模型

## 参考链接

1. [https://blog.csdn.net/zhangxb35/article/details/74716245](https://blog.csdn.net/zhangxb35/article/details/74716245)
2. [https://spaces.ac.cn/archives/4122](https://spaces.ac.cn/archives/4122)

---

# 文件：AI算法\NLP\特征挖掘\基于深度学习的模型.md

---

# 基于深度学习的模型

## 知识体系

主要包括深度学习相关的特征抽取模型，包括卷积网络、循环网络、注意力机制、预训练模型等。

### CNN

TextCNN 是 CNN 的 NLP 版本，来自 Kim 的 [[1408.5882] Convolutional Neural Networks for Sentence Classification](https://arxiv.org/abs/1408.5882)

结构如下：

![](http://qnimg.lovevivian.cn/paper-textcnn-1.jpg)

大致原理是使用多个不同大小的 filter（也叫 kernel） 对文本进行特征提取，如上图所示：

- 首先通过 Embedding 将输入的句子映射为一个 `n_seq * embed_size` 大小的张量（实际中一般还会有 batch_size）
- 使用 `(filter_size, embed_size)` 大小的 filter 在输入句子序列上平滑移动，这里使用不同的 padding 策略，会得到不同 size 的输出
- 由于有 `num_filters` 个输出通道，所以上面的输出会有 `num_filters` 个
- 使用 `Max Pooling` 或 `Average Pooling`，沿着序列方向得到结果，最终每个 filter 的输出 size 为 `num_filters`
- 将不同 filter 的输出拼接后展开，作为句子的表征

### RNN

RNN 的历史比 CNN 要悠久的多，常见的类型包括：

- 一对一（单个 Cell）：给定单个 Token 输出单个结果
- 一对多：给定单个字符，在时间步向前时同时输出结果序列
- 多对一：给定文本序列，在时间步向前执行完后输出单个结果
- 多对多1：给定文本序列，在时间步向前时同时输出结果序列
- 多对多2：给定文本序列，在时间步向前执行完后才开始输出结果序列

由于 RNN 在长文本上有梯度消失和梯度爆炸的问题，它的两个变种在实际中使用的更多。当然，它们本身也是有一些变种的，这里我们只介绍主要的模型。

- LSTM：全称 Long Short-Term Memory，一篇 Sepp Hochreiter 等早在 1997 年的论文[《LONG SHORT-TERM MEMORY》](https://www.bioinf.jku.at/publications/older/2604.pdf)中被提出。主要通过对原始的 RNN 添加三个门（遗忘门、更新门、输出门）和一个记忆层使其在长文本上表现更佳。
  
  ![](https://upload.wikimedia.org/wikipedia/commons/thumb/9/93/LSTM_Cell.svg/1280px-LSTM_Cell.svg.png)
- GRU：全称 Gated Recurrent Units，由 Kyunghyun Cho 等人 2014 年在论文[《Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation》](https://arxiv.org/pdf/1406.1078v3.pdf) 中首次被提出。主要将 LSTM 的三个门调整为两个门（更新门和重置门），同时将记忆状态和输出状态合二为一，在效果没有明显下降的同时，极大地提升了计算效率。
  
  ![](https://upload.wikimedia.org/wikipedia/commons/thumb/3/37/Gated_Recurrent_Unit%2C_base_type.svg/1920px-Gated_Recurrent_Unit%2C_base_type.svg.png)

## Questions

### CNN相关

#### CNN 有什么好处？

- 稀疏（局部）连接：卷积核尺寸远小于输入特征尺寸，输出层的每个节点都只与部分输入层连接
- 参数共享：卷积核的滑动窗在不同位置的权值是一样的
- 等价表示（输入/输出数据的结构化）：输入和输出在结构上保持对应关系（长文本处理容易）

#### CNN 有什么不足？

- 只有局部语义，无法从整体获取句子语义
- 没有位置信息，丢失了前后顺序信息

#### 卷积层输出 size？

给定 n×n 输入，f×f 卷积核，padding p，stride s，输出的尺寸为：

$$
\lfloor \frac{n+2p-f}{s} + 1 \rfloor \times \lfloor \frac{n+2p-f}{s} + 1 \rfloor
$$

### RNN

#### LSTM 网络结构？

LSTM 即长短时记忆网络，包括三个门：更新门（输入门）、遗忘门和输出门。公式如下：

$$
\hat{c}^{<t>} = \tanh (W_c [a^{<t-1}>, x^{<t>}] + b_c) \\
\Gamma_u = \sigma(W_u [a^{<t-1}>, x^{<t>}] + b_u) \\
\Gamma_f = \sigma(W_f [a^{<t-1}>, x^{<t>}] + b_f) \\
\Gamma_o = \sigma(W_o [a^{<t-1}>, x^{<t>}] + b_o) \\
c^{<t>} = \Gamma_u * \hat{c}^{<t>} + \Gamma_f*c^{<t-1>} \\
a^{<t>} = \Gamma_o * c^{<t>}
$$

#### 如何解决 RNN 中的梯度消失或梯度爆炸问题？

- 梯度截断
- ReLU、LeakReLU、Elu 等激活函数
- Batch Normalization
- 残差连接
- LSTM、GRU 等架构

#### 假设输入维度为 m，输出为 n，求 GRU 参数？

输入  W：3nm，隐层 W：3nn，隐层 b：3n，合计共：`3*(nn+nm+n)`。当然，也有的实现会把前一时刻的隐层和当前时刻的输入分开，使用两个 bias，此时需要再增加 3n 个参数。

#### LSTM 和 GRU 的区别？

- GRU 将 LSTM 的更新门、遗忘门和输出门替换为更新门和重置门
- GRU 将记忆状态和输出状态合并为一个状态
- GRU 参数更少，更容易收敛，但数据量大时，LSTM 效果更好

### Attention

#### Attention 机制

Attention 核心是从输入中有选择地聚焦到特定重要信息上的一种机制。有三种不同用法：

- 在 encoder-decoder attention 层，query 来自上一个 decoder layer，memory keys 和 values 来自 encoder 的 output
- encoder 包含 self-attention，key value 和 query 来自相同的位置，即前一层的输出。encoder 的每个位置都可以注意到前一层的所有位置
- decoder 与 encoder 类似，通过将所有不合法连接 mask 以防止信息溢出

#### 自注意力中为何要缩放？

维度较大时，向量内积容易使得 SoftMax 将概率全部分配给最大值对应的 Label，其他 Label 的概率几乎为 0，反向传播时这些梯度会变得很小甚至为 0，导致无法更新参数。因此，一般会对其进行缩放，缩放值一般使用维度 dk 开根号，是因为点积的方差是 dk，缩放后点积的方差为常数 1，这样就可以避免梯度消失问题。

另外，Hinton 等人的研究发现，在知识蒸馏过程中，学生网络以一种略微不同的方式从教师模型中抽取知识，它使用大模型在现有标记数据上生成软标签，而不是硬的二分类。直觉是软标签捕获了不同类之间的关系，这是大模型所没有的。这里的软标签就是缩放的 SoftMax。

至于为啥最后一层为啥一般不需要缩放，因为最后输出的一般是分类结果，参数更新不需要继续传播，自然也就不会有梯度消失的问题。

### Transformer

#### Transformer 中为什么用 Add 而不是 Concat？

在 Embedding 中，Add 等价于 Concat，三个 Embedding 相加与分别 One-Hot Concat 效果相同。

### ELMO

#### 简单介绍下ELMO

使用双向语言模型建模，两层 LSTM 分别学习语法和语义特征。首次使用两阶段训练方法，训练后可以在下游任务微调。

Feature-Based 微调，预训练模型作为纯粹的表征抽取器，表征依赖微调任务网络结构适配（任务缩放因子 γ）。

### ELMO的缺点

ELMO 的缺点主要包括：不完全的双向预训练（Bi 是分开的，仅在 Loss 合并）；需要进行任务相关的网络设计（每种下游任务都要特定的设计）；仅有词向量无句向量（没有句向量任务）。

### GPT

#### 简单介绍下GPT

使用 Transformer 的 Decoder 替换 LSTM 作为特征提取器。

Model-Based 微调，预训练模型作为任务网络的一部分参与任务学习，简化了下游任务架构设计。

#### GPT的缺点

GPT 的缺点包括：单项预训练模型；仅有词向量无句向量（仅学习语言模型）。

### BERT

#### 简单介绍下BERT

使用 Transformer Encoder 作为特征提取器，交互式双向语言建模（MLM），Token 级别+句子级别任务（MLM+NSP），两阶段预训练。

Feature-Based 和 Model-Based，实际一般使用 Model-Based。

#### BERT缺点

BERT 的缺点是：字粒度难以学到词、短语、实体的完整语义。

### ERNIE

#### ERNIE对BERT进行了哪些优化？

对 BERT 的缺点进行了优化，Mask 从字粒度的 Token 修改为完整的词或实体。ERNIE2.0 引入更多的预训练任务以捕捉更丰富的语义知识。

---

# 文件：AI算法\NLP\特征挖掘\BERT\BERT面试题.md

---

## **BERT 的基本原理是什么？**

BERT 来自 Google 的论文Pre-training of Deep Bidirectional Transformers for Language Understanding，BERT 是“Bidirectional Encoder Representations from Transformers”的首字母缩写，整体是一个自编码语言模型（Autoencoder LM），并且其设计了两个任务来预训练该模型。

- 第一个任务是采用 MaskLM 的方式来训练语言模型，通俗地说就是在输入一句话的时候，随机地选一些要预测的词，然后用一个特殊的符号[MASK]来代替它们，之后让模型根据上下文去学习这些地方该填的词。
- 第二个任务在双向语言模型的基础上额外增加了一个句子级别的连续性预测任务，即预测输入 BERT 的两段文本是否为连续的文本，引入这个任务可以更好地让模型学到连续的文本片段之间的关系。

最后的实验表明 BERT 模型的有效性，并在 11 项 NLP 任务中夺得 SOTA 结果。

BERT 相较于原来的 RNN、LSTM 可以做到并发执行，同时提取词在句子中的关系特征，并且能在多个不同层次提取关系特征，进而更全面反映句子语义。相较于 word2vec，其又能根据句子上下文获取词义，从而避免歧义出现。同时缺点也是显而易见的，模型参数太多，而且模型太大，少量数据训练时，容易过拟合。

## **BERT 的输入和输出分别是什么？**

BERT 模型的主要输入是文本中各个字/词(或者称为 token)的原始词向量，该向量既可以随机初始化，也可以利用word2vec等算法进行预训练以作为初始值；输出是文本中各个字/词融合了全文语义信息后的向量表示。

模型输入除了字向量(英文中对应的是 Token Embeddings)，还包含另外两个部分：

1. 文本向量(英文中对应的是 Segment Embeddings)：该向量的取值在模型训练过程中自动学习，用于刻画文本的全局语义信息，并与单字/词的语义信息相融合
2. 位置向量(英文中对应的是 Position Embeddings)：由于出现在文本不同位置的字/词所携带的语义信息存在差异（比如：“我爱你”和“你爱我”），因此，BERT 模型对不同位置的字/词分别附加一个不同的向量以作区分

最后，BERT 模型将字向量、文本向量和位置向量的加和作为模型输入。特别地，在目前的 BERT 模型中，文章作者还将英文词汇作进一步切割，划分为更细粒度的语义单位（WordPiece），例如：将 playing 分割为 play 和##ing；此外，对于中文，目前作者未对输入文本进行分词，而是直接将单字作为构成文本的基本单位。

需要注意的是，上面只是简单介绍了单个句子输入 BERT 模型中的表示，实际上，在做 Next Sentence Prediction 任务时，在第一个句子的首部会加上一个[CLS] token，在两个句子中间以及最后一个句子的尾部会加上一个[SEP] token。

## **BERT是怎么用Transformer的？**

BERT只使用了Transformer的Encoder模块，原论文中，作者分别用12层和24层Transformer Encoder组装了两套BERT模型，分别是：

![image-20211101145141135](img/image-20211101145141135-16497775072021.png)

其中层的数量(即，Transformer Encoder块的数量)为 L，隐藏层的维度为H，自注意头的个数为A。在所有例子中，我们将前馈/过滤器(Transformer Encoder端的feed-forward层)的维度设置为4H，即当H=768 时是 3072；当H=1024  是4096  。

**需要注意的是，与Transformer本身的Encoder端相比，BERT的Transformer Encoder端输入的向量表示，多了Segment Embeddings。**

## BERT 的三个 Embedding 直接相加会对语义有影响吗？

Embedding 的数学本质，就是以 one hot 为输入的单层全连接。

也就是说，世界上本没什么 Embedding，有的只是 one hot。

现在我们将 token, position, segment 三者都用 one hot 表示，然后 concat 起来，然后才去过一个单层全连接，等价的效果就是三个 Embedding 相加。

因此，BERT 的三个 Embedding 相加，其实可以理解为 token, position, segment 三个用 one hot 表示的特征的 concat，而特征的 concat 在深度学习领域是很常规的操作了。

用一个例子理解一下：

假设 token Embedding 矩阵维度是 [4,768]；position Embedding 矩阵维度是 [3,768]；segment Embedding 矩阵维度是 [2,768]。

对于一个字，假设它的 token one-hot 是 [1,0,0,0]；它的 position one-hot 是 [1,0,0]；它的 segment one-hot 是 [1,0]。

那这个字最后的 word Embedding，就是上面三种 Embedding 的加和。

如此得到的 word Embedding，和 concat 后的特征：[1,0,0,0,1,0,0,1,0]，再经过维度为 [4+3+2,768] = [9, 768] 的 Embedding 矩阵，得到的 word Embedding 其实就是一样的。

Embedding 就是以 one hot 为输入的单层全连接。

## **BERT 的MASK方式的优缺点？**

答：BERT的mask方式：在选择mask的15%的词当中，80%情况下使用mask掉这个词，10%情况下采用一个任意词替换，剩余10%情况下保持原词汇不变。

**优点：**1）被随机选择15%的词当中以10%的概率用任意词替换去预测正确的词，相当于文本纠错任务，为BERT模型赋予了一定的文本纠错能力；2）被随机选择15%的词当中以10%的概率保持不变，缓解了finetune时候与预训练时候输入不匹配的问题（预训练时候输入句子当中有mask，而finetune时候输入是完整无缺的句子，即为输入不匹配问题）。

**缺点：**针对有两个及两个以上连续字组成的词，随机mask字割裂了连续字之间的相关性，使模型不太容易学习到词的语义信息。主要针对这一短板，因此google此后发表了BERT-WWM，国内的哈工大联合讯飞发表了中文版的BERT-WWM。

## BERT中的NSP任务是否有必要？

答：在此后的研究（论文《Crosslingual language model pretraining》等）中发现，NSP任务可能并不是必要的，消除NSP损失在下游任务的性能上能够与原始BERT持平或略有提高。这可能是由于Bert以单句子为单位输入，模型无法学习到词之间的远程依赖关系。针对这一点，后续的RoBERTa、ALBERT、spanBERT都移去了NSP任务。

## BERT深度双向的特点，双向体现在哪儿？

BERT的预训练模型中，预训练任务是一个mask LM ，通过随机的把句子中的单词替换成mask标签， 然后对单词进行预测。

这里注意到，对于模型，输入的是一个被挖了空的句子， 而由于Transformer的特性， 它是会注意到所有的单词的，这就导致模型会根据挖空的上下文来进行预测， 这就实现了双向表示， 说明BERT是一个双向的语言模型。

BERT使用Transformer-encoder来编码输入，encoder中的Self-attention机制在编码一个token的时候同时利用了其上下文的token，其中‘同时利用上下文’即为双向的体现，而并非像Bi-LSTM那样把句子倒序输入一遍。

延伸：LSTM可以做这样的双向mask预测任务吗？（可以但很慢，比如ELMo，知乎有相关提问 https://www.zhihu.com/question/482319300）

## BERT深度双向的特点，深度体现在哪儿？

答：针对特征提取器，Transformer只用了self-attention，没有使用RNN、CNN，并且使用了残差连接有效防止了梯度消失的问题，使之可以构建更深层的网络，所以BERT构建了多层深度Transformer来提高模型性能。

## BERT中并行计算体现在哪儿？

答：不同于RNN计算当前词的特征要依赖于前文计算，有时序这个概念，是按照时序计算的，而BERT的Transformer-encoder中的self-attention计算当前词的特征时候，没有时序这个概念，是同时利用上下文信息来计算的，一句话的token特征是通过矩阵并行‘瞬间’完成运算的，故，并行就体现在self-attention。

## BERT 的 embedding 向量如何得来的？

以中文为例，「BERT 模型通过查询字向量表将文本中的每个字转换为一维向量，作为模型输入(还有 position embedding 和 segment embedding)；模型输出则是输入各字对应的融合全文语义信息后的向量表示。」

而对于输入的 token embedding、segment embedding、position embedding 都是随机生成的，需要注意的是在 Transformer 论文中的 position embedding 由 sin/cos 函数生成的固定的值，而在这里代码实现中是跟普通 word embedding 一样随机生成的，可以训练的。作者这里这样选择的原因可能是 BERT 训练的数据比 Transformer 那篇大很多，完全可以让模型自己去学习。

## BERT中Transformer中的Q、K、V存在的意义？

答：在使用self-attention通过上下文词语计算当前词特征的时候，X先通过WQ、WK、WV线性变换为QKV，然后使用QK计算得分，最后与V计算加权和而得。

倘若不变换为QKV，直接使用每个token的向量表示点积计算重要性得分，那在softmax后的加权平均中，该词本身所占的比重将会是最大的，使得其他词的比重很少，无法有效利用上下文信息来增强当前词的语义表示。而变换为QKV再进行计算，能有效利用上下文信息，很大程度上减轻上述的影响。

## BERT中Transformer中Self-attention后为什么要加前馈网络？

答：由于self-attention中的计算都是线性了，为了提高模型的非线性拟合能力，需要在其后接上前馈网络。

## BERT中Transformer中的Self-attention多个头的作用？

答：类似于cnn中多个卷积核的作用，使用多头注意力，能够从不同角度提取信息，提高信息提取的全面性。

## BERT参数量计算

**bert的参数主要可以分为四部分：embedding层的权重矩阵、multi-head attention、layer normalization、feed forward。**

```python3
BertModel(vocab_size=30522，
hidden_size=768，max_position_embeddings=512，
token_type_embeddings=2)
```

**一、embedding层**

embedding层有三部分组成：token embedding、segment embedding和position embedding。

token embedding：词表大小词向量维度就是对应的参数了，也就是30522*768

segment embedding：主要用01来区分上下句子，那么参数就是2*768

position embedding：文本输入最长为512，那么参数为512*768

因此embedding层的参数为(30522+2+512)*768=23835648

**二、multi-head attention**

Q，K，V就是我们输入的三个句子词向量，从之前的词向量分析可知，输出向量大小从len -> len x hidden_size，即len x 768。如果是self-attention，Q=K=V，如果是普通的attention，Q !=K=V。但是，不管用的是self-attention还是普通的attention，参数计算并不影响。因为在输入单头head时，对QKV的向量均进行了不同的线性变换，引入了三个参数，W1，W2，W3。其维度均为：768 x 64。

为什么是64呢，从下图可知，
Wi 的维度 ：dmodel x dk | dv | dq
而：dk | dv | dq = dmodle/h，h是头的数量，dmodel模型的大小，即h=12，dmodle=768；
所以：dk | dv | dq=768/12=64
得出：W1，W2，W3的维度为768 x 64

![在这里插入图片描述](img/qkv-16497775072022.png)

`每个head的参数为768*768/12，对应到QKV三个权重矩阵自然是768*768/12*3，12个head的参数就是768*768/12*3*12，拼接后经过一个线性变换，这个线性变换对应的权重为768*768。`

因此1层multi-head attention部分的参数为`768*768/12*3*12+768*768=2359296`

12层自然是`12*2359296=28311552`

**三、layer normalization**

文章其实并没有写出layernorm层的参数，但是在代码中有，分别为gamma和beta。有三个地方用到了layer normalization，分别是embedding层后、multi-head attention后、feed forward后
①词向量处

![在这里插入图片描述](img/layernorm1-16497775072023.png)

②多头注意力之后

![在这里插入图片描述](img/layernorm2-16497775072024.png)

③最后的全连接层之后

![在这里插入图片描述](img/layernorm3-16497775072025.png)

但是参数都很少，gamma和beta的维度均为768。因此总参数为768 * 2 + 768 * 2 * 2 * 12（层数）

这三部分的参数为`768*2+12*(768*2+768*2)=38400`

**四、feed forward**

![在这里插入图片描述](img/20191017120044663-16497775072026.png)

以上是论文中全连接层的公式，其中用到了两个参数W1和W2，Bert沿用了惯用的全连接层大小设置，即4 * dmodle，为3072，因此，W1，W2大小为768 * 3072，2个为 2 * 768 * 3072。

`12*（768*3072+3072*768）=56623104`

**五、总结**

总的参数=embedding+multi-head attention+layer normalization+feed forward

=23835648+28311552+38400+56623104

=108808704

≈110M

另一种算法

- 词向量参数（包括layernorm） + 12 * （Multi-Heads参数 + 全连接层参数 + layernorm参数）= （30522+512 + 2）* 768 + 768 * 2 + 12 * （768 * 768 / 12 * 3 * 12 + 768 * 768 + 768 * 3072 * 2 + 768 * 2 * 2） = 108808704.0 ≈ 110M

PS：这里介绍的参数仅仅是encoder的参数，基于encoder的两个任务next sentence prediction 和 MLM涉及的参数（分别是768 * 2，768 * 768，总共约0.5M）并未加入，此外涉及的bias由于参数很少，这里也并未加入。

参考

- https://zhuanlan.zhihu.com/p/357353536
- https://blog.csdn.net/weixin_43922901/article/details/102602557
- https://github.com/google-research/bert/issues/656

## **为什么BERT比ELMo效果好？**

从网络结构以及最后的实验效果来看，BERT比ELMo效果好主要集中在以下几点原因：

(1).LSTM抽取特征的能力远弱于Transformer

(2).即使是拼接双向特征，ELMo的特征融合能力仍然偏弱(没有具体实验验证，只是推测)

(3).其实还有一点，BERT的训练数据以及模型参数均多于ELMo，这也是比较重要的一点

## **ELMo和BERT的区别是什么？**

ELMo模型是通过语言模型任务得到句子中单词的embedding表示，以此作为补充的新特征给下游任务使用。因为ELMO给下游提供的是每个单词的特征形式，所以这一类预训练的方法被称为“Feature-based Pre-Training”。而BERT模型是“基于Fine-tuning的模式”，这种做法和图像领域基于Fine-tuning的方式基本一致，下游任务需要将模型改造成BERT模型，才可利用BERT模型预训练好的参数。

## BERT为什么意义重大

BERT相较于原来的RNN、LSTM可以做到并发执行，同时提取词在句子中的关系特征，并且能在多个不同层次提取关系特征，进而更全面反映句子语义。相较于word2vec，其又能根据句子上下文获取词义，从而避免歧义出现。同时缺点也是显而易见的，模型参数太多，而且模型太大，少量数据训练时，容易过拟合。

BERT的“里程碑”意义在于：证明了一个非常深的模型可以显著提高NLP任务的准确率，而这个模型可以从无标记数据集中预训练得到。

既然NLP的很多任务都存在数据少的问题，那么要从无标注数据中挖潜就变得非常必要。在NLP中，一个最直接的有效利用无标注数据的任务就是语言模型，因此很多任务都使用了语言模型作为预训练任务。但是这些模型依然比较“浅”，比如上一个大杀器，AllenNLP的[ELMO](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1802.05365)也就是三层的BiLSTM。

那么有没有可以胜任NLP任务的深层模型？有，就是transformer。这两年，transformer已经在机器翻译任务上取得了很大的成功，并且可以做的非常深。自然地，我们可以用transformer在语言模型上做预训练。因为transformer是encoder-decoder结构，语言模型就只需要decoder部分就够了。OpenAI的[GPT](https://link.zhihu.com/?target=https%3A//s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)就是这样。但decoder部分其实并不好。因为我们需要的是一个完整句子的encoder，而decoder的部分见到的都是不完整的句子。所以就有了BERT，利用transformer的encoder来进行预训练。但这个就比较“反直觉”，一般人想不到了。

**我们再来看下BERT有哪些“反直觉”的设置？**

ELMO的设置其实是最符合直觉的预训练套路，两个方向的语言模型刚好可以用来预训练一个BiLSTM，非常容易理解。但是受限于LSTM的能力，无法变深了。那如何用transformer在无标注数据行来做一个预训练模型呢？一个最容易想到的方式就是GPT的方式，事实证明效果也不错。那还有没有“更好”的方式？直观上是没有了。而BERT就用了两个反直觉的手段来找到了一个方法。

(1) 用比语言模型更简单的任务来做预训练。直觉上，要做更深的模型，需要设置一个比语言模型更难的任务，而BERT则选择了两个看起来更简单的任务：完形填空和句对预测。

(2) 完形填空任务在直观上很难作为其它任务的预训练任务。在完形填空任务中，需要mask掉一些词，这样预训练出来的模型是有缺陷的，因为在其它任务中不能mask掉这些词。而BERT通过随机的方式来解决了这个缺陷：80%加Mask，10%用其它词随机替换，10%保留原词。这样模型就具备了迁移能力。

感觉上，作者Jacob Devlin是拿着锤子找钉子。既然transformer已经证明了是可以handle大数据，那么就给它设计一种有大数据的任务，即使是“简单”任务也行。理论上BiLSTM也可以完成BERT里的两个任务，但是在大数据上BERT更有优势。

**3. 对NLP的影响**

总体上，BERT模型的成功还在于是一种表示学习，即通过一个深层模型来学习到一个更好的文本特征。这种非RNN式的模型是非[图灵完备](https://www.zhihu.com/question/USER_CANCEL)的，无法单独完成NLP中推理、决策等计算问题。当然，一个好的表示会使得后续的任务更简单。

BERT能否像ResNet那样流行还取决于其使用的便利性，包括模型实现、训练、可迁移性等，可能有好的模型出现，但类似的预训练模型会成为NLP任务的标配，就像Word2vec，Glove那样。

最后，BERT也打开了一个思路：可以继续在无标注数据上挖潜，而不仅仅限于语言模型。

## **BERT有什么局限性？**

从XLNet论文中，提到了BERT的两个缺点，分别如下：

- BERT在第一个预训练阶段，假设句子中多个单词被Mask掉，这些被Mask掉的单词之间没有任何关系，是条件独立的，然而有时候这些单词之间是有关系的，比如”New York is a city”，假设我们Mask住”New”和”York”两个词，那么给定”is a city”的条件下”New”和”York”并不独立，因为”New York”是一个实体，看到”New”则后面出现”York”的概率要比看到”Old”后面出现”York”概率要大得多。
  
  - 但是需要注意的是，这个问题并不是什么大问题，甚至可以说对最后的结果并没有多大的影响，因为本身BERT预训练的语料就是海量的(动辄几十个G)，所以如果训练数据足够大，其实不靠当前这个例子，靠其它例子，也能弥补被Mask单词直接的相互关系问题，因为总有其它例子能够学会这些单词的相互依赖关系。
- BERT的在预训练时会出现特殊的[MASK]，但是它在下游的fine-tune中不会出现，这就出现了预训练阶段和fine-tune阶段不一致的问题。其实这个问题对最后结果产生多大的影响也是不够明确的，因为后续有许多BERT相关的预训练模型仍然保持了[MASK]标记，也取得了很大的结果，而且很多数据集上的结果也比BERT要好。但是确确实实引入[MASK]标记，也是为了构造自编码语言模型而采用的一种折中方式。

另外还有一个缺点，是BERT在分词后做[MASK]会产生的一个问题，为了解决OOV的问题，我们通常会把一个词切分成更细粒度的WordPiece。BERT在Pretraining的时候是随机Mask这些WordPiece的，这就可能出现只Mask一个词的一部分的情况，例如：

![https://pic3.zhimg.com/80/v2-fb520ebe418cab927efb64d6a6ae019e_720w.jpg](img/v2-fb520ebe418cab927efb64d6a6ae019e_720w-16497775072037.jpg)

probability这个词被切分成"pro"、”#babi”和”#lity”3个WordPiece。有可能出现的一种随机Mask是把”#babi” Mask住，但是”pro”和”#lity”没有被Mask。这样的预测任务就变得容易了，因为在”pro”和”#lity”之间基本上只能是”#babi”了。这样它只需要记住一些词(WordPiece的序列)就可以完成这个任务，而不是根据上下文的语义关系来预测出来的。类似的中文的词”模型”也可能被Mask部分(其实用”琵琶”的例子可能更好，因为这两个字只能一起出现而不能单独出现)，这也会让预测变得容易。

为了解决这个问题，很自然的想法就是词作为一个整体要么都Mask要么都不Mask，这就是所谓的Whole Word Masking。这是一个很简单的想法，对于BERT的代码修改也非常少，只是修改一些Mask的那段代码。

## **BERT应用于有空格丢失或者单词拼写错误等数据是否还是有效？有什么改进的方法？**

**BERT应用于有空格丢失的数据是否还是有效？**

按照常理推断可能会无效了，因为空格都没有的话，那么便成为了一长段文本，但是具体还是有待验证。而对于有空格丢失的数据要如何处理呢？一种方式是利用Bi-LSTM + CRF做分词处理，待其处理成正常文本之后，再将其输入BERT做下游任务。

**BERT应用于单词拼写错误的数据是否还是有效？**

如果有少量的单词拼写错误，那么造成的影响应该不会太大，因为BERT预训练的语料非常丰富，而且很多语料也不够干净，其中肯定也还是会含有不少单词拼写错误这样的情况。但是如果单词拼写错误的比例比较大，比如达到了30%、50%这种比例，那么需要通过人工特征工程的方式，以中文中的同义词替换为例，将不同的错字/别字都替换成同样的词语，这样减少错别字带来的影响。例如花被、花珼、花钡均替换成花呗。

## **BERT模型的mask相对于CBOW有什么异同点？**

**相同点：**CBOW的核心思想是：给定上下文，根据它的上文 Context-Before 和下文 Context-after 去预测input word。而BERT本质上也是这么做的，但是BERT的做法是给定一个句子，会随机Mask 15%的词，然后让BERT来预测这些Mask的词。

**不同点：**首先，在CBOW中，每个单词都会成为input word，而BERT不是这么做的，原因是这样做的话，训练数据就太大了，而且训练时间也会非常长。

其次，对于输入数据部分，CBOW中的输入数据只有待预测单词的上下文，而BERT的输入是带有[MASK] token的“完整”句子，也就是说BERT在输入端将待预测的input word用[MASK] token代替了。

另外，通过CBOW模型训练后，每个单词的word embedding是唯一的，因此并不能很好的处理一词多义的问题，而BERT模型得到的word embedding(token embedding)融合了上下文的信息，就算是同一个单词，在不同的上下文环境下，得到的word embedding是不一样的。

**为什么BERT中输入数据的[mask]标记为什么不能直接留空或者直接输入原始数据，在self-attention的Q K V计算中，不与待预测的单词做Q K V交互计算？**

这个问题还要补充一点细节，就是数据可以像CBOW那样，每一条数据只留一个“空”，这样的话，之后在预测的时候，就可以将待预测单词之外的所有单词的表示融合起来(均值融合或者最大值融合等方式)，然后再接上softmax做分类。

乍一看，感觉这个idea确实有可能可行，而且也没有看到什么不合理之处，但是需要注意的是，这样做的话，需要每预测一个单词，就要计算一套Q、K、V。就算不每次都计算，那么保存每次得到的Q、K、V也需要耗费大量的空间。总而言之，这种做法确实可能也是可行，但是实际操作难度却很大，从计算量来说，就是预训练BERT模型的好几倍(至少)，而且要保存中间状态也并非易事。其实还有挺重要的一点，如果像CBOW那样做，那么文章的“创新”在哪呢~

## **词袋模型到word2vec改进了什么？word2vec到BERT又改进了什么？**

**词袋模型到word2vec改进了什么？**

词袋模型(Bag-of-words model)是将一段文本（比如一个句子或是一个文档）用一个“装着这些词的袋子”来表示，这种表示方式不考虑文法以及词的顺序。**而在用词袋模型时，文档的向量表示直接将各词的词频向量表示加和**。通过上述描述，可以得出词袋模型的两个缺点：

- 词向量化后，词与词之间是有权重大小关系的，不一定词出现的越多，权重越大。
- 词与词之间是没有顺序关系的。

而word2vec是考虑词语位置关系的一种模型。通过大量语料的训练，将每一个词语映射成一个低维稠密向量，通过求余弦的方式，可以判断两个词语之间的关系，word2vec其底层主要采用基于CBOW和Skip-Gram算法的神经网络模型。

因此，综上所述，词袋模型到word2vec的改进主要集中于以下两点：

- 考虑了词与词之间的顺序，引入了上下文的信息
- 得到了词更加准确的表示，其表达的信息更为丰富

**word2vec到BERT又改进了什么？**

word2vec到BERT的改进之处其实没有很明确的答案，如同上面的问题所述，BERT的思想其实很大程度上来源于CBOW模型，如果从准确率上说改进的话，BERT利用更深的模型，以及海量的语料，得到的embedding表示，来做下游任务时的准确率是要比word2vec高不少的。实际上，这也离不开模型的“加码”以及数据的“巨大加码”。再从方法的意义角度来说，BERT的重要意义在于给大量的NLP任务提供了一个泛化能力很强的预训练模型，而仅仅使用word2vec产生的词向量表示，不仅能够完成的任务比BERT少了很多，而且很多时候直接利用word2vec产生的词向量表示给下游任务提供信息，下游任务的表现不一定会很好，甚至会比较差。

## BERT的优缺点

优点：

- 并行，解决长时依赖，双向特征表示，特征提取能力强，有效捕获上下文的全局信息，缓解梯度消失的问题等，BERT擅长解决NLU任务。

缺点：

- 生成任务表现不佳：预训练过程和生成过程的不一致，导致在生成任务上效果不佳；
- 采取独立性假设：没有考虑预测[MASK]之间的相关性，是对语言模型联合概率的有偏估计（不是密度估计）；
- 输入噪声[MASK]，造成预训练-精调两阶段之间的差异；
- 无法适用于文档级别的NLP任务，只适合于句子和段落级别的任务；

## elmo、GPT、bert三者之间有什么区别？

之前介绍词向量均是静态的词向量，无法解决一词多义等问题。下面介绍三种elmo、GPT、bert词向量，它们都是基于语言模型的动态词向量。下面从几个方面对这三者进行对比：

（1）**特征提取器**：elmo采用LSTM进行提取，GPT和bert则采用Transformer进行提取。很多任务表明Transformer特征提取能力强于LSTM，elmo采用1层静态向量+2层LSTM，多层提取能力有限，而GPT和bert中的Transformer可采用多层，并行计算能力强。

（2）**单/双向语言模型**：

- GPT采用单向语言模型，elmo和bert采用双向语言模型。但是elmo实际上是两个单向语言模型（方向相反）的拼接，这种融合特征的能力比bert一体化融合特征方式弱。
- GPT和bert都采用Transformer，Transformer是encoder-decoder结构，GPT的单向语言模型采用decoder部分，decoder的部分见到的都是不完整的句子；bert的双向语言模型则采用encoder部分，采用了完整句子。

## BERT变体有哪些

### XLNet

这个是融合了GPT和Bert两家的模型。

GPT是自编码模型， 通过双向LSTM编码提取语义。

Bert是自回归模型，不分前后，一坨扔进去，再用attention加强提取语义的效果。

XLNet融合了Bert的结构和GPT的有向预测，具体做法有点抽象，请耐心。

首先，XLNet觉得Bert给的任务太简单了。Bert是Mask language model，举例说明，假如Bert顺序输入abcd四个词，Bert会随机mask掉一部分词，假设mask成了ab[mask]d，接下来Bert要根据这个mask序列预测mask位置的词"c"。

XLNet认为不该一下子把所有词都告诉网络，可以先告诉网络第一个词是a，然后预测一下，再告诉网络一二位置的词是ab，再预测一下，最后告诉网络一二四位置的词是abd，再预测一下。这样力求网络能以最少的信息预测出结果。更有甚者，可能先告诉b，再告诉bd，再告诉abd.

为了实现上面的训练方式，XLNet这样做：

对于序列abcd，先给其加上position embedding. 文字不好表述，就是加了一维位置向量。

随机打散，可能变成bcda, 可能变成adbc. 这里以adbc为例。

根据a->d->b->c这个序列，我们预测a时什么都看不到，预测d时能看到a，预测b时能看到ad，预测c时能看到adb。我们调换一下顺序：a:None,b:ad,c:abd,d:a。这四种情况就对应了上述在分别mask掉a、b、c、d时的某一种情况。我们完全可以根据原始的输入"abcd"再加上01的掩码获得上述情况下的各种输入，即a位置掩盖所有，b位置掩盖c，c位置不掩盖，d位置掩盖bc。如果是自监督学习，就自己掩盖自己，如果是任务学习，就不掩盖自己。

总之，使用这种掩码机制可以等效为mask机制+序列预测的难度加强版。因为掩码机制使用矩阵，看着很像attention，就蹭个热度。又加之网络本身有自己的标准attention，故得名“双流attention”.

通过掩码机制，可以省略添加mask标记的过程，而众所周知mask标记会些许干扰语义模型的表示，所以XLNet算是解决了这个问题。

### RoBERTa

RoBERTa 模型是BERT 的改进版(从其名字来看，A Robustly Optimized BERT，即简单粗暴称为强力优化的BERT方法)。 在模型规模、算力和数据上，与BERT相比主要有以下几点改进：

- 更大的模型参数量（论文提供的训练时间来看，模型使用 1024 块 V100 GPU 训练了 1 天的时间）
- 更大batch size。RoBERTa 在训练过程中使用了更大的bacth size。尝试过从 256 到 8000 不等的bacth size。
- 更多的训练数据（包括：CC-NEWS 等在内的 160GB 纯文本。而最初的BERT使用16GB BookCorpus数据集和英语维基百科进行训练）

另外，RoBERTa在训练方法上有以下改进：

- 去掉下一句预测(NSP)任务。
- 动态掩码。BERT 依赖随机掩码和预测 token。原版的 BERT 实现在数据预处理期间执行一次掩码，得到一个静态掩码。 而 RoBERTa 使用了动态掩码：每次向模型输入一个序列时都会生成新的掩码模式。这样，在大量数据不断输入的过程中，模型会逐渐适应不同的掩码策略，学习不同的语言表征。
- 文本编码。Byte-Pair Encoding（BPE）是字符级和词级别表征的混合，支持处理自然语言语料库中的众多常见词汇。原版的 BERT 实现使用字符级别的 BPE 词汇，大小为 30K，是在利用启发式分词规则对输入进行预处理之后学得的。Facebook 研究者没有采用这种方式，而是考虑用更大的 byte 级别 BPE 词汇表来训练 BERT，这一词汇表包含 50K 的 subword 单元，且没有对输入作任何额外的预处理或分词。

### ALBERT

ALBERT是紧跟着RoBERTa出来的，也是针对Bert的一些调整，重点在减少模型参数，对速度倒是没有特意优化。

提出了两种能够大幅减少预训练模型参数量的方法

- Factorized embedding parameterization。将embedding matrix分解为两个大小分别为V x E和E x H矩阵。这使得embedding matrix的维度从O(V x H)减小到O(V x E + E x H)。
- Cross-layer parameter sharing，即多个层使用相同的参数。参数共享有三种方式：只共享feed-forward network的参数、只共享attention的参数、共享全部参数。ALBERT默认是共享全部参数的。

使用Sentence-order prediction (SOP)来取代NSP。具体来说，其正例与NSP相同，但负例是通过选择一篇文档中的两个连续的句子并将它们的顺序交换构造的。这样两个句子就会有相同的话题，模型学习到的就更多是句子间的连贯性。

## 参考资料

[BERT面试8问8答](https://www.jianshu.com/p/56a621e33d34)

[关于BERT的若干问题整理记录](https://zhuanlan.zhihu.com/p/95594311)

[如何评价 BERT 模型？](https://www.zhihu.com/question/298203515)

[transformer、bert、ViT常见面试题总结](https://www.jianshu.com/p/55b4de3de410)

[BERT及其变种们](https://blog.csdn.net/qq_39006282/article/details/107251957)

---

# 文件：AI算法\推荐\collaborative_filtering.md

---

Author: Summer;     Email: huangmeihong11@sina.com

# 协同过滤(collaborative filtering)

## 直观解释

协同过滤是推荐算法中最常用的算法之一，它根据user与item的交互，发现item之间的相关性，或者发现user之间的相关性，进行推荐。比如你有位朋友看电影的爱好跟你类似，然后最近新上了《调音师》，他觉得不错，就会推荐给你，这是最简单的基于user的协同过滤算法（user-based collaboratIve filtering），还有一种是基于item的协同过滤算法（item-based collaborative filtering），比如你非常喜欢电影《当幸福来敲门的时候》，那么观影系统可能会推荐一些类似的励志片给你，比如《风雨哈佛路》等。如下主要分析user-based，item-based同理。

## 导图

![图片](https://uploader.shimo.im/f/8mslGdORa5YzVRXW!thumbnail)

## 核心公式

* 符号定义
  $r_{u,i}$：user $u$ 对 item $i$ 的评分
  $\bar{r}_{u}$：user $u$ 的平均评分
  $P_{a,b}$：用户$a,b$都有评价的items集合
* 核心公式

1. item-based CF 邻域方法预测公式

$$
\operatorname{Pred}(u, i)=\overline{r}_{u}+\frac{\sum_{j \in S_{i}}\left(\operatorname{sim}(i, j) \times r_{u, j}\right)}{\sum_{j \in S_{i}} \operatorname{sim}(i, j)}
$$

1. 偏差优化目标

$$
\min _{b} \sum_{(u, i) \in K}\left(r_{(u, i)}-\mu-b_{u}-b_{i}\right)^{2}
$$

其中$(u，i) \in K$表示所有的评分，$\mu$总评分均值，$b_u$为user $u$的偏差，$b_i$为item $i$ 的偏差。

1. - 加入正则项后的Funk SVD 优化公式

$$
\min _{u v} \sum_{(u, i) \in k n o w n}\left(r_{u,i}-u_{u} v_{i}\right)+\lambda\left(|u|^{2}+|v|^{2}\right)
$$

其中$u_u$为user $u$的偏好，即为user特征矩阵$U$的第$u$行，$v_i$为item $i$的特征，即为特征矩阵$V$的第$i$列

## 注意要点

* 相似度与距离之间的关系
  距离越大，相似度越小；距离越小，相似度越高。即在求解最大相似度的时候可以转为求解最小距离。
* 在协同过滤中，常用的相似度函数有哪些，简要说明
  
  * 杰卡德相似度（Jaccard similarity）
    公式：
  
  $$
  sim_{jaccard}(u_{1}, u_{2})=\frac{ \text {items} \text { bought by } u_{1}\  and\  u_{2}}{ \text { items  bought by  } u_{1}\  or\  u_{2}}
  $$
  
  适用于二元情况，即定性情况，比如买或者没买，喜欢或者不喜欢，在数据稀疏的情况，可以转为二元应用。
  
  * 余弦相似度
    公式：
    
    $$
    \operatorname{sim}(u_{1}, u_{2})=\frac{r_{u_{1}} \cdot r_{u_{2}}}{\left|r_{u_{1}}\right|_{2}|r_{u_{2}}|_{2}}=\frac{\sum_{i \in P_{u_1,u_2}} r_{u_{1}, i} r_{u_{2}, i}}{\sqrt{\sum_{i \in P_{u_1}} r_{u_{1},i}^{2}} \sqrt{\sum_{i \in P_{u_2}}r_{u_{2},i}^{2}}}
    $$
    
    考虑不同用户的评价范围不一样，比如乐天派一般评分范围一般会高于悲观的人，会将评分进行去中心化再进行计算，即
  * 修正余弦相似度，公式变为

$$
\operatorname{sim}(u_{1}, u_{2})=\frac{r_{u_{1}} \cdot r_{u_{2}}}{\left|r_{u_{1}}\right|_{2}|r_{u_{2}}|_{2}}=\frac{\sum_{i \in P_{u_1,u_2}} (r_{u_{1}, i}-{\bar{r}_{u_{1}}}) (r_{u_{2}, i}-\bar{r}_{u_2})}{\sqrt{\sum_{i \in P_{u_1}} (r_{u_{1},i}-\bar{r}_{u_{1}})^{2}} \sqrt{\sum_{i \in P_{u_2}}(r_{u_{2},i}-\bar{r}_{u_{2}})^{2}}}
$$

适用于定量情况，比如评分场景，要求数据具有一定的稠密度。注意如果计算一个评价很少电影的用户与一个评价很多电影的用户会导致相似度为0.

* 皮尔森相关系数
  公式：

$$
eratorname{sim}(u_1, u_2)=\frac{\sum_{i \in P_{u_1.u_2}}\left(r_{u_1, i}-\overline{r}_{u_1}\right)\left(r_{u_2, i}-\overline{r}_{u_2}\right)}{\sqrt{\sum_{i \in P_{u_1.u_2}}\left(r_{u_1, i}-\overline{r}_{u_1}\right)^{2}} \sqrt{\sum_{i \in P_{u_1.u_2}}\left(r_{u_2, i}-\overline{r}_{u_2}\right)^{2}}}
$$

皮尔森系数跟修正的余弦相似度几乎一致，两者的区别在于分母上，皮尔逊系数的分母采用的评分集是两个用户的共同评分集（就是两个用户都对这个物品有评价），而修正的余弦系数则采用两个用户各自的评分集。

* $L_{p}-norms$
  公式：

$$
sim(u_1,u_2) =\frac{1}{ \sqrt[p]{| r_{u_1}-r_{u_2} |^p}+1}
$$

$p$取不同的值对应不同的距离公式，空间距离公式存在的不足这边也存在。对数值比较敏感。

* 有了相似度测量后，那么基于邻域的推荐思路是怎样的呢？
  过滤掉被评论较少的items以及较少评价的users，然后计算完users之间的相似度后，寻找跟目标user偏好既有大量相同的items，又存在不同的items的近邻几个users(可采用K-top、阈值法、聚类等方式)，然后进行推荐。步骤如下：
  (1) 选择：选出最相似几个用户，将这些用户所喜欢的物品提取出来并过滤掉目标用户已经喜欢的物品
  (2) 评估：对余下的物品进行评分与相似度加权
  (3) 排序：根据加权之后的值进行排序
  (4) 推荐：由排序结果对目标用户进行推荐
* 协同过滤算法具有特征学习的特点，试解释原理以及如何学习

1. 特征学习：把users做为行，items作为列，即得评分矩阵$R_{m,n}=[r_{i,j}]$，通过矩阵分解的方式进行特征学习，即将评分矩阵分解为$R=U_{m,d}V_{d,n}$，其中$U_{m,d}$为用户特征矩阵，$V_{d,n}$表示items特征矩阵，其中$d$表示对items进行$d$个主题划分。举个简单例子，比如看电影的评分矩阵划分后，$U$中每一列表示电影的一种主题成分，比如搞笑、动作等，$V$中每一行表示一个用户的偏好，比如喜欢搞笑的程度，喜欢动作的程度，值越大说明越喜欢。这样，相当于，把电影进行了主题划分，把人物偏好也进行主题划分，主题是评分矩阵潜在特征。
2. 学习方式
3. - SVD，分解式为
     
     $$
     R_{m,n}=U_{m,m}\Sigma_{m,n}V_{n,n}^T
     $$
     
     其中$U$为user特征矩阵，$\Sigma$为权重矩阵体现对应特征提供的信息量，$V$为item特征矩阵。同时可通过SVD进行降维处理，如下
     ![图片](https://uploader.shimo.im/f/dk4h20R8bkQUajmh!thumbnail)
     奇异值分解的方式，便于处理要目标user（直接添加到用户特征矩阵的尾部即可），然而要求评分矩阵元素不能为空，因此需要事先进行填充处理，同时由于user和item的数量都比较多，矩阵分解的方式计算量大，且矩阵为静态的需要随时更新，因此实际中比较少用。
4. - Funk SVD， Funk SVD 是去掉SVD的$\Sigma$成分，优化如下目标函数，可通过梯度下降法，得到的$U,V$矩阵
     
     $$
     =\min _{u v} \sum_{(u, i) \in k n o w n}\left(r_{u,i}-u_{u} v_{i}\right)+\lambda\left(|u|^{2}+|v|^{2}\right)
     $$
     
     Funk SVD 只要利用全部有评价的信息，不需要就空置进行处理，同时可以采用梯度下降法，优化较为方便，较为常用。
     
     有了user特征信息和item特征信息，就可用$u_{u} v_{i}$对目标用户进行评分预测，如果目标用户包含在所计算的特征矩阵里面的话。针对于新user、新item，协同过滤失效。

* 如何简单计算user偏差以及item偏差？
  
  $$
  b_u=\frac{1}{|I_u|}\sum_{i \in I_u}(r_{u,i}-\mu) \
  b_i=\frac{1}{|U_i|}\sum_{u \in U_i}(r_{u,i}-b_u-\mu)
  $$
* 如何选择协同过滤算法是基于user还是基于item
  一般，谁的量多就不选谁。然而基于user的会给推荐目标带来惊喜，选择的范围更为宽阔，而不是基于推荐目标当前的相似item。因此如果要给推荐目标意想不到的推荐，就选择基于user的方式。可以两者结合。
* 协同过滤的优缺点
  
  1. 缺点：
     (1)稀疏性—— 这是协同过滤中最大的问题，大部分数据不足只能推荐比较流行的items，因为很多人只有对少量items进行评价，而且一般items的量非常多，很难找到近邻。导致大量的user木有数据可推荐（一般推荐比较流行的items），大量的item不会被推荐
     (2)孤独用户——孤独user具有非一般的品味，难以找到近邻，所以推荐不准确
     (3) 冷启动——只有大量的评分之后，才能协同出较多信息，所以前期数据比较少，推荐相对不准确；而如果没有人进行评分，将无法被推荐
     (4)相似性——协同过滤是与内容无关的推荐，只根据用户行为，所以倾向于推荐较为流行的items。
* 优点：
  (1)不需要领域知识，存在users和items的互动，便能进行推荐
  (2)简单易于理解
  (3)相似度计算可预计算，预测效率高
* 协同过滤与关联规则的异同
  关联规则是不考虑tems或者使用它们的users情况下分析内容之间的关系，而协同过滤是不考虑内容直接分析items之间的关系或者users之间的关系。两者殊途同归均能用于推荐系统，但是计算方式不同。
* 实践中的一些注意点
  (1) 过滤掉被评价较少的items
  (2) 过滤掉评价较少的users
  (3) 可用聚类方式缩小搜索空间，但是面临找不到相同偏好的用户（如果用户在分界点，被误分的情况），这种方式通过缩小搜索空间的方式优化协同过滤算法
  (4) 使用的时候，可以考虑时间范围，偏好随着时间的改变会改变

## 面试真题

使用协同过滤算法之前，数据应该如何进行预处理？
协同过滤的方式有哪些？
如何通过相似度计算设计协同过滤推荐系统？
请谈谈你对协同过滤中特征学习的理解？
如何将协同过滤用于推荐系统？
FUNK SVD相对于SVD有哪些优势？
如何求解FUNK SVD？
请描述下协同过滤的优缺点？

---

# 文件：AI算法\推荐\deepfm.md

---

# DeepFM

DeepFM模型是2017年由哈工大与华为联合提出的模型，是对Wide&Deep模型的改进。与DCN不同的是，DeepFM模型是将Wide部分替换为了FM模型，增强了模型的低阶特征交互的能力。关于低阶特征交互，文章的Introduction中也提到了其重要性，例如：

1、用户经常在饭点下载送餐APP，故存在一个2阶交互：app种类与时间戳；

2、青少年喜欢射击游戏和RPG游戏，存在一个3阶交互：app种类、用户性别和年龄；

用户背后的特征交互非常的复杂，低阶和高阶的特征交互都是很重要的，这也证明了Wide&Deep这种模型架构的有效性。DeepFM是一种**端到端的模型**，强调了包括低阶和高阶的特征交互接下来直接对DeepFM模型架构进行介绍，并与其他之前提到过的模型进行简单的对比。

## 模型结构

DeepFM的模型结构非常简单，由Wide部分与Deep部分共同组成，如下图所示：

<img src="http://gzy-gallery.oss-cn-shanghai.aliyuncs.com/work_img/21.png" style="zoom: 50%;" />

在论文中模型的目标是**共同学习低阶和高阶特征交互**，应用场景依旧是CTR预估，因此是一个二分类任务（$y=1$表示用户点击物品，$y=0$则表示用户未点击物品）

### Input与Embedding层

关于输入，包括离散的分类特征域（如性别、地区等）和连续的数值特征域（如年龄等）。分类特征域一般通过one-hot或者multi-hot（如用户的浏览历史）进行处理后作为输入特征；数值特征域可以直接作为输入特征，也可以进行离散化进行one-hot编码后作为输入特征。

对于每一个特征域，需要单独的进行Embedding操作，因为每个特征域几乎没有任何的关联，如性别和地区。而数值特征无需进行Embedding。

Embedding结构如下：

<img src="http://gzy-gallery.oss-cn-shanghai.aliyuncs.com/work_img/22.png" style="zoom: 50%;" />

文章中指出每个特征域使用的Embedding维度$k$都是相同的。

【注】与Wide&Deep不同的是，DeepFM中的**Wide部分与Deep部分共享了输入特征**，即Embedding向量。

#### Wide部分---FM

<img src="http://gzy-gallery.oss-cn-shanghai.aliyuncs.com/work_img/23.png" style="zoom:67%;" />

FM模型[^4]是2010年Rendle提出的一个强大的**非线性分类模型**，除了特征间的线性(1阶)相互作用外，FM还将特征间的(2阶)相互作用作为各自特征潜向量的内积进行j建模。通过隐向量的引入使得FM模型更好的去处理数据稀疏行的问题，想具体了解的可以看一下原文。DeepFM模型的Wide部分就直接使用了FM，Embedding向量作为FM的输入。

$$
y_{F M}=\langle w, x\rangle+\sum_{j_{1}=1}^{d} \sum_{j_{2}=j_{1}+1}^{d}\left\langle V_{i}, V_{j}\right\rangle x_{j_{1}} \cdot x_{j_{2}}
$$

其中$w \in \mathbf{R}^d$，$\langle w, x\rangle$表示1阶特征，$V_i \in \mathbf{R}^k$表示第$i$个隐向量，$k$表示隐向量的维度，

$$
\displaystyle\sum_{j_{1}=1}^{d} \sum_{j_{2}=j_{1}+1}^{d}\left\langle V_{i}, V_{j}\right\rangle x_{j_{1}} \cdot x_{j_{2}}
$$

表示2阶特征。

具体的对于2阶特征，FM论文中有下述计算（采取原文的描述形式），为线性复杂复杂度$O(kn)$：

$$
\begin{aligned} & \sum_{i=1}^{n} \sum_{j=i+1}^{n}\left\langle\mathbf{v}_{i}, \mathbf{v}_{j}\right\rangle x_{i} x_{j} \\=& \frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^{n}\left\langle\mathbf{v}_{i}, \mathbf{v}_{j}\right\rangle x_{i} x_{j}-\frac{1}{2} \sum_{i=1}^{n}\left\langle\mathbf{v}_{i}, \mathbf{v}_{i}\right\rangle x_{i} x_{i} \\=& \frac{1}{2}\left(\sum_{i=1}^{n} \sum_{j=1}^{n} \sum_{f=1}^{k} v_{i, f} v_{j, f} x_{i} x_{j}-\sum_{i=1}^{n} \sum_{f=1}^{n} v_{i, f} v_{i, f} x_{i} x_{i}\right) \\=& \frac{1}{2} \sum_{f=1}^{k}\left(\left(\sum_{i=1}^{n} v_{i, f} x_{i}\right)\left(\sum_{j=1}^{n} v_{j, f} x_{j}\right)-\sum_{i=1}^{n} v_{i, f}^{2} x_{i}^{2}\right) \\=& \frac{1}{2} \sum_{f=1}^{k}\left(\left(\sum_{i=1}^{n} v_{i, f} x_{i}\right)^{2}-\sum_{i=1}^{n} v_{i, f}^{2} x_{i}^{2}\right) \end{aligned}
$$

#### Deep部分

<img src="http://gzy-gallery.oss-cn-shanghai.aliyuncs.com/work_img/24.png" style="zoom:67%;" />

Deep部分是一个前向传播的神经网络，用来学习高阶特征交互。

### Output层

FM层与Deep层的输出相拼接，最后通过一个逻辑回归返回最终的预测结果：

$$
\hat y=sigmoid(y_{FM}+y_{DNN})
$$

## 面试相关

1、Wide&Deep与DeepFM的区别？

Wide&Deep模型，Wide部分采用人工特征+LR的形式，而DeepFM的Wide部分采用FM模型，包含了1阶特征与二阶特征的交叉，且是端到端的，无需人工的特征工程。

2、DeepFM的Wide部分与Deep部分分别是什么？Embedding内容是否共享

Wide：FM，Deep：DNN；

Embedding内容是共享的，在FM的应用是二阶特征交叉时的表征。

---

# 文件：AI算法\推荐\FTRL.md

---

# FTRL

FTRL(Follow the Regularized Leader) 由Google的H. Berendan McMahan 等人于2010年提出【4】,FTRL是一种在线最优化求解算法,结合L1-FOBOS和L1-RDA算法,用于解决在线学习中,权重参数不能产生较好的稀疏性的问题。
由于在线学习涉及内容较多，本文从提升模型稀疏性的角度入手，简单介绍经典的TG, L1-FOBOS, L1-RDA 和 FTRL 算法的原理。

## 模型稀疏性

众所周知，Lasso对权重参数(W)引入L1正则项使得模型的训练结果具有稀疏性,稀疏的模型不仅有变量选择的功能，同时在模型线上进行预测时，可以大大减小运算量。但是在在线学习的场景下,利用SGD的方式进行权重参数(W)的更新,每次只使用一个样本，权重参数的更新具有很大的随机性,无法将权重参数准确地更新为0。为解决这一问题，TG, L1-FOBOS, L1-RDA，FTRL 等一系列算法被提出。

## TG(Truncated Gradient)

为了得到具有稀疏性的权重参数(W)，最简单的方法就是引入一个阈值，当某个权重参数的值小于该阈值就将其置0。TG方法就是在这个想法的基础上，稍加改进，使用如下式的梯度更新方式。

$$
W^{(t+1)}=T_{1}\left(W^{(t)}-\eta^{(t)} G^{(t)}, \eta^{(t)} \lambda^{(t)}, \theta\right)
$$

$$
T_{1}\left(v_{i}, \alpha, \theta\right)=\left\{\begin{array}{ll}
\max \left(0, v_{i}-\alpha\right) & \text { if } v_{i} \in[0, \theta] \\
\min \left(0, v_{i}+\alpha\right) & \text { if } v_{i} \in[-\theta, 0] \\
v_{i} & \text { otherwise }
\end{array}\right.
$$

其中$G^{(t)}$是当前参数的梯度，$\eta^{(t)}$是学习率，$\lambda^{(t)}$控制梯度阶段发生的频次，每k次进行一次梯度截断。$\theta$为梯度截断时设置的阈值。通过调节$\lambda,\theta$可以权重参数的稀疏性。

$$
\lambda^{(t)}=\left\{
\begin{aligned}
k\lambda & , & t\ mod\ k = 0 \\
0 & , & otherwise
\end{aligned}
\right.
$$

## L1-FOBOS

FOBOS(Forward-Backward Splitting)分两步更新权重。

$$
W^{\left(t+\frac{1}{2}\right)}=W^{(t)}-\eta^{(t)} G^{(t)}
$$

$$
W^{(t+1)}=\arg \min _{W}\left\{\frac{1}{2}\left\|W-W^{\left(t+\frac{1}{2}\right)}\right\|^{2}+\eta^{\left(t+\frac{1}{2}\right)} \Psi(W)\right\}
$$

FOBOS的第一步就是正常的梯度下降算法，第二部对W进行调整，引入正则项使得参数具有稀疏性。将以上两部转换为一步，可以有如下表达。

$$
W^{(t+1)}=\operatorname{argmin}_{W}\left\{\frac{1}{2}\left\|W-W^{(t)}+\eta^{(t)} G^{(t)}\right\|^{2}+\eta^{\left(t+\frac{1}{2}\right)} \Psi(W)\right\}
$$

实际使用中，将FOBOS中的正则算子$\Psi(W)$替换成$\lambda\Vert W\Vert_{1}$,通过数学推导，最终可以获得如下的梯度新公式。

$$
w_i^{(t)}=sgnw_{i}^{(t)}-\eta^{(t)}g_i^{(t)})\max(0, \vert w_{i}^{(t)}-\eta^{(t)}g_i^{(t)}\vert - \eta^{(t+\frac{1}{2})}\lambda)
$$

从公式中可以发现，一旦权重参数更新后的值$\vert w_{i}^{(t)}-\eta^{(t)}g_i^{(t)}\vert$小于$\eta^{(t+\frac{1}{2})}\lambda$就将改权重参数置0。

## L1-RDA

RDA(Regularized Dual Average)是牺牲一定精度，进一步提升权重参数稀疏性的方法，如下是L1-RDA使用的权重参数更新公式。

$$
W^{(t+1)}=\underset{W}{\arg\min}\{\frac{1}{t}\Sigma_{r=1}^t G^{(r)}\cdot W+\lambda\Vert W\Vert_1 + \frac{\gamma}{2\sqrt t}\Vert W\Vert_{2}^2\}
$$

其中$\Sigma_{r=1}^t G^{(r)}$是历史的梯度的平均值。
通过数学推导L1-RDA有如下等价的参数更新公式。

$$
w_i^{(t+1)}=\left\{
\begin{matrix}
0 \ & , & if \vert \bar{g}_i^{(t)} < \lambda \\
-\frac{\sqrt{t}}{\gamma}(\bar g _i ^{(t)}-\lambda sgn(\bar g_i^{(t)})) & , & otherwise
\end{matrix}
\right.
$$

从公式中可以发现，一旦权重参数的历史平均梯度小于阈值$\lambda$就将该权重参数置0。

## FTRL

通常情况下，L1-FOBOS在计算最优解的精度上较高,而L1-RDA在损失一定精度的前提下可以获得更加稀疏的权重参数(W)。FTRL结合L1-FOBOS和L1-RDA的优点而产生的算法。
通过数学推导，L1-FOBOS可以写为：

$$
W^{(t+1)}=\arg \min _{W}\left\{G^{(t)} \cdot W+\lambda\|W\|_{1}+\frac{1}{2 \eta^{(t)}}\left\|W-W^{(t)}\right\|_{2}^{2}\right\}
$$

L1-RDA可以写为：

$$
W^{(t+1)}=\arg \min _{W}\left\{G^{(1: t)} \cdot W+t \lambda\|W\|_{1}+\frac{1}{2 \eta^{(t)}}\|W-0\|_{2}^{2}\right\}
$$

$$
其中G^{(1: t)}=\Sigma_i^t G^{(i)}
$$

FTRL结合上两时，可以写作：

$$
W^{(t+1)}=\arg \min _{W}\left\{G^{(1: t)} \cdot W+\lambda_{1}\|W\|_{1}+\lambda_{2} \frac{1}{2}\|W\|_{2}^{2}+\frac{1}{2} \sum_{\mathrm{s}=1} \sigma^{(s)}\left\|W-W^{(s)}\right\|_{2}^{2}\right\}
$$

$其中引入\Vert W\Vert_2^2不会影响稀疏性，同时会使解更加“光滑”。$

通过数学推导，FTRL有如下表达形式：

$$
w_{i}^{(t+1)}=\left\{\begin{array}{ll}
0 & \text { if }\left|z_{i}^{(t)}\right|<\lambda_{1} \\
-\left(\lambda_{2}+\sum_{s=1}^{t} \sigma^{(s)}\right)^{-1}\left(z_{i}^{(t)}-\lambda_{1} \operatorname{sgn}\left(z_{i}^{(t)}\right)\right) & \text { otherwise }
\end{array}\right.
$$

$$
Z^{(t)}=G^{(1: t)}-\sum_{s=1}^{t} \sigma^{(s)} W^{(s)}
$$

##总结
本文简单梳理了在线学习中提升权重参数稀疏性的算法的思想，公式较为繁多。对其中的基础知识和公式推导感兴趣的小伙伴可以参考冯扬的《在线最优化求解》【1】，对于FTRL的工程实现感兴趣的小伙伴可以参阅H. Brendan McMahan 等人于2013发表的论文【2】 ，【3】是2011年发表的一篇关于FTRL和FOBOS, RDA比较的论文。

##参考文献
【1】[冯扬————在线最优化解法](https://wenku.baidu.com/view/a76c760c4b7302768e9951e79b89680203d86bcc.html)
【2】[Ad Click Prediction: a View from the Trenches](https://static.googleusercontent.com/media/research.google.com/zh-CN//pubs/archive/41159.pdf)
【3】[Follow-the-Regularized-Leader and Mirror Descent: Equivalence Theorems and Implicit Updates](https://arxiv.org/abs/1009.3240v1)
【4】[Adaptive Bound Optimization for Online Convex Optimization](https://arxiv.org/abs/1002.4908)

---

# 文件：AI算法\推荐\gbdt_lr.md

---

# gbdt lr

gbdt+lr是facebook提出在线广告模型，我们知道LR之前在广告和推荐系统由于其快速的计算
而被广泛使用，使用由于lr是线性模型，其模型表现能力不强，需要做大量的特征工程。
facebook提出提出使用决策树进行特征embedding。
为了提升线性分类器的准确度，有两种方法进行特征变换：

1. 对于连续特征。先进行离散化bin，然后把bin的编号作为离散型特征。这样的话，线性模型可以分段的学习到一个非线性的映射，在每一段内的映射是不变的。另外，对于bin边界的学习非常重要
2. 对于离散特征。做笛卡尔积，生成的是tuple input features。笛卡尔积穷举了所有的特征组合，其中也包含部分没用的组合特征，不过可以筛选出来。
   提升决策树(boosted decision tree)就可以很方便很好的实现上面我们说的这种非线性和tuple特征变换。对于一个样本，针对每一颗树得到一个类别型特征。该特征取值为样本在树中落入的叶节点的编号。举例来说：
   <img src="../assert/gbdt-lr.png"/>
   上图中的提升决策树包含两棵子树，第一棵树包含3个叶节点，第二棵树包含2个叶节点。输入样本x，在两棵树种分别落入叶子节点2和叶子节点1。那么特征转换就得到特征向量[0 1 0 1 0]。也就是说，把叶节点编号进行one-hot编码。
   那么， 怎么样直观的理解这种特征变化：

+ 看做是一种有监督的特征编码。把实值的vector转换成紧凑的二值的vector。
+ 从根节点到叶节点的一条路径，表示的是在特征上的一个特定的规则。所以，叶节点的编号代表了这种规则。表征了样本中的信息，而且进行了非线性的组合变换。
+ 最后再对叶节点编号组合，相当于学习这些规则的权重。

# 核心思想

1. 数据更新

> 由于推荐和广告等相关的问题，是一个动态的环境，需要对模型进行实时更新，所有对于lr进行在线学习和更新，gbdt可以每天或者几天更新一次。

2. 在线学习的学习率如何设置

> 一般情况有很多学习率更新的方法，可以根据当前系统进行实验得到最好的学习率设置策略。论文中给出一下几种方法:

+ Per-coordinate learning rate:

$$
\eta_{t,i}=\frac{\alpha}{\beta+\sqrt{\sum_{j=1}^{t}}\bigtriangledown_{j,i}^2}
$$

其中，$\alpha, \beta$是两个超参数.

+ Per-weight square root learning rate:

$$
\eta_{t,i}=\frac{\alpha}{\sqrt{n_{t,i}}}
$$

其中，$n_{t,i}$是特征i所有实例的前t次的总和。

+ Per-weight learning rate:

$$
\eta_{t,i}=\frac{\alpha}{n_{t,i}}
$$

+ Global learning rate:

$$
\eta_{t,i}=\frac{\alpha}{\sqrt{t}}
$$

+ Constant learning rate:

$$
\eta_{t,i}=\alpha
$$

3. 为了保证数据的新鲜性，需要进行在线数据加入，所有的曝光的实例，设置时间t，在时间t内被点击设置为label=1,否则设置label=0,注意时间t不能太大也不能太小，根据现实业务进行设置.
4. 样本的均匀采样和负样本数据的下采样，由于负样本太多需要对负样本进行下采样。
5. Model Re-Calibration

> 负样本欠采样可以加快训练速度并提升模型性能。但是同样带来了问题：改变了训练数据分布。所以需要进行校准。

$$
q=\frac{p}{p+(1-p)/w}
$$

其中:

+ w是采样率
+ p是在采样后空间中给出的CTR预估值
+ 计算得到的q就是修正后的结果

# 面试十问

1. lr的权重个数和gbdt的什么有关?

> lr的权重个数，等于gbdt所有叶子节点的个数.

2. 负样本欠采样之后会对模型有什么影响，怎么解决?

> 负样本欠采样可以加快训练速度并提升模型性能。但是同样带来了问题：改变了训练数据分布。所以需要进行校准。
> 
> $$
> q=\frac{p}{p+(1-p)/w}
> $$

3. GBDT特征的重要性是如何评估的?

> 特征j的全局重要度通过特征j在单颗树中的重要度的平均值来衡量:
> 
> $$
> \hat J_j^2=\frac{1}{M}\sum_{m=1}^{M}\hat J_j^2(T_m)
> $$
> 
> 其中，M是树的数量，特征j在单棵树中的重要度如下:
> 
> $$
> \hat J_j^2(T)=\sum_{t=1}^{L-1}\hat i_j^2 I(v_t=j)
> $$

4. gbdt+lr如何训练

> 一般是先训练gbdt在训练lr，首先将数据data分成两部分a和b，a用来训练gbdt，b用来训练lr。其中用a训练gbdt的时候，需要将a分成train_a, valid_a, test_a, 得到gbdt之后。将b通过gbdt得到所有对应叶子节点的下标进行one-hot编码.
> 继续训练b，将b通过gbdt得到update_b, 将update_b分成训练、验证和测试集，训练得到LR.

5. 为什么建树采用ensemble决策树?

> 一棵树的表达能力很弱，不足以表达多个有区分性的特征组合，多棵树的表达能力更强一些。GBDT每棵树都在学习前面棵树尚存的不足，迭代多少次就会生成多少颗树。按paper以及Kaggle竞赛中的GBDT+LR融合方式，多棵树正好满足LR每条训练样本可以通过GBDT映射成多个特征的需求。

6. 为什么建树采用GBDT而非RF?

> RF也是多棵树，但从效果上有实践证明不如GBDT。且GBDT前面的树，特征分裂主要体现对多数样本有区分度的特征；后面的树，主要体现的是经过前N颗树，残差仍然较大的少数样本。优先选用在整体上有区分度的特征，再选用针对少数样本有区分度的特征，思路更加合理，这应该也是用GBDT的原因。

7. GBDT与LR融合方案

> AD ID类特征在CTR预估中是非常重要的特征，直接将AD ID作为feature进行建树不可行，故考虑为每个AD ID建GBDT树。但互联网时代长尾数据现象非常显著，广告也存在长尾现象，为了提升广告整体投放效果，不得不考虑长尾广告。在GBDT建树方案中，对于曝光充分训练样本充足的广告，可以单独建树，发掘对单个广告有区分度的特征，但对于曝光不充分样本不充足的长尾广告，无法单独建树，需要一种方案来解决长尾广告的问题。

> 综合考虑方案如下，使用GBDT建两类树，非ID建一类树，ID建一类树。

> 1）非ID类树：不以细粒度的ID建树，此类树作为base，即便曝光少的广告、广告主，仍可以通过此类树得到有区分性的特征、特征组合。

> 2）ID类树：以细粒度 的ID建一类树，用于发现曝光充分的ID对应有区分性的特征、特征组合。如何根据GBDT建的两类树，对原始特征进行映射？以如下图3为例，当一条样本x进来之后，遍历两类树到叶子节点，得到的特征作为LR的输入。当AD曝光不充分不足以训练树时，其它树恰好作为补充。

# 面试真题

1. 为什么建树采用GBDT而非RF?

# 参考

1. https://blog.csdn.net/u010352603/article/details/80681100
2. http://www.cbdio.com/BigData/2015-08/27/content_3750170.htm
3. https://blog.csdn.net/u014297722/article/details/89420421

---

# 文件：AI算法\推荐\Graph_Embedding.md

---

# Graph Embedding

在许多推荐场景下，可以用网络结构数据来刻画对象（用户、商品等）之间的关系。例如：可以将用户和商品作为网络中的结点，用户和商品之间的边代表购买关系。

Graph Embedding 是一种将网络中对象之间的关系转换为每个对象的（向量）特征的一种技术。其主要想法是输入网络后，为每个对象生成一个（向量）特征，满足在网络中越相似的对象，其向量特征之间距离越接近。

下面主要介绍DeepWalk和Node2Vec两种Graph Embedding 算法。这两种算法利用网络生成对象序列后，采用word2vec算法生成对象的Graph Embedding。

## 1. Deep Walk

DeepWalk 主要由RandomWalk 和 Word2Vec 两部分组成。RandomWalk 用于生成结点（对象）序列，Word2Vec利用结点序列生成对象的Embedding。

在RandomWalk中，给定网络中以任意一点为起点，每次在当前结点的邻居中等概率选择一个节点放入已生成的序列中，并把该结点作为下一个结点重复上述采样过程，直到获得的序列长度达到预设的要求。

在获得足够多的结点序列后，使用Word2Vec算法生成每个对象的Embedding。在论文中使用Word2Vec中的SkipGram算法。

具体算法如下所示。

<div align=center>
<img src="https://raw.githubusercontent.com/Yzmshjd/picBed/main/interview/graph_embedding/alg1.png" width='600' height='500'>
</div>

在DeepWalk中使用深度优先的方式生成对象序列，为了丰富对网络中相似结点的含义，也可以尝试用广度优先的方式生成对象序列。Node2Vec 就是一种在生成对象序列时结合深度优先和广度优先的算法。

## 2. Node2Vec

### 2.1 序列生成算法

Node2Vec 在RandomWalk的基础上引入search bias $\alpha$，通过调节超参数$\alpha$，控制对象序列生成过程中广度优先和深度优先的强度。

RandomWalk的搜索方法比较朴素。在相邻结点之间根据边的权重或者其他业务理解定义转移概率。特别地，DeepWalk 采用等概率的方式搜索下一个结点。转移概率可以有如下的表达形式。

<div align=center>
<img src="https://raw.githubusercontent.com/Yzmshjd/picBed/main/interview/graph_embedding/transition.png" height='100'>
</div>

进一步，Node2Vec在未归一化的转移概率$\pi_{vx}$之前乘以偏置项$\alpha$，来反映序列生成算法对于深度优先和广度优先的偏好。以下是偏置项$\alpha$的具体表达形式。

<div align=center>
<img src="https://raw.githubusercontent.com/Yzmshjd/picBed/main/interview/graph_embedding/prob.png">
</div>

其中$d_{tx}$为顶点$t$和顶点$x$之间的最短路径长度，$p, q$控制深度优先和广度优先的强度。

假设当前随机游走经过边$(t,x)$后达到顶点$v$，以$\pi_{vx}=\alpha_{pq}(t,x)\omega_{vx}$的未归一化概率搜索下一个结点。

<div align=center>
<img src="https://raw.githubusercontent.com/Yzmshjd/picBed/main/interview/graph_embedding/graph.png" width='500'>
</div>

偏置项$\alpha$受到超参数p和q的控制，具体来说p, q的大小会对搜索策略产生如下影响。

Return parameter p的影响：

1. 超参数p影响回到之前结点t的概率大小。如果p越小,则回到之前结点t的概率越大，搜索策略越倾向于在初始结点的附近进行搜索。

In-out parameter q的影响：

1. 超参数q控制着搜索算法对于广度优先和深度优先的偏好。从示意图中，我们可以看到q越小，越倾向于搜索远离初始结点t，与倾向于深度优先的方式。

### 2.2 Embedding学习

Node2vec 采用和SkipGram类似的想法，学习从节点到embedding的函数$f$，使得给定结点$u$，其近邻结点$N_S(u)$的出现的概率最大。近邻结点的是由序列生成算法获得的一系列点。具体数学表达如下。

<div align=center>
<img src='https://raw.githubusercontent.com/Yzmshjd/picBed/main/interview/graph_embedding/opt.png' width=250, height=50>
</div>

在原文中使用了条件独立性假设和特征空间独立行假设，并使用softmax函数来表示概率，将上述优化问题化简为容易求解的优化问题。采用SGD算法获得生成Embedding的函数$f$。具体的化简过程可以参考原文。

如下是Node2Vec的整个算法过程，其中采用了时间复杂度为O(1)的alias采样方法，具体可以参考[2]。

<div align=center>
<img src="https://raw.githubusercontent.com/Yzmshjd/picBed/main/interview/graph_embedding/alg2.png", width='600', height='500'>
</div>

## 面试真题

1. 请结合业务谈一下怎样在推荐场景中建立网络。
2. 在Node2Vec建立对象序列的过程中，怎样实现深度优先和广度优先的？

## 参考文献

1. [浅梦的学习笔记——DeepWalk](https://blog.csdn.net/u012151283/article/details/86806922)
2. [浅梦的学习笔记——Node2Vec](https://blog.csdn.net/u012151283/article/details/87081272)
3. [《深度学习推荐系统》——王喆著](https://zhuanlan.zhihu.com/p/119248677?utm_source=zhihu&utm_medium=social&utm_oi=26827615633408)
4. [DeepWalk: Online Learning of Social Representations](http://www.perozzi.net/publications/14_kdd_deepwalk.pdf)
5. [node2vec: Scalable Feature Learning for Networks](https://arxiv.org/abs/1607.00653)

---

# 文件：AI算法\推荐\向量化搜索.md

---

# 向量化搜索

在高维空间内快速搜索最近邻（Approximate Nearest Neighbor）。召回中，Embedding向量的搜索。

FAISS、kd-tree、局部敏感哈希、【Amnoy、HNSW】

## FAISS

faiss是Facebook的AI团队开源的一套用于做聚类或者相似性搜索的软件库，底层是用C++实现。Faiss因为超级优越的性能，被广泛应用于推荐相关的业务当中。

faiss工具包一般使用在推荐系统中的向量召回部分。在做向量召回的时候要么是u2u，u2i或者i2i，这里的u和i指的是user和item。我们知道在实际的场景中user和item的数量都是海量的，最容易想到的基于向量相似度的召回就是使用两层循环遍历user列表或者item列表计算两个向量的相似度，但是这样做在面对海量数据是不切实际的，faiss就是用来加速计算某个查询向量最相似的topk个索引向量。

**faiss查询的原理：**

faiss使用了PCA和PQ(Product quantization乘积量化)两种技术进行向量压缩和编码，当然还使用了其他的技术进行优化，但是PCA和PQ是其中最核心部分。

### **主要流程**

- 构建索引`index`
- 根据不同索引的特性，对索引进行训练（`train`）
- `add` 添加`xb`数据到索引
- 针对`xq`进行搜索`search`操作

### Example

1、数据集

```python
d = 64                           # dimension
nb = 100000                      # 完整数据集
nq = 10000                       # 查询数据
np.random.seed(1234)           
xb = np.random.random((nb, d)).astype('float32')
xb[:, 0] += np.arange(nb) / 1000.
xq = np.random.random((nq, d)).astype('float32')
xq[:, 0] += np.arange(nq) / 1000.
```

2、构建索引

Faiss围绕`Index`对象构建。它封装了数据库向量集，并可选地对其进行预处理以提高搜索效率。索引的类型很多，我们将使用最简单的索引，它们仅对它们执行暴力L2距离搜索：`IndexFlatL2`。

`d`在我们的例子中，所有索引都需要知道何时建立索引，即它们所操作的向量的维数

```python
index = faiss.IndexFlatL2(d)   # build the index
```

3、对索引进行训练

然后，大多数索引还需要训练阶段，以分析向量的分布。对于`IndexFlatL2`，我们可以跳过此操作。

4、添加数据到索引

构建和训练索引后，可以对索引执行两项操作：`add`和`search`。

将元素添加到索引，我们称之为`add`上`xb`。我们还可以显示索引的两个状态变量：`is_trained`，指示是否需要训练的布尔值，以及`ntotal`索引向量的数量。

一些索引还可以存储与每个向量相对应的整数ID（但不能存储`IndexFlatL2`）。如果未提供ID，则`add`只需将向量序号用作ID，即。第一个向量为0，第二个为1，依此类推。

```python
index.add(xb)                  # add vectors to the index
```

5、对查询数据进行搜索操作

可以对索引执行的基本搜索操作是`k`-最近邻搜索，即。对于每个查询向量，`k`在数据库中找到其最近的邻居。

该操作的结果可以方便地存储在一个大小为`nq`-by-的整数矩阵中`k`，其中第i行包含查询向量i的邻居ID（按距离递增排序）。除此矩阵外，该`search`操作还返回一个具有相应平方距离的`nq`按`k`浮点矩阵。

```python
k = 4                          # we want to see 4 nearest neighbors
D, I = index.search(xb[:5], k) # sanity check, 首先搜索一些数据库向量，以确保最近的邻居确实是向量本身
print(I)
print(D)
D, I = index.search(xq, k)     # actual search
print(I[:5])                   # neighbors of the 5 first queries
print(I[-5:])                  # neighbors of the 5 last queries
```

### 索引方式

Faiss中的稠密向量各种索引都是基于 `Index`实现的，主要的索引方法包括： `IndexFlatL2`、`IndexFlatIP`、`IndexHNSWFlat`、`IndexIVFFlat`、`IndexLSH`、`IndexScalarQuantizer`、`IndexPQ`、`IndexIVFScalarQuantizer`、`IndexIVFPQ`、`IndexIVFPQR`等，[每个方法的具体介绍](https://github.com/facebookresearch/faiss/wiki/Faiss-indexes#summary-of-methods)。

`IndexFlatL2`：

- 基于L2距离的暴力全量搜索，速度较慢，不需要训练过程。

`IndexIVFFlat`：

- 先聚类再搜索，可以加快检索速度；
- 先将`xb`中的数据进行聚类（聚类的数目是超参），`nlist`: 聚类的数目
- `nprobe`: 在多少个聚类中进行搜索，默认为`1`, `nprobe`越大，结果越精确，但是速度越慢

```python
def IndexIVFFlat(nlist):
    quantizer = faiss.IndexFlatL2(d)
    index = faiss.IndexIVFFlat(quantizer, d, nlist)
    print(index.is_trained)
    index.train(xb)
    print(index.is_trained)
    index.add(xb)
    return index
```

`IndexIFVPQ`

- 基于乘积量化（product quantizers）对存储向量进行压缩，节省存储空间
- `m`：乘积量化中，将原来的向量维度平均分成多少份，`d`必须为`m`的整数倍
- `bits`: 每个子向量用多少个`bits`表示

```python
def IndexIVFPQ(nlist, m, bits):
    quantizer = faiss.IndexFlatL2(d)
    index = faiss.IndexIVFPQ(quantizer, d, nlist, m, bits)
    index.train(xb)
    index.add(xb)
    return index
```

## kd树

kd树是一种对k维空间中的实例点进行**存储**以便对其进行**快速检索**的树形数据结构。kd树是**二叉树**，表示对k维空间的一个划分（partition）。**构造kd树相当于不断地用垂直于坐标轴的超平面将k维空间切分，构成一系列的k维超矩形区域**。kd树的每个结点对应于一个k维超矩形区域。

### kd树的结构

kd树是一个二叉树结构，它的每一个节点记载了【特征坐标，切分轴，指向左枝的指针，指向右枝的指针】。

其中，特征坐标是线性空间$\mathbf{R}^n$的一个点$(x_1,...,x_n)$。

切分轴由一个整数$r$表示，这里$1\leq r\leq n$，是我们在$n$ 维空间中沿第$n$维进行一次分割。

节点的左枝和右枝分别都是 kd 树，并且满足：如果 y 是左枝的一个特征坐标，那么$y_r \leq x_r$并且如果 z 是右枝的一个特征坐标，那么$x_r \leq z_r $。

### kd树的构造

通过数据集来构造kd树存储空间，在推荐系统中即用物品Embedding池进行构建。

- 输入：k维空间数据集$T=\left\{x_{1}, x_{2}, \cdots, x_{N}\right\}$，其中$x_{i}=\left(x_{i}^{(1)}, x_{i}^{(2)}, \cdots, x_{i}^{(k)}\right)^{\mathrm{T}},i=1,2,...,N$；
- 输出：kd树；
- （1）开始：构造根结点，根结点对应于包含$T$的$k$维空间的超矩形区域。
  
  选择$x^{(1)}$为坐标轴，以$T$中所有实例的$x^{(1)}$坐标的**中位数为切分点**【若超平面上没有切分点，可以适当移动位置，使得超平面上有点】，将根结点对应的超矩形区域切分为两个子区域。切分由通过切分点并与坐标轴$x^{(1)}$垂直的超平面实现。
  
  由根结点生成深度为1的左、右子结点：左子结点对应坐标$x^{(1)}$小于切分点的子区域，右子结点对应于坐标$x^{(1)}$大于切分点的子区域。
  将**落在切分超平面上的实例点保存在根结点**。
- （2）重复：对深度为$j$的结点，选择$x^{(l)}$为切分的坐标轴，$l=j(\bmod k)+1$，以该结点的区域中所有实例的$x^{(l)}$坐标的中位数为切分点，将该结点对应的超矩形区域切分为两个子区域。切分由通过切分点并与坐标轴$x^{(l)}$垂直的超平面实现。
  由该结点生成深度为$j+1$的左、右子结点：左子结点对应坐标$x^{(l)}$小于切分点的子区域，右子结点对应坐标$x^{(l)}$大于切分点的子区域
  将落在切分超平面上的实例点保存在该结点。
- （3）直到两个子区域没有实例存在时停止。从而形成kd树的区域划分。

最后每一部分都只剩一个点，将他们记在最底部的节点中。因为不再有未被记录的点，所以不再进行切分。

![img](https://pic2.zhimg.com/80/v2-93ada931fd95e04f829318d5983aebe5_1440w.png)

![img](https://pic1.zhimg.com/80/v2-ef599210d778bc0b11ae7b1d0116c28c_1440w.png)

### 搜索kd树

在推荐系统中，即通过用户的Embedding向量来查找与其近邻的$K$个物品Embedding向量。

- 输入：已构造的kd树；目标点$x$；
- 输出：$x$的$k$近邻；
- 设有一个$ k$个空位的列表，用于保存已搜寻到的最近点。
- （1）在kd树中找出包含目标点$x$的叶结点：从根结点出发，递归地向下访问树。若目标点$x$当前维的坐标小于切分点的坐标，则移动到左子结点，否则移动到右子结点，直到子结点为叶结点为止；
- （2）如果**“当前k近邻点集”元素数量小于$k$**或者**叶节点距离小于“当前k近邻点集”中最远点距离**，那么将叶节点插入“当前k近邻点集”；
- （3）递归地向上回退，在每个结点进行以下操作：
  
  - 如果“当前k近邻点集”元素数量小于k或者当前节点距离小于“当前k近邻点集”中最远点距离，那么将该节点插入“当前k近邻点集”。
  - 检查该子结点的父结点的另一子结点对应的区域是否与以目标点为球心、以目标点与于“当前k近邻点集”中最远点间的距离为半径的超球体相交。如果相交，可能在另一个子结点对应的区域内存在距目标点更近的点，移动到另一个子结点，接着，递归地进行最近邻搜索；如果不相交，向上回退；
- 当回退到根结点时，搜索结束，最后的“当前k近邻点集”即为$x$的k近邻点。

kd树的平均计算复杂度是$log(N)$。

参考资料：[kd 树算法之详细篇](https://zhuanlan.zhihu.com/p/23966698)

## 局部敏感哈希

局部敏感哈希的基本思想：

> 让相邻对的点落入同一个“桶”，这样在进行最近邻搜索时，仅需要在一个桶内，或相邻的几个桶内的元素中进行搜索即可。如果保持每个桶中的元素个数在一个常数附近，就可以把最近邻搜索的时间复杂度降低到常数级别。

首先需要明确一个概念，

> 在欧式空间中，将高维空间的点映射到低维空间，原本相近的点在低维空间中肯定依然相近，但原本远离的点则有一定概率变成相近的点。

所以**利用低维空间可以保留高维空间相近距离关系的性质**，就可以构造局部敏感哈希的桶。

对于Embedding向量，可以用内积操作构建局部敏感哈希桶。假设$\mathbf{v}$是高维空间中的$k$维Embedding向量，$\mathbf{x}$是随机生成的$k$维向量。内积操作可以将$\mathbf{v}$映射到1维空间，成为一个数值：

$$
h(\mathbf{v})=\mathbf{v}\cdot \mathbf{x}
$$

因此，可以使用哈希函数$h(v)$进行分桶：

$$
h^{x,b}(\mathbf{v})=\lfloor x \frac{\mathbf{x}\cdot \mathbf{v}+ b}{w}\rfloor x
$$

其中$\lfloor \rfloor$是向下取整，$w$是分桶宽度，$b$是0到$w$间的一个均匀分布随机变量，避免分桶边界固化。

如果仅采用一个哈希函数进行分桶，则必然存在相近点误判的情况。有效的解决方法是采用$m$个哈希函数同时进行分桶。同时掉进$m$个哈希函数的同一个桶的两点，是相似点的概率大大增加。找到相邻点集合后，取$K$近邻个。

采用多个哈希函数进行分桶，存在一个待解决的问题，到底通过“与”还是“或”：

- 与：候选集规模减小，计算量降低，但可能会漏掉一些近邻点；
- 或：候选集中近邻点的召回率提高，但候选集的规模变大，计算开销变大；

以上是将欧式空间中内积操作的局部敏感哈希使用方法；还有余弦相似度、曼哈顿距离、切比雪夫距离、汉明距离等。

---

# 文件：AI算法\风控\面试题.md

---

# 风控算法的一些面试题小结

> 来源：https://zhuanlan.zhihu.com/p/719621121

## 1.**[逻辑回归](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=逻辑回归&zhida_source=entity)的优缺点，局限性在哪？**

优点：

- 实现简单，速度快，占用内存小，可在短时间内迭代多个版本的模型。
- 模型的可解释性非常好，可以直接看到各个特征对模型结果的影响，可解释性在金融领域非常重要，所以在目前业界大部分使用的仍是[逻辑回归模型](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=逻辑回归模型&zhida_source=entity)。
- 模型客群变化的敏感度不如其他高复杂度模型，因此稳健更好，[鲁棒性](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=鲁棒性&zhida_source=entity)更强。
- 特征工程做得好，模型的效果不会太差，并且特征工程可以并行开发，大大加快开发的速度。
- 模型的结果可以很方便的转化为[策略规则](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=策略规则&zhida_source=entity)，且线上部署简单。

2）缺点和局限性:

- 容易欠拟合，相比[集成模型](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=集成模型&zhida_source=entity)，准确度不是很高。
- 对数据的要求比较高，逻辑回归对缺失值，异常值，[共线性](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=共线性&zhida_source=entity)都比较敏感，且不能直接处理非线性的特征。所以在数据清洗和特征工程上会花去很大部分的时间。
- 在金融领域对场景的适应能力有局限性，例如数据不平衡问题，[高维特征](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=高维特征&zhida_source=entity)，大量多类特征，逻辑回归在这方面不如[决策树](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=决策树&zhida_source=entity)适应能力强。

总结而言 就是 模型简单、解释性强、方便部署、对负责数据处理能力有限。

## 2.逻辑回归输出值是0到1之间的值，这个值是真实的概率吗？

逻辑回归输出的值在0到1之间，并非所有在0到1之间的值都可以表示概率。这个0到1之间的值是通过[sigmoid函数](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=sigmoid函数&zhida_source=entity)将线性回归的结果映射到0到1之间，我们也可以采用其他函数将数值映射到非0到1之间，因此，用sigmoid函数得到的结果并不是绝对的真实概率值，但是可以认为他是一个接近真实概率的值。

## 3.逻辑回归做分类时样本应满足什么条件

逻辑回归做分类时样本应满足的条件包括因变量为分类变量、观测间独立性、样本量要求、线性关系、无[多重共线性](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=多重共线性&zhida_source=entity)、无明显的离群点等。

## 4.**逻辑回归解决过拟合的方法有哪些？**

- 减少特征数量，在实际使用中会用很多方法进行特征筛选，例如基于[IV值](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=IV值&zhida_source=entity)的大小，变量的稳定性，变量之间的相关性等。
- [正则化](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=正则化&zhida_source=entity)，常用的有L1正则化和L2正则化。

## 5.**什么是特征的离散化和特征交叉？逻辑回归为什么要对特征进行离散化？**

- [特征离散化](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=特征离散化&zhida_source=entity)是将数值型特征（一般是连续型的）转变为离散特征，例如评分卡中的woe转化，就是将特征进行分箱，再将每个分箱映射到[woe值](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=woe值&zhida_source=entity)上，就转换为了离散特征。[特征交叉](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=2&q=特征交叉&zhida_source=entity)也叫作特征组合，是将单独的特征进行组合，使用相乘/相除/[笛卡尔积](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=笛卡尔积&zhida_source=entity)等形成合成特征，有助于表示非线性关系。比如使用One-Hot向量的方式进行特征交叉。这种方式一般适用于离散的情况，我们可以把它看做基于业务理解的逻辑和操作，例如经度和纬度的交叉，年龄和性别的交叉等。
- 实际工作中很少直接将连续型变量带入逻辑回归模型中，而是将特征进行离散化后再加入模型，例如评分卡的分箱和woe转化。这样做的优势有以下几个：
- - 1）特征离散化之后，起到了简化模型的作用，使模型变得更稳定，降低了模型过拟合的风险。
  - 2）离散化之后的特征对异常数据有很强的鲁棒性，实际工作中的哪些很难解释的异常数据一般不会做删除处理，如果特征不做离散化，这个异常数据带入模型，会给模型带来很大的干扰。
  - 3）离散特征的增加和减少都很容易，且稀疏向量的[内积乘法](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=内积乘法&zhida_source=entity)运算速度快，易于模型的快速迭代。
  - 4）逻辑回归属于[广义线性模型](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=广义线性模型&zhida_source=entity)，表达能力有限，特征离散化之后，每个离散变量都有单独的权重，相当于给模型引入了非线性，能够提高模型的表达能力。
  - 5）离散化后的特征可进行特征交叉，进一步引入非线性，提高模型的表达能力。

总而言之，特征离散化就是将数值型特征转为离散型特征，便于逻辑回归进行建模，特征交叉就是提高特征的非线性关系。

**6.做评分卡中为什么要进行WOE化？**

- 更好的解释性，变量离散化之后可将每个箱体映射到woe值，而不是通常做one-hot转换。
- woe化之后可以计算每个变量的IV值，可用来筛选变量。
- 对[离散型变量](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=离散型变量&zhida_source=entity)，woe可以观察各个level间的跳转对odds的提升是否呈线性。
- 对连续型变量，[woe](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=7&q=woe&zhida_source=entity)和IV值为分箱的合理性提供了一定的依据，也可分析变量在业务上的可解释性。
- 用[woe编码](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=woe编码&zhida_source=entity)可以处理缺失值问题。

## 7.**逻辑回归的[特征系数](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=特征系数&zhida_source=entity)的绝对值可以认为是特征的重要性吗？**

首先特征系数的绝对值越大，对分类效果的影响越显著，但不能表示系数更大的特征重要性更高。因为改变变量的尺度就会改变系数的绝对值，而且如果特征是线性相关的，则系数可以从一个特征转移到另一个特征，特征间相关性越高，用系数解释变量的重要性就越不可靠。

## 8.逻辑回归为什么要用[极大似然函数](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=极大似然函数&zhida_source=entity)作为损失函数？

![img](https://picx.zhimg.com/v2-92b4c97fbc3cf76a9b14d839ca4a247f_1440w.jpg)

## 9.**决策树模型的优缺点及适用性？**

优点

- 直观易理解：决策树的结构类似于人类的决策过程，因此它的机制解释起来简单，即使是非专业人士也能理解其工作原理。
- 可解释性强：决策树可以生成清晰的规则，这些规则可以直接用于业务决策或者进一步的分析。
- 数据处理能力：决策树能够处理标称型和数值型数据，同时也能处理有缺失属性的样本以及不相关的特征。
- 运行速度快：在测试数据集时，决策树的运行速度比较快，这使得它在实际应用中非常高效。
- 适应性强：[决策树算法](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=决策树算法&zhida_source=entity)适用于小数据集，并且[时间复杂度](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=时间复杂度&zhida_source=entity)较小，适合快速处理大量数据。

缺点

- 容易过拟合：[决策树模型](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=2&q=决策树模型&zhida_source=entity)容易对训练数据过度学习，导致泛化性能差，即在新数据上的表现可能不佳。
- 稳定性问题：决策树对于数据的微小变化可能非常敏感，这可能导致模型的不稳定。
- 复杂性控制：随着数据量的增加，决策树可能会变得过于复杂，难以管理和解释。

## 10.**简述一下决策树的原理以及树的构建过程。**

决策树时基于树的结构进行决策的，学习过程包括[特征选择](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=特征选择&zhida_source=entity)，决策树的生成和剪枝过程。决策树的学习过程通常是递归地选择最优特征，并用最优特征对数据集进行分割。决策树通过一系列的分裂和判断条件来预测目标变量的值，其构建过程是一个递归地将数据集划分为越来越小的子集的过程，直到满足停止条件。

## **11.简述一下ID3，C4.5，CART三类决策树的原理和异同点。**

- ID3选择最佳分割点是基于[信息增益](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=信息增益&zhida_source=entity)的，信息增益越大，表明使用这个属性来划分所获得的“纯度提升”越大。C4.5对ID3进行了改进，因为ID3使用的信息增益对数据划分时，可能出现每个结点只包含一个样本，这些子节点的纯度已经达到最大，但是，这样的决策树并不具有[泛化能力](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=泛化能力&zhida_source=entity)，无法对新样本进行预测。且ID3不能处理连续型变量和缺失值。而C4.5使用[信息增益率](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=信息增益率&zhida_source=entity)来选择属性，克服了信息增益选择属性时偏向选择值多的属性的不足。且可以处理连续型变量和缺失值。
- C4.5是基于ID3的改进版，只能用于分类。而CART树既可以做分类，也可以做回归。CART的本质是对特征空间进行二元划分，所以CART生成的是一颗二叉树，且可以对类别型变量和数值型变量进行分裂。对分类型变量进行划分时，分为等于该属性和不等于该属性，在对连续型变量进行划分时，分为大于和小于，在做分类是使用的是GINI系数作为划分标准，在做回归时使用的是[均方误差](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=均方误差&zhida_source=entity)。

## 12.**决策树对缺失值是如何处理的？**

决策树处理缺失要考虑以下三个问题：

当开始选择哪个属性来划分数据集时，样本在某几个属性上有缺失怎么处理：

- 忽略这些缺失的样本。
- 填充缺失值，例如给属性A填充一个均值或者用其他方法将缺失值补全。
- 计算信息增益率时根据缺失率的大小对信息增益率进行打折，例如计算属性A的信息增益率，若属性A的缺失率为0.9，则将信息增益率乘以0.9作为最终的信息增益率。

一个属性已经被选择，那么在决定分割点时，有些样本在这个属性上有缺失怎么处理？

- 忽略这些缺失的样本。
- 填充缺失值，例如填充一个均值或者用其他方法将缺失值补全。
- 把缺失的样本，按照无缺失的样本被划分的子集样本个数的相对比率，分配到各个子集上去，至于那些缺失样本分到子集1，哪些样本分配到子集2，这个没有一定准则，可以随机而动。
- 把缺失的样本分配给所有的子集，也就是每个子集都有缺失的样本。
- 单独将缺失的样本归为一个分支。

决策树模型构建好后，测试集上的某些属性是缺失的，这些属性该怎么处理？

- 如果有单独的缺失值分支，依据此分支。
- 把待分类的样本的属性A分配一个最常出现的值，然后进行[分支预测](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=分支预测&zhida_source=entity)。
- 待分类的样本在到达属性A结点时就终止分类，然后根据此时A结点所覆盖的叶子节点类别状况为其分配一个发生概率最高的类。

总结而言就是 删除、填充、分配权重、专门分支、概率最大类别

## **13.为什么决策树不需要对数据做归一化等预处理？**

决策树是一种[概率模型](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=概率模型&zhida_source=entity)，所以不需要做归一化，因为它不关心变量的值，而是关心变量的分布和变量之间的条件概率，所以归一化这种数值缩放，不影响分裂结点位置。

## 14.**如何解决决策树的过拟合问题？**

剪枝技术

- 预剪枝：预剪枝是在决策树完全生成之前就停止树的构建过程。这种方法可以通过设定树的最大深度、节点包含的最小样本数等规则来实现。预剪枝的优点是简单易行，但缺点是很难精确控制何时停止增长，可能会导致欠拟合。
- 后剪枝：后剪枝是先让决策树完整生成，然后根据一定的标准（如验证集的误差）来剪掉某些子树。

限制树的深度

- 设定最大深度：通过设置决策树的最大深度，可以限制树的复杂度，从而减少过拟合的风险。这种方法简单直观，但需要合理选择深度参数。
- 设定最小叶节点样本数：当某个节点的样本数小于预设的阈值时，停止该节点的进一步分裂。这可以防止树在只有少量样本的情况下继续生长，从而避免过拟合。

特征选择

- 去除无关特征：只保留对目标变量有显著影响的特征，可以减少模型的复杂度，从而降低过拟合的风险。
- 特征工程：通过对原始数据进行转换和组合，创造出更有信息量的特征。良好的特征工程可以提高模型的性能，同时减少过拟合的可能性。

## 15.**什么是集成学习？集成学习有哪些框架？简单介绍各个框架的常用算法。**

- 集成学习是一种优化手段和策略，通常是结合多个简单的弱分类器来集成模型组，去做更可靠的决策。一般的弱分类器可以是决策树，SVM，kNN等构成，其中的模型可以单独来训练，并且这些弱分类器以某种方式结合在一起去做出一个总体预测。集成学习就是找出哪些弱分类器可以结合在一起，以及如何结合的方法。目前集成学习主要有bagging，boosting，stacking三种：
- bagging：对训练集进行随机子抽样，对每个子训练集构建基模型，对所有的基模型的预测结果进行综合产生最后的预测结果。如果是分类算法，则用多数投票法确定最终类别，如果是回归算法，则将各个回归结果做算术平均作为最终的预测值。常用的bagging算法：[随机森林](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=随机森林&zhida_source=entity)
- boosting：训练过程为阶梯状，基模型按照次序进行训练（实际上可以做到并行处理），先给定一个初始训练数据，训练出第一个基模型，根据基模型的表现对样本进行调整，在之前基模型预测错误的样本上投入更多的关注，然后用调整后的样本训练下一个基模型，重复上述过程N次，将N个基模型进行加权结合，输出最后的结果。常用的算法有GBDT，XGBOOST等。
- stacking：是一种组合分类器的方法，以两层为例，第一层由多个基学习器组成，其输入为原始训练集，第二层的模型则是以第一层基学习器的输出作为训练集进行再训练(一般用LR进行回归组合），从而得到完整的stacking模型。要得到stacking模型，关键在于如何构造第二层的特征，构造第二层特征的原则是尽可能的避免信息泄露，因此对原始训练集常常采用类似于K折[交叉验证](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=交叉验证&zhida_source=entity)的划分方法。各个基模型要采用相同的Kfold，这样得到的第二层特征的每一折（对应于之前的K折划分）都将不会泄露进该折数据的目标值信息 ，从而尽可能的降低过拟合的风险。

## 16.**简单描述一下模型的偏差和方差？bagging和boosting主要关注哪个？**

- 偏差描述的是预测值与真实值的差距，偏差越大，越偏离真实数据。
- 方差描述的是预测值的变化范围，离散程度，方差越大，数据分布越分散。
- bagging主要关注的是降低方差，boosting主要关注降低偏差。

## 17.**简述一下随机森林的原理，随机森林的构造过程。**

随机森林是bagging算法的代表，使用了CART树作为弱分类器，将多个不同的决策树进行组合，利用这种组合来降低单棵决策树的可能带来的片面性和判断不准确性。对于普通的决策树，是在所有样本特征中找一个最优特征来做决策树的左右子树划分，而随机森林会先通过自助采样的方法（bootstrap）得到N个训练集，然后在单个训练集上会随机选择一部分特征，来选择一个最优特征来做决策树的左右子树划分，最后得到N棵决策树，对于分类问题，按多数投票的准则确定最终结果，对于回归问题，由多棵决策树的预测值的平均数作为最终结果。随机森林的随机性体现在两方面，一个是选取样本的随机性，一个是选取特征的随机性，这样进一步增强了模型的泛化能力。

## 18.**随机森林的优缺点？**

优点：

- 训练可以高度并行化，训练速度快，效率高。
- 两个随机性的引入，使得随机森林不容易过拟合，具有很好的抗噪声能力。
- 由于每次不再考虑全部的特征属性，二是特征的一个子集，所以相对于bagging计算开销更小，效率更高。
- 对于数据的适应能力强，可以处理连续型和离散型的变量，数据无需规范化。
- 可以输出变量的重要程度，被认为是一种不错的降维方法。

缺点：

- 在某些噪声较大的分类问题和或回归问题上容易过拟合。
- 模型的可解释性比较差，无法控制模型内部的运行。
- 对于小数据或者低维数据，效果可能会不太好。

## **19.随机森林为什么不容易过拟合？随机森林的随机怎么体现？**

随机森林之所以不容易过拟合，是因为它通过随机性引入了模型的多样性，同时利用集成学习的优势提高了模型的泛化能力。尽管每棵决策树都可能过拟合，但当它们组合在一起时，过拟合的部分会被相互抵消，从而使得整个模型具有良好的泛化性能。

随机性

- 样本的随机选择：在构建每棵树时，随机森林采用自助采样法从原始数据集中抽取样本，大约有1/3的样本被抽样多次，2/3的样本一次都没有被抽中。这种随机性确保了每棵树的训练集都是不同的，从而增加了模型的多样性。
- 特征的随机选择：在每个节点分裂时，随机森林不是考虑所有的特征，而是从所有特征中随机选择一部分特征，再从中选出最佳的分割特征。这样的做法进一步增加了模型的随机性和多样性。

## 20.随机森林怎么判断特征重要性的？

- 随机森林对于特征重要性的评估思想：判断每个特征在随机森林中的每颗树上做了多大的贡献，然后取个平均值，最后比一比特征之间的贡献大小。其中关于贡献的计算方式可以是基尼指数或袋外数据错误率。
- 基于**[基尼系数](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=基尼系数&zhida_source=entity)**：如果特征X出现在决策树J中的结点M，则计算节点M分枝前后的Gini指数变化量，假设随机森林由N棵树，则计算N次的Gini系数，最后将所有的Gini系数做一个归一化处理就得到了该特征的重要性。
- 基于**袋外数据错误率**：袋外数据指的是每次随机抽取未被抽取达到的数据，假设袋外的样本数为O，将这O个数据作为测试集，代入已生成好的随机森林分类器，得到预测的分类结果，其中预测错误的样本数为X，则袋外数据误差为X/O，这个袋外数据误差记为errOOB1，下一步对袋外数据的特征A加入噪声干扰，再次计算袋外误差errOOB2，假设随机森林由N个分类器，则特征A的重要性为：sum(errOOB2-errOOB1)/N,其依据就是，如果一个特征很重要，那么其变动后会非常影响测试误差，如果测试误差没有怎么改变，则说明特征A不重要。

## 21.**XGBOOST和GBDT的区别在哪里？**

XGBoost和GBDT都是集成学习中用于回归和分类的梯度提升算法。以下是两者的对比介绍：

优化方法

- XGBoost：XGBoost使用二阶泰勒展开，利用了损失函数的一阶和二阶导数信息，这允许算法在优化时更加精确地调整步长和方向。
- GBDT：GBDT仅使用一阶导数信息，即梯度信息进行优化，通常采用简单的[梯度下降法](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=梯度下降法&zhida_source=entity)。

正则化

- XGBoost：XGBoost在代价函数中加入了正则项来控制模型的复杂度，如L1和L2正则化，这有助于减少过拟合。
- GBDT：GBDT通常不包括显式的正则项。

缺失值处理

- XGBoost：XGBoost能够自动处理数据中的缺失值，通过分裂点的选择来优化含有缺失值的特征。
- GBDT：GBDT需要对缺失值进行预处理，如填充或删除。

并行计算

- XGBoost：XGBoost支持特征级别的并行计算，显著提高了算法的计算效率。
- GBDT：GBDT通常是串行执行，虽然可以通过一些技术实现并行化，但不如XGBoost高效。

## 22.**为什么XGBOOST要用泰勒展开，优势在哪里？**

xgboost使用了一阶和二阶偏导，二阶导数有利于梯度下降的更快更准，使用泰勒展开取得函数做自变量的二阶导数形式，可以在不选定损失函数具体形式的情况下，仅仅依靠输入数据的值就可以进行叶子分裂优化计算，本质上也就把损失函数的选取和模型算法的优化分开来了，这种去耦合增加了xgboost的适用性，使得它按需选取损失函数，既可以用于分类，也可以用于回归。

## 23.**XGBOOST是如何处理缺失值的？**

xgboost为缺失值设定了默认的分裂方向，xgboost在树的构建过程中选择能够最小化训练误差的方向作为默认的分裂方向，即在训练时将缺失值划入左子树计算训练误差，再划入右子树计算训练误差，然后将缺失值划入误差小的方向。

## 24.**XGBOOST的并行化是如何实现的？**

- xgboost的并行不是在tree粒度上的并行，xgboost也是一次迭代完才能进行下一次迭代（第t次迭代的损失函数包含了第t-1次迭代的预测值），它的并行处理是在特征粒度上的，在决策树的学习中首先要对特征的值进行排序，然后找出最佳的分割点，xgboost在训练之前，就预先对数据做了排序， 然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量。这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。
- 可并行的近似[直方图](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=直方图&zhida_source=entity)算法。树节点在进行分裂时，我们需要计算每个特征的每个分割点对应的增益，即用贪心法枚举所有可能的分割点。当数据无法一次载入内存或者在分布式情况下，[贪心算法](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=贪心算法&zhida_source=entity)效率就会变得很低，所以xgboost还提出了一种可并行的近似直方图算法，用于高效地生成候选的分割点。

## 25.**XGBOOST采样时有放回的还是无放回的**

xgboost属于boosting方法的一种，所以采样时样本是不放回的，因而每轮计算样本不重复，另外，xgboost支持子采样，每轮计算可以不使用全部的样本，以减少过拟合。另外一点是xgboost还支持列采样，每轮计算按百分比随机抽取一部分特征进行训练，既可以提高速度又能减少过拟合。

## 26.**XGBOOST的调参步骤是怎样的？**

- 保持learning rate和其他booster相关的参数不变，调节和estimators的参数。learing_rate可设为0.1, max_depth设为4-6之间，min_child_weight设为1，subsample和colsample_bytree设为0.8 ，其他的参数都设为默认值即可。
- 调节max_depth 和 min_child_weight参数，首先，我们先大范围地粗调参数，然后再小范围地微调。
- gamma参数调优
- subsample和colsample_bytree 调优
- 正则化参数调优，选择L1正则化或者L2正则化
- 缩小learning rate，得到最佳的learning rate值

## 27.**LightGBM相比XGBOOST在原理和性能上的差异？**

1.速度和内存上的优化：

- xgboost用的是预排序（pre-sorted）的方法， 空间消耗大。这样的算法需要保存数据的特征值，还保存了特征排序的结果（例如排序后的索引，为了后续快速的计算分割点），这里需要消耗训练数据两倍的内存。 其次，时间上也有较大的开销，在遍历每一个分割点的时候，都需要进行分裂增益的计算，消耗的代价大。
- LightGBM用的是直方图（Histogram）的决策树算法，直方图算法的基本思想是先把连续的浮点特征值离散化成k个整数，同时构造一个宽度为k的直方图。在遍历数据的时候，根据离散化后的值作为索引在直方图中累积统计量，当遍历一次数据后，直方图累积了需要的统计量，然后根据直方图的离散值，遍历寻找最优的分割点。

2.准确率上的优化：

- xgboost 通过level（depth）-wise策略生长树， Level-wise过一次数据可以同时分裂同一层的叶子，容易进行多线程优化，也好控制模型复杂度，不容易过拟合。但实际上Level-wise是一种低效的算法，因为它不加区分的对待同一层的叶子，带来了很多没必要的开销，因为实际上很多叶子的分裂增益较低，没必要进行搜索和分裂。
- LightGBM通过leaf-wise（best-first）策略来生长树， Leaf-wise则是一种更为高效的策略，每次从当前所有叶子中，找到分裂增益最大的一个叶子，然后分裂，如此循环。因此同Level-wise相比，在分裂次数相同的情况下，Leaf-wise可以降低更多的误差，得到更好的精度。Leaf-wise的缺点是可能会长出比较深的决策树，产生过拟合。因此LightGBM在Leaf-wise之上增加了一个最大深度的限制，在保证高效率的同时防止过拟合。

3.对类别型特征的处理**：**

- xgboost不支持直接导入类别型变量，需要预先对类别型变量作亚编码等处理。如果类别型特征较多，会导致[哑变量](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=哑变量&zhida_source=entity)处理后衍生后的特征过多，学习树会生长的非常不平衡，并且需要非常深的深度才能来达到较好的准确率。
- LightGBM可以支持直接导入类别型变量（导入前需要将字符型转为整数型，并且需要声明类别型特征的字段名），它没有对类别型特征进行[独热编码](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=独热编码&zhida_source=entity)，因此速度比独热编码快得多。LightGBM使用了一个特殊的算法来确定属性特征的分割值。基本思想是对类别按照与目标标签的相关性进行重排序，具体一点是对于保存了类别特征的直方图根据其累计值(sum_gradient/sum_hessian)重排序,在排序好的直方图上选取最佳切分位置。

## 28.**特征工程的一般步骤是什么？什么是特征工程的迭代？**

特征工程常规步骤：

- 数据获取，数据的可用性评估（覆盖率，准确率，获取难度）
- 探索性数据分析，对数据和特征有一个大致的了解，同时进行数据的质量检验，包括缺失值，异常值，重复值，一致性，正确性等。
- 特征处理，包括数据预处理和特征转换两部分，数据预处理主要做清洗工作（缺失值，异常值，错误值，数据格式），特征转换即对连续特征，离散特征，时间序列特征进行转换，便于入模。
- 特征构建，特征构建的目的是找寻与目标变量相关且区分度较好的特征。常用的方法有特征交叉，[四则运算](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=四则运算&zhida_source=entity)，基于业务理解进行头脑风暴构建特征等。
- 特征筛选，大量的特征中选择少量的有用特征，也叫作特征降维，常用的方法有[过滤法](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=过滤法&zhida_source=entity)，包装法，嵌入法。

特征工程的迭代:

- 选择特征：具体问题具体分析，通过查看大量的数据和基于对业务的理解，从数据中查找可以提出出数据的关键。
- [设计特征](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=设计特征&zhida_source=entity)：可以自动进行特征提取工作，也可以手工进行特征的构建。
- 选择特征：使用不同的特征构造方法，从多个角度来评判这个特征是否适合放入模型中。
- 计算模型：计算模型在该特征上所提升的准确率。
- 上线测试：通过在线测试的效果来评估特征是否有效。

## 29.**常用的特征工程方法有哪些？**

- 特征处理：数据的预处理包括异常值和缺失值，要根据实际的情况来处理。特征转换主要有标准化，归一化，区间缩放，二值化等，根据特征类型的不同选择合适的转换方法。
- 特征构建：特征之间的四则运算（有业务含义）,基于业务理解构造特征，分解类别特征，特征交叉组合等。
- 特征筛选：过滤法，封装法，嵌入法。

## 30.**在实际的风控建模中怎么做好特征工程？**

- 因为做[风控模型](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=风控模型&zhida_source=entity)大部分的数据源来自第三方，所以第三方数据的可用性评估非常重要，一方面需要了解这些特征底层的衍生逻辑，判断是否与目标变量相关。另一方面考察数据的覆盖率和真实性，覆盖率较低和真实性存疑的特征都不能使用在模型中。
- 基于金融的数据特点，在特征筛选这个步骤上考量的因素主要有：一个是时间序列上的稳定性，衡量的指标可以是PSI，方差或者IV。一个是特征在样本上覆盖率，也就是特征的缺失率不能太高。另外就是特征的可解释性，特征与目标变量的关系要在业务上要解释的通。
- 如果第三方返回有用户的原始底层数据，例如社保的缴纳记录，运营商的通话/短信记录，则需要在特征衍生上进行特殊处理，基于自身对数据的敏感性和业务的理解，构建具有金融，风险属性的特征，也可以与业务部门进行沟通找寻与业务相关的特征。

## **31.在风控项目中原始数据通常有哪些问题？该如何解决？**

- 一些特征的底层逻辑不清晰，字面上的意思可能与实际的衍生逻辑相悖，这个需要与第三方数据供应商进行沟通，了解清楚特征的衍生逻辑。
- 数据的真实性可能存在问题。比如一个特征是历史总计，但第三方只是爬取了用户近2年的数据，这样的特征就不符合用户的真实情况。所以对数据的真实性校验显得非常重要。
- 有缺失的特征占的比例较高。在进行缺失值处理前先分析缺失的原因，而不是盲目的进行填充，删除等工作。另外也要分析缺失是否有风险属性，例如芝麻分缺失的用户相对来说风险会较高，那么缺失可以当做一个类别来处理。
- 大量多类特征如何使用。例如位置信息，设备信息这些特征类别数较多，如果做亚编码处理会造成[维度灾难](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=维度灾难&zhida_source=entity)，目前常用的方法一个是降基处理，减少类别数，另一个是用xgboost来对类别数做重要性排序，筛选重要性较高的类别再做亚编码处理。

## 32.**在做评分卡或其他模型中，怎么衡量特征(数据)的有用性？**

- 特征具有金融风险属性，且与目标变量的关系在业务上有良好的可解释性。
- 特征与目标变量是高度相关的，衡量的指标主要是IV。
- 特征的准确率，这个需要了解特征的衍生逻辑，并与实际一般的情况相比较是否有异常。
- 特征的覆盖率，一般来说覆盖率要达到70%以上。
- 特征的稳定性，特征的覆盖率，分布，区分效果在时间序列上的表现比较稳定。
- 特征的及时性，最好是能代表用户最近的信用风险情况。

## 33.**缺失值的处理方式有哪些？风控建模中该如何合理的处理缺失？**

- 首先要了解缺失产生的原因，因数据获取导致的缺失建议用填充的方式(缺失率比较低的情况下），因用户本身没有这个属性导致的缺失建议把缺失当做一个类别。另外可以分析缺失是否有风险属性，有的话最好当做一个类别来处理。
- 风控模型对于缺失率的要求比较高，尤其是评分卡。个人认为，缺失率在30%以上的特征建议不要用，缺失率在10%以下的变量可用中位数或随机森林来填充，10%-30%的缺失率建议当做一个类别。对于xgboost和lightgbm这类可以自动处理缺失值的模型可以不做处理。

## **34.如何发现数据中的异常值？对异常值是怎么处理的？**

- 一种是基于统计的异常点检测算法例如极差，[四分位数间距](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=四分位数间距&zhida_source=entity)，均差，标准差等，这种方法适合于挖掘单变量的数值型数据。另一种主要通过距离方法来检测异常点，将数据集中与大多数点之间距离大于某个阈值的点视为异常点，检测的标准有[欧式距离](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=欧式距离&zhida_source=entity)，绝对距离。
- 对于异常值先检查下是不是数据错误导致的，数据错误的异常作删除即可。如果无法判别异常的原因，要根据实际情况而定，像评分卡会做WOE转换，所以异常值的影响不大，可以不做处理。若异常值的数量较多，建议将异常值归为一类，数量较少作删除也可以。

## **35.对于时间序列特征，连续特征，离散特征这三类是怎么做特征转换的？**

- 时间序列特征：将时间变量的维度进行分离（年/月/日/时/分/秒），或者与位置变量进行结合衍生成新的特征。
- 连续型特征：标准化，归一化，区间缩放，离散化。在评分卡中主要用的是离散化，离散化常用的方法有卡房分箱，决策树分箱，等频和等深分箱。
- 离散型特征：如果类别数不是很多，适合做亚编码处理，对于无序离散变量用独热编码，有序离散变量用顺序编码。如果类别数较多，可用平均数编码的方法。

## **36.如何处理样本不平衡的问题？**

- 在风控建模中出现样本不平衡主要是坏样本的数量太少，碰到这个问题不要急着试各种[抽样方法](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=抽样方法&zhida_source=entity)，先看一下坏用户的定义是否过于严格，过于严格会导致坏样本数量偏少，中间样本偏多。坏用户的定义一般基于滚动率分析的结果，不过实际业务场景复杂多样，还是得根据情况而定。
- 确定好坏用户定义是比较合理的之后，先尝试能不能扩大数据集，比如一开始取得是三个月的用户数据，试着将时间线延长来增加数据。因为机器学习是使用现在的数据在整个数据分布上进行估计，因此更多的数据往往能够得到更多的分布信息，以及更好的分布估计。
- 对数据集进行抽样，一种是进行[欠采样](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=欠采样&zhida_source=entity)，通过减少大类的数据样本来降低数据的不平衡，另一种是进行[过采样](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=过采样&zhida_source=entity)，通过增加小类数据的样本来降低不平衡，实际工作中常用SMOTE方法来实现过采样。
- 尝试使用xgboost和lightgbm等对不平衡数据处理效果较好的模型。

## **37.特征衍生的方法有哪些？说说你平时工作中是怎么做特征衍生的？**

常规的特征衍生方法：

- 基于对业务的深入理解，进行头脑风暴，构造特征。
- 特征交叉，例如对类别特征进行交叉相乘。
- 分解类别特征，例如对于有缺失的特征可以分解成是否有这个类别的二值化特征，或者将缺失作为一个类别，再进行亚编码等处理。
- 重构数值量（单位转换，整数小数拆分，构造阶段性特征）
- 特征的四则运算，例如取平均/最大/最小，或者特征之间的相乘相除。

平时工作特征衍生的做法：

- 因为风控模型通常需要好的解释能力，所以在特征衍生时也会考虑到衍生出来的特征是否与目标变量相关。例如拿到运营商的通话记录数据，可以衍生一个"在敏感时间段（深夜）的通话次数占比"，如果占比较高，用户的风险也较大。
- 平常会将大量的时间和精力花在底层数据的衍生上，这个不仅需要对业务的理解，也需要一定的想象力进行头脑风暴，即使衍生出来的特征90%都效果不佳，但只要剩下的10%是好的特征，那对于模型效果的提升是很显著的。
- 对于评分卡来说，特征需要好的解释能力，所以一些复杂的衍生方法，像特征交叉，log转换基本不会用到。但如果是xgboost等复杂模型，进行特征交叉等方法或许有比较好的效果。

## **38.特征筛选的作用和目的？筛选的特征需要满足什么要求？**

作用和目的：

- 简化模型，增加模型的可解释性， 降低模型过拟合的风险。
- 缩短模型的训练时间。
- 避免维度灾难。

筛选特征满足的要求：

具有良好的区分能力。

- 可解释性好，与目标变量的关系在业务上能解释的通。
- 在时间序列上有比较好的稳定性。
- 特征的用户覆盖率符合要求。

## **39.特征筛选的方法有哪些？每种方法的优缺点？实际工作中用到了哪些方法？**

Filter（过滤法）：按照发散性或者相关性对各个特征进行评分，设定阈值或者待选择阈值的个数，选择特征。

- 相关系数，方差（适用于连续型变量），[卡方检验](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=卡方检验&zhida_source=entity)（适用于类别型变量），[信息熵](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=信息熵&zhida_source=entity)，IV。实际工作中主要基于IV和相关性系数（皮尔逊系数）。
- 优点：算法的通用性强；省去了分类器的训练步骤，[算法复杂性](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=算法复杂性&zhida_source=entity)低，因而适用于大规模数据集；可以快速去除大量不相关的特征，作为特征的预筛选器非常合适。
- 缺点：由于算法的评价标准独立于特定的学习算法，所选的特征子集在分类准确率方面通常低于Wrapper方法。

Wrapper（封装法）：封装式特征选择是利用学习算法的性能评价特征子集的优劣。因此，对于一个待评价的特征子集，Wrapper方法需要训练一个分类器，根据分类器的性能对该特征子集进行评价。

- 方法有完全搜索（递归消除法），[启发式搜索](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=启发式搜索&zhida_source=entity)（前向/后向选择法，逐步选择法），随机搜索（训练不同的特征子集）。实际工作中主要用到启发式搜索，例如评分卡的逐步逻辑回归。
- 优点：相对于Filter方法，Wrapper方法找到的特征子集分类性能通常更好。
- 缺点：Wrapper方法选出的特征通用性不强，当改变学习算法时，需要针对该学习算法重新进行特征选择；由于每次对子集的评价都要进行分类器的训练和测试，所以算法计算复杂度很高，尤其对于大规模数据集来说，算法的执行时间很长。

Embedded（嵌入法）：先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征。类似于Filter方法，但是是通过训练来确定特征的优劣。

- 一种是基于惩罚项，例如[岭回归](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=岭回归&zhida_source=entity)，lasso回归，L1/L2正则化。另一种是基于树模型输出的特征重要性，在实际工作中较为常用，可选择的模型有随机森林，xgboost，lightgbm。
- 优点：效果最好速度最快，模式单调，快速并且效果明显。
- 缺点：如何参数设置， 需要对模型的算法原理有较好的理解。

## **40.简单介绍一下风控模型常用的评估指标。**

- [混淆矩阵](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=混淆矩阵&zhida_source=entity)指标：精准率，查全率，假正率。当模型最后转化为规则时，一般用这三个指标来衡量规则的有效性。要么注重精准率，要么注重查全率，两者不可兼而得之。
- ROC曲线和AUC值，ROC曲线是一种对于查全率和假正率的权衡，具体方法是在不同阈值下以查全率作为纵轴，假正率作为横轴绘制出一条曲线。曲线越靠近左上角，意味着越多的正例优先于负例，模型的整体表现也就越好。AUC是ROC曲线下面的面积，AUC可以解读为从所有正例中随机选取一个样本A，再从所有负例中随机选取一个样本B，分类器将A判为正例的概率比将B判为正例的概率大的可能性。在对角线（随机线）左边的点上TPR总大于FPR，意为正例被判为正例的概率大于负例被判为正例的概率。从另一个角度看，由于画ROC曲线时都是先将所有样本按分类器的预测概率排序，所以AUC反映的是分类器对样本的排序能力。AUC越大，自然排序能力越好，即分类器将越多的正例排在负例之前。
- KS：用于区分预测正负样本分隔程度的评价指标，KS越大，表示模型能将好坏样本区分开的程度越大。KS的绘制方法是先将每个样本的预测结果化为概率或者分数，将最低分到最高分（分数越低，坏的概率越大）进行排序做样本划分，横轴就是样本的累计占比，纵轴则是好坏用户的累计占比分布曲线，KS值为两个分布的最大差值（绝对值）。KS值仅能代表模型的区隔能力，KS不是越高越好，KS如果过高，说明好坏样本分的过于开了，这样整体分数（概率）就是比较极端化的分布状态，这样的结果基本不能用。
- 基尼系数：其横轴是根据分数（概率）由高到低累计的好用户占总的好用户的比例，纵轴是分数（概率）从高到低坏用户占总的坏用户的比例。由于分数高者为低风险用户，所以累计坏用户比例的增长速度会低于累计好用户比例，因此，基尼曲线会呈现向下弯曲的形式，向下突出的半月形的面积除以下方三角形的面积即是基尼系数。基尼系数越大，表示模型对于好坏用户的区分能力越好。

## **41.什么是模型的欠拟合和过拟合？**

- 欠拟合指的是模型没有很好的捕捉到数据特征，不能很好的拟合数据。
- 过拟合指的是模型把数据学习的太彻底，以至于把噪声数据学习进去了，这样模型在预测未知数据时，就不能正确的分类，模型的泛化能力太差。

## **42.如何判断模型是否存在过拟合或欠拟合？对应的解决方法有哪些？**

- 判断模型是否存在过拟合/欠拟合主要用学习曲线，学习曲线指的是通过画出不同训练集大小时训练集和交叉验证的准确率，可以看到模型在新数据上的表现，进而来判断模型是否方差偏高（过拟合）或偏差过高（欠拟合）。当训练集和测试集的误差收敛但却很高时，即为欠拟合，当训练集和测试集的误差之间有大的差距时，为过拟合。
- 解决欠拟合的方法：增加效果好的特征，添加多项式特征，减小正则化参数等。
- 解决过拟合的方法：使用更多的数据，选择更加合适的模型，加入正则项等。

## **43.什么是正则化？什么是L1正则化和L2正则化？**

- 正则化是在模型的loss function的基础上，加上了一些正则化项或者称为模型复杂度惩罚项，它会向学习算法略微做些修正，从而让模型能更好地泛化。这样反过来能提高模型在不可见数据上的性能。
- L1正则化就是在loss function后边所加正则项为L1范数，加上L1范数容易得到稀疏解，所以L1正则化会趋向于产生少量的特征。
- L2正则化就是loss function后边所加正则项为L2范数的平方，加上L2正则相比于L1正则来说，得到的解比较平滑（不是稀疏），所以L2正则化会使特征的解趋近于0，但不会为0。

## **44.正则化为什么可以防止过拟合？**

最简单的解释是正则化对模型参数添加了先验，在数据少的时候，[先验知识](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=先验知识&zhida_source=entity)可以防止过拟合。举个例子：抛一枚硬币5次，得到的全是正面，则得出结论：正面朝上的概率为1，这类似于模型的过拟合，如果加上硬币朝上的概率是0.5的先验，结果就不会这么离谱，这就是正则。

## **45.什么是交叉验证？交叉验证的目的是什么？有哪些优点？**

交叉验证概念：

交叉验证，就是重复的使用数据，把得到的样本数据进行切分，组合为不同的训练集和测试集，用训练集来训练模型，用测试集来评估模型预测的好坏。在此基础上可以得到多组不同的训练集和测试集，某次训练集中的某样本在下次可能成为测试集中的样本，即所谓"交叉"。　

交叉验证的目的：

评估给定算法在特定数据集上训练后的泛化性能，比单次划分训练集和测试集的方法更加稳定，全面。

交叉验证的优点：

- 如果只是对数据随机划分为训练集和测试集，假如很幸运地将难以分类的样本划分进训练集中，则在测试集会得出一个很高的分数，但如果不够幸运地将难以分类的样本划分进测试集中，则会得到一个很低的分数。所以得出的结果随机性太大，不够具有代表性。而交叉验证中每个样本都会出现在训练集和测试集中各一次，因此，模型需要对所有样本的泛化能力都很好，才能使其最后交叉验证得分，及其平均值都很高，这样的结果更加稳定，全面，具有说服力。
- 对数据集多次划分后，还可以通过每个样本的得分比较，来反映模型对于训练集选择的敏感性信息。
- 对数据的使用更加高效，可以得到更为精确的模型。

## **46.交叉验证常用的方法有哪些？**

- 标准K折交叉验证：K是自定义的数字，通常取5或10，如果设为5折，则会训练5个模型，得到5个精度值。
- 分层K折交叉验证：如果一个数据集经过标准K折划分后，在测试集上只有一种类别，则无法给出分类器整体性能的信息，这种情况用标准K折是不合理的。而在分层K折交叉验证中，每个折中的类别比例与整个数据集类别比例相同，这样能对泛化性能做出更可靠的估计。
- 留一法交叉验证：每次划分时，把单个数据点作为测试集，如果数据量小，能得到更好的估计结果，数据量很大时则不适用。
- 打乱划分交叉验证：每次划分数据时为训练集取样train_size个点，为测试集取样test_size个点，将这一划分划分方法重复n_splits次。这种方法还允许每次迭代中使用部分数据，可通过设置train_size和test_size之和不为0来实现，用这种方法对数据进行二次采样可能对大型数据上的试验很用用。另外也有分层划分的形式（ StratifiedShuffleSplit），为分类任务提供更可靠的结果。
- 分组交叉验证：适用于数据中的分组高度相关时，以group数组作为参数，group数组表示数据中的分组，在创建训练集和测试集的时候不应该将其分开，也不应该与类别标签弄混。

## 47.**互联网金融场景下的的风控模型种类？**

- 获客阶段：用户响应模型，风险预筛选模型。
- 授信阶段：申请评分模型，反欺诈模型，[风险定价](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=风险定价&zhida_source=entity)模型，收益评分模型。
- 贷后阶段：行为评分模型，交易欺诈模型，客户流失模型。
- 催收阶段：早期催收模型，晚期催收模型。

## 48.**简单描述一下风控建模的流程？、**

- 前期准备工作：不同的模型针对不同的业务场景，在建模项目开始前需要对业务的逻辑和需求有清晰的理解，明确好模型的作用，项目周期时间和安排进度，以及模型效果的要求。
- 模型设计：包括模型的选择（评分卡还是集成模型），单个模型还是做模型的细分，是否需要做拒绝推论，观察期，表现期的定义，好坏用户的定义，数据的获取途径等都要确定好。
- 数据拉取及清洗：根据观察期和表现期的定义从数据池中取数，并进行前期的数据清洗和稳定性验证工作，数据清洗包括用户唯一性检查，缺失值检查，异常值检查等。稳定性验证主要考察变量在时间序列上的稳定性，衡量的指标有PSI，平均值/方差，IV等。
- 特征工程：主要做特征的预处理和筛选，如果是评分卡，需要对特征进行离散化，归一化等处理，再对特征进行降维，降维的方法有IV筛选，相关性筛选，显著性筛选等。另外会基于对业务的深入理解做特征构造工作，包括特征交叉，特征转换，对特征进行四则运算等。
- 模型建立和评估：选择合适的模型，像评分卡用逻辑回归，只需要做出二分类预测可以选择xgboost等集成模型，模型建好后需要做模型评估，计算AUC,KS，并对模型做交叉验证来评估泛化能力及模型的稳定性。
- 模型上线部署：在风控后台上配置模型规则，对于一些复杂的模型还得需要将模型文件进行转换，并封装成一个类，用Java等其他形式来调用。
- 模型监控：前期主要监控模型整体及变量的稳定性，衡量标准主要是PSI，并每日观察模型规则的拒绝率与线下的差异。后期积累一定线上用户后可评估线上模型的AUC,KS，与线下进行比较，衡量模型的线上的实际效果。

## 49.**评分卡，集成模型在线上是如何部署的？**

- 评分卡的部署较为简单，因为评分卡将变量映射到了一个个区间及得分，所以在普通的风控决策引擎上就可配置。
- 像一些比较复杂的模型，例如xgboost和lightgbm，一般是将模型文件转换为pmml格式，并封装pmml，在风控后台上上传pmml文件和变量参数文件，并配置好模型的阈值。python模型和R模型都可以用这种方式来部署。

## 50.**对于金融场景，稳定胜于一切，那在建模过程中如何保证模型的稳定性？**

- 在数据预处理阶段可以验证变量在时间序列上的稳定性，通过这个方法筛掉稳定性不好的变量，也能达到降维的目的。筛选的手段主要有：计算月IV的差异，观察变量覆盖率的变化，两个时间点的PSI差异等。
- 异常值的检查，剔除噪声，尤其对于逻辑回归这种对于噪声比较敏感的模型。
- 在变量筛选阶段剔除与业务理解相悖的变量，如果是评分卡，可以剔除区分度过强的变量，这种变量一般不适合放入模型中，否则会造成整个模型被这个变量所左右，造成模型的稳定性下降，过拟合的风险也会增加。
- 做交叉验证，一种是时间序列上的交叉验证，考察模型在时间上的稳定性，另一种是K折随机交叉验证，考察模型的随机稳定性。
- 选择稳定性较好的模型，例如随机森林或xgboost这类泛化能力较好的模型。

## 51.**模型转化为规则后决策点（cutoff点）怎么设定？**

- 规则只是判断用户好坏，而不会像模型会输出违约概率，所以设定决策点时需要考虑到规则的评估指标（精准率，查全率，误伤率，拒绝率），一般模型开发前会设定一个预期的拒绝率，在这个拒绝率下再考量精确率，查全率和误伤率的取舍，找到最佳的平衡点。
- 好的模型能接受更多的好用户，拒绝掉更多的坏用户，也就是提高好坏件比例，所以可事先设定一个预期目标的好坏件比例来选择最佳的决策点。

## **52.模型上线后是怎么监控的？**

前期监控（模型上线后一个月内）：

- 模型最后设定cutoff点后可以得出模型的拒绝率（线下拒绝率）, 上线后需要比较模型每日的拒绝率与线下拒绝率。如果两者差异较大，说明线上的用户与建模的用户分布有很大差异，原因可能是没做拒绝推断，或者用户属性随着时间发生了偏移。
- 监控模型整体的稳定性，通常用PSI来衡量两个时间点的差异程度。模型的稳定性是一个需要长期观察的指标，可绘制月/周PSI变化趋势图来分析稳定性的变化，从中可以发现用户是否随着时间推移属性发生了变化，以便及时对模型做出合理的调整。
- 变量稳定度分析，目的是如果模型的稳定性不好，可利用变量稳定度分析来了解是哪些变量造成的。对于不稳定的变量要分析其原因，并对模型做出调整，弃用不稳定的变量或者找其他变量来替换。

后期监控（用户表现出了好坏程度）：

- 此时已积累了一些线上的好坏用户，可做模型的线上效果的评估，评估的指标有AUC, KS, 基尼系数，如果模型的线下效果好，但线上效果却不理想，这个模型是要做优化的。
- 好坏用户的评分分布。绘制线上好坏用户的评分分布图，如果符合期望（高分段好用户占比多，低分段坏用户占比多），则说明模型的线上的区隔能力较好。
- 变量鉴别力分析。用线上的好坏用户来计算变量的IV值，评价变量的预测能力，预测能力不好的变量可以考虑弃用。

## 53.**当模型上线后发现稳定性不佳，或者线上的区分效果不好，你是怎么对模型作调整的？**

- 模型稳定性不佳先检查当初建模时有没有考量过特征的稳定性，在模型前期监控一般会做变量的稳定性分析，如果发现稳定性不佳的变量，考虑弃用或用其他变量替代。另外可以分析下线上用户和建模用户的分布差异，考虑在建模时增加拒绝推断的步骤，让建模样本的分布更加接近于实际整体的申请用户。
- 线上的效果不好可以从变量角度分析，做一下变量鉴别度分析，剔除掉效果不好的变量，挖掘新的变量入模。如果一个模型已上线较长的时间，用户的属性也慢慢发生偏移，建议重新取数做一个新的模型替代旧模型。

## **54.对于高维稀疏特征，或者是弱特征，你是怎么处理的？**

- 对于高维稀疏特征，逻辑回归的效果要比GBDT好。这是由于逻辑回归的正则项是对特征权重的惩罚，以至于特征的权重不至于过大，而树模型的惩罚项主要是深度和叶子节点数目，而对于高维稀疏特征,10000个样本可能9990个值是0，那只需要一个节点就可以划分9990和剩下的10个样本，可见惩罚项之小，所以GBDT对于高维稀疏特征很容易过拟合。平时工作中如果用的是逻辑回归评分卡，则可以对稀疏特征进行离散化，离散成值为0或不为0，再用woe进行编码。而如果使用xgboost等集成模型，最好还是不要用高维的稀疏特征。
- 弱特征指的是与目标变量关系不大的特征，或者是区分能力较弱的特征。在大数据风控中弱特征的种类很多，包括社交，通话，位置等信息，而且建模时弱特征会多达数百个。如果是用评分卡建模，弱特征一般会被舍弃掉，因为评分卡的入模特征数不宜过多，一般在15个以下，所以要找寻比较强的特征。而对于xgboost等模型，本身对数据的要求不是很高，并且精度好，一些弱特征进行交叉组合或许能给模型带来不错的效果。

## 55.**如何根据风险因素对用户分层，构建客群差异化的模型？**

做客群差异化模型之前最好做一下用户画像，在风控领域中做用户画像的目的是：

- 系统性的梳理用户群体，找到异同点对用户进行划分群体，分类的维度很多，可以是静态属性，购买偏好，也可以是褥羊毛党等风险属性。
- 便于更深刻的理解业务，理解用户需求，风控离不开业务，只有深刻理解业务后，才能发现更多潜在的风险。
- 便于后续的数据挖掘，了解坏用户的行为特征，并且根据用户特征做[关联规则](https://zhida.zhihu.com/search?content_id=248046120&content_type=Article&match_order=1&q=关联规则&zhida_source=entity)分析。
- 对不同类型的用户，做针对性的风控规则和风控模型。

平常工作中的做法：

- 对用户做静态属性的划分，比如按性别，年龄，收入，职业等。例如刚毕业工作的年轻人和收入比较稳定的中年人，他们的借款需求，风险程度就不一样，可以先对用户群体做这样的划分，再对每个群体单独建立模型。
- 根据用户风险属性做差异化模型，例如对手机分期业务做一个套现风险模型，挖掘套现风险属性，目标变量变成是否为套现用户。

## 56.**额度，利率的风险定价模型你是如何设计的？**

- 首先做风险定价模型需要熟悉产品的属性和特点，像小额现金贷和大额分期贷两种产品的额度定价逻辑就不同。另外也要了解产品的盈利模式和预期的利润，这点需要与业务部门做好沟通，通常关于额度，利率也是业务或者产品制定的。
- 风险定价模型一般采用评分卡模型，最后设定cutoff点后对通过的用户进行风险等级划分，对于风险高的用户给的额度较低，或者利率较高。一般来说中低额度的用户占大部分，高额度用户占小部分，最后可以得出一个平均额度或利率，这个值事先可以根据预期的利润/资损来计算。

## 57. **风控流程中不同环节的评分卡是怎么设计的？**

- 申请评分A卡用在贷前审核阶段，主要的作用是决定用户是否准入和对用户进行风险定价（确定额度和利率），用到的数据是用户以往的信用历史，多头借贷，消费记录等信息，并且做A卡一般需要做拒绝推断。A卡一般预测用户的首笔借款是否逾期，或者预测一段时间内是否会逾期，设计的方式也多种多样，有风险差异化评分卡，群体差异化评分卡，或者做交叉评分卡等。
- 行为B卡主要用在借贷周期较长的产品上，例如手机分期。作用一是防控贷中风险，二是对用户的额度做一个调整。用到的数据主要是用户在本平台的登录，浏览，消费行为数据，还有借还款，逾期等借贷表现数据。
- 催收C卡主要是对逾期用户做一个画像分析，通过深度挖掘用户特征，对逾期用户进行分群，做智能催收策略等。

---

# 文件：大数据技术\README.md

---

## 背景

随着大数据技术的快速发展，大数据也越来越贴近公司的核心业务，从spark开发到数据仓库的建设，大数据涵盖了方法面面，正是这样庞杂的知识体系，给大数据面试带来了很多困扰点，面试者往往需要收集方方面面的资料进行知识体系的巩固。

笔者结合自己的工作经验，已经面试经验，整理大数据方面的相关知识点，用于面试与复习，也希望这份资料能同样帮助到你。当然受限于笔者的自身水平，如您在阅读过程中发现问题，麻烦联系我，谢谢。

## 笔者联系方式

E-mail:  xiongweinie@foxmail.com

## 大数据技术框架

- Java基础
- Linux基础
- Spark
- Hive
- Hbase
- Hadoop
- Flink
- 数据仓库建设
- 真实面试经验

---

# 文件：开发\Java后端开发.md

---

# Java 基础

## 知识体系

## Questions

### 1. `HashMap` 1.8与1.7的区别

|                |          1.7          |          1.8          |
| :------------: | :-------------------: | :-------------------: |
|    底层结构    |       数组+链表       |   数组+链表/红黑树   |
|    插入方式    |        头插法        |        尾插法        |
|   计算hash值   | 4次位运算+5次异或运算 | 1次位运算+1次异或运算 |
|   扩容、插入   |     先扩容再插入     |     先插入再扩容     |
| 扩容后位置计算 |       重新hash       | 原位置或原位置+旧容量 |

#### (1) 扩容因子默认为什么是0.75

如果扩容因子过高，空间利用率提高但是哈希冲突概率增加；如果扩容因子过低，会造成频繁扩容，哈希冲突概率降低，但是空间利用率变低。选择0.75是基于泊松分布，是时间和空间成本上寻求的一种折中选择

#### (2)为什么链表长度为8要转化为红黑树

首先和`hashcode`碰撞次数的泊松分布有关，主要是为了寻找一种时间和空间的平衡。在负载因子0.75（HashMap默认）的情况下，单个`hash`槽内元素个数为8的概率小于百万分之一，将7作为一个分水岭，等于7时不做转换，大于等于8才转红黑树，小于等于6才转链表。链表中元素个数为8时的概率已经非常小，再多的就更少了，所以原作者在选择链表元素个数时选择了8，是根据概率统计而选择的。红黑树中的`TreeNode`是链表中的`Node`所占空间的2倍，虽然红黑树的查找效率为$O(logN)$，要优于链表的$O(N)$，但是当链表长度比较小的时候，即使全部遍历，时间复杂度也不会太高。所以，要寻找一种时间和空间的平衡，即在链表长度达到一个阈值之后再转换为红黑树

### 2. `String` 、`StringBuffer` 、`StringBuilder` 的区别

|              | 是否可变 | 线程是否安全 | 性能 |
| :-----------: | :------: | :----------: | :--: |
|    String    |  不可变  |     安全     |  低  |
| StringBuilder |   可变   |    不安全    |  高  |
| StringBuffer |   可变   |     安全     | 较高 |

### 3.强引用、软引用、弱引用、虚引用的区别

* **强引用**：如果一个对象具有强引用，垃圾回收器绝不会回收它。当内存空间不足时，JVM 宁愿抛出OOM错误，使程序异常终止；
* **软引用**：如果内存空间足够，垃圾回收器就不会回收它。如果内存空间不足了，就会回收这些对象的内存，只要垃圾回收器没有回收它，该对象就可以被程序使用
* **弱引用**：只具有弱引用的对象拥有更短暂的生命周期。在垃圾回收器线程扫描它所管辖的内存区域的过程中，一旦发现了只具有弱引用的对象，不管当前内存空间足够与否，都会回收它的内存
* **虚引用**：在任何时候都可能被垃圾回收器回收

### 4.`==`和 `equals` 区别

* ==：如果是基本数据类型，比较的是值，如果是引用数据类型，比较的是地址
* `equals`：继承自 `Object` 类，具体实现时可以覆盖父类的实现。如果没有复写，则和 == 一样，比较地址；如果复写了则根据复写的判断方式

### 5.`Object` 类的 `hashCode` 方法的作用

`hashCode `的哈希码主要作用是给散列表快速确定索引的，通过哈希码先判断对象是否有可能相同（因为相同的对象哈希码也一定相同），再去用 equals() 做进一步的比较，这样可以大大减少使用 `equals()`比较的次数

# Java并发

## 知识体系

## Questions

### 1.线性池了解吗？参数有哪些？任务到达线程池的过程？线程池的大小如何设置？

#### 线程池参数

|           参数           |                                                    作用                                                    |
| :----------------------: | :---------------------------------------------------------------------------------------------------------: |
|       corePoolSize       |                                               核心线程池大小                                               |
|     maximumPoolSize     |                                               最大线程池大小                                               |
|      keepAliveTime      | 线程池中超过 corePoolSize 数目的空闲线程最大存活时间；可以allowCoreThreadTimeOut(true) 使得核心线程有效时间 |
|         TimeUnit         |                                           keepAliveTime 时间单位                                           |
|        workQueue        |                                                阻塞任务队列                                                |
|      threadFactory      |                                                新建线程工厂                                                |
| RejectedExecutionHandler |    拒绝策略。当提交任务数超过 maxmumPoolSize+workQueue 之和时，任务会交给RejectedExecutionHandler 来处理    |

#### 线程处理任务过程

* 当线程池小于 `corePoolSize`，新提交任务将创建一个新线程执行任务，即使此时线程池中存在空闲线程
* 当线程池达到 `corePoolSize` 时，新提交任务将被放入 `workQueue ` 中，等待线程池中任务调度执行
* 当 `workQueue` 已满，且 `maximumPoolSize` 大于 `corePoolSize` 时，新提交任务会创建新线程执行任务
* 当提交任务数超过 `maximumPoolSize` 时，新提交任务由 `RejectedExecutionHandler` 处理
* 当线程池中超过 `corePoolSize` 线程，空闲时间达到 `keepAliveTime` 时，关闭空闲线程

#### 线程池的大小设置

* `CPU` 密集型
  
  * `CPU` 密集的意思是该任务需要大量的运算，而没有阻塞，`CPU` 一直全速运行
  * `CPU` 密集型任务尽可能的少的线程数量，一般为 `CPU` 核数 +1 个线程的线程池
* `IO` 密集型
  
  * 由于 `IO` 密集型任务线程并不是一直在执行任务，可以多分配一点线程数，如 `CPU * 2`
  * 也可以使用公式：`CPU` 核数 / (1 - 阻塞系数)；其中阻塞系数在 0.8 ～ 0.9 之间

### 2.`Java`乐观锁机制，`CAS`思想？缺点？是否原子性？

#### `Java`乐观锁机制

乐观锁体现的是悲观锁的反面。它是一种积极的思想，它总是认为数据是不会被修改的，所以是不会对数据上锁的。但是乐观锁在更新的时候会去判断数据是否被更新过。乐观锁的实现方案一般有两种（版本号机制和 `CAS`）。乐观锁适用于读多写少的场景，这样可以提高系统的并发量。在 `Java` 中 `java.util.concurrent.atomic`下的原子变量类就是使用了乐观锁的一种实现方式 `CAS` 实现的。

乐观锁，大多是基于数据版本 (`Version`)记录机制实现。即为数据增加一个版本标识，在基于数据库表的版本解决方案中，一般是通过为数据库表增加一个 `“version”` 字段来 实现。 读取出数据时，将此版本号一同读出，之后更新时，对此版本号加一。此时，将提 交数据的版本数据与数据库表对应记录的当前版本信息进行比对，如果提交的数据 版本号大于数据库表当前版本号，则予以更新，否则认为是过期数据。

#### `CAS`思想

`CAS` 就是 `compare and swap`（比较交换），是一种很出名的无锁的算法，就是可以不使用锁机制实现线程间的同步。使用CAS线程是不会被阻塞的，所以又称为非阻塞同步。`CAS` 算法涉及到三个操作：需要读写内存值 `V`； 进行比较的值 `A`； 准备写入的值 `B`。当且仅当 `V` 的值等于 `A` 的值等于 `V` 的值的时候，才用 `B` 的值去更新 `V` 的值，否则不会执行任何操作（比较和替换是一个原子操作 `A` 和 `V` 比较，`V` 和 `B` 替换），一般情况下是一个自旋操作，即不断重试。缺点：高并发的情况下，很容易发生并发冲突，如果 `CAS` 一直失败，那么就会一直重试，浪费 `CPU` 资源。

#### 原子性

功能限制 `CAS` 是能保证单个变量的操作是原子性的，在 `Java` 中要配合使用 `volatile` 关键字来保证线程的安全；当涉及到多个变量的时候 `CAS` 无能为力；除此之外 `CAS` 实现需要硬件层面的支持，在 `Java` 的普通用户中无法直接使用，只能借助 `atomic` 包下的原子类实现，灵活性受到了限制。

### 3.`ReenTrantLock` 使用方法？底层实现？和 `synchronized` 区别？

由于 `ReentrantLock` 是 `java.util.concurrent` 包下提供的一套互斥锁，相比 `Synchronized`，`ReentrantLock`类提供了一些高级功能，主要有以下三项：

* 等待可中断，持有锁的线程长期不释放的时候，正在等待的线程可以选择放弃等待，这相当于 `Synchronized` 来说可以避免出现死锁的情况。通过 `lock.lockInterruptibly()`来实现这个机制
* 公平锁，多个线程等待同一个锁时，必须按照申请锁的时间顺序获得锁，`Synchronized` 锁非公平锁，`ReentrantLock` 默认的构造函数是创建的非公平锁，可以通过参数 `true` 设为公平锁，但公平锁表现的性能不是很好
* 锁绑定多个条件，一个 `ReentrantLock` 对象可以同时绑定对个对象。`ReenTrantLock` 提供了一个 `Condition`（条件）类，用来实现分组唤醒需要唤醒的线程们，而不是像 `synchronized` 要么随机唤醒一个线程要么唤醒全部线程

#### 使用方法

基于 `API` 层面的互斥锁，需要 `lock()` 和 `unlock()` 方法配合 `try/finally` 语句块来完成

#### 底层实现

` ReenTrantLock` 的实现是一种自旋锁，通过循环调用 `CAS` 操作来实现加锁。它的性能比较好也是因为避免了使线程进入内核态的阻塞状态。想尽办法避免线程进入内核的阻塞状态是我们去分析和理解锁设计的关键钥匙

#### `ReenTrantLock`和`synchronized` 的区别

1、**底层实现**上来说，`synchronized` 是 `JVM` 层面的锁，是 `Java` 关键字，通过 `monitor` 对象来完成（`monitorenter与monitorexit`），对象只有在同步块或同步方法中才能调用 `wait/notify` 方法；`ReentrantLock` 是从 `jdk1.5` 以来（`java.util.concurrent.locks.Lock`）提供的 `API` 层面的锁。`synchronized` 的实现涉及到锁的升级，具体为无锁、偏向锁、自旋锁、向OS申请重量级锁；`ReentrantLock` 实现则是通过利用 `CAS` （`CompareAndSwap`）自旋机制保证线程操作的原子性和 `volatile` 保证数据可见性以实现锁的功能

2、**是否可手动释放：`**synchronized` 不需要用户去手动释放锁，`synchronized` 代码执行完后系统会自动让线程释放对锁的占用； `ReentrantLock` 则需要用户去手动释放锁，如果没有手动释放锁，就可能导致死锁现象。一般通过`lock()` 和 `unlock()` 方法配合 `try/finally` 语句块来完成，使用释放更加灵活

3、**是否可中断** `synchronized` 是不可中断类型的锁，除非加锁的代码中出现异常或正常执行完成； `ReentrantLock` 则可以中断，可通过 `trylock(long timeout,TimeUnit unit)` 设置超时方法或者将 `lockInterruptibly()` 放到代码块中，调用 `interrupt` 方法进行中断。

4、**是否公平锁 **`synchronized` 为非公平锁 `ReentrantLock` 则即可以选公平锁也可以选非公平锁，通过构造方法`new ReentrantLock` 时传入 `boolean` 值进行选择，为空默认 `false` 非公平锁，`true` 为公平锁

### 4.介绍一下 `Java` 的内存模型

`Java` 内存模型（`Java Memory Model，JMM`）就是一种符合内存模型规范的，屏蔽了各种硬件和操作系统的访问差异的，保证了 `Java` 程序在各种平台下对内存的访问都能保证效果一致的机制及规范。`JMM` 是一种规范，是解决由于多线程通过共享内存进行通信时，存在的本地内存数据不一致、编译器会对代码指令重排序、处理器会对代码乱序执行等带来的问题。目的是保证并发编程场景中的原子性、可见性和有序性。所以，`Java` 内存模型，除了定义了一套规范，还提供了一系列原语，封装了底层实现后，供开发者直接使用。我们前面提到，并发编程要解决**原子性、有序性和一致性**的问题。

* **原子性：** 在 `Java` 中，为了保证原子性，提供了两个高级的字节码指令 `Monitorenter` 和 `Monitorexit`。这两个字节码，在 `Java` 中对应的关键字就是 `Synchronized`。因此，在 `Java` 中可以使用 `Synchronized` 来保证方法和代码块内的操作是原子性的
* **可见性：**` Java` 内存模型是通过在变量修改后将新值同步回主内存，在变量读取前从主内存刷新变量值的这种依赖主内存作为传递媒介的方式来实现的。`Java` 中的 `Volatile` 关键字修饰的变量在被修改后可以立即同步到主内存。被其修饰的变量在每次使用之前都从主内存刷新。因此，可以使用 `Volatile` 来保证多线程操作时变量的可见性。除了 `Volatile`，`Java` 中的 `Synchronized` 和 `Final` 两个关键字也可以实现可见性。只不过实现方式不同
* **有序性：**在 `Java` 中，可以使用 `Synchronized` 和 `Volatile` 来保证多线程之间操作的有序性。区别：`Volatile` 禁止指令重排。`Synchronized` 保证同一时刻只允许一条线程操作

### 5.`volatile` 作用？底层实现？单例模式中 `volatile` 的作用？

#### 作用

保证数据的“可见性”：被 `volatile` 修饰的变量能够保证每个线程能够获取该变量的最新值，从而避免出现数据脏读的现象。 禁止指令重排：在多线程操作情况下，指令重排会导致计算结果不一致。

#### 底层实现

观察加入 `volatile` 关键字和没有加入 `volatile` 关键字时所生成的汇编代码发现，加入 `volatile` 关键字时，会多出一个 `lock` 前缀指令， `lock` 前缀指令实际上相当于一个内存屏障（也成内存栅栏），内存屏障会提供3个功能：

* 它确保指令重排序时不会把其后面的指令排到内存屏障之前的位置，也不会把前面的指令排到内存屏障的后面；即在执行到内存屏障这句指令时，在它前面的操作已经全部完成
* 它会强制将对缓存的修改操作立即写入主存
* 如果是写操作，它会导致其他 `CPU` 中对应的缓存行无效

#### 单例模式中 `volatile` 的作用

防止代码读取到 `instance` 不为 `null` 时，`instance` 引用的对象有可能还没有完成初始化

```java
class Singleton{
    private volatile static Singleton instance = null;   //禁止指令重排
    private Singleton() {      
    }
    public static Singleton getInstance() {
        if(instance==null) {
            synchronized (Singleton.class) {
                if(instance==null)
                    instance = new Singleton();
            }
        }
        return instance;
    }
}
```

### 6.`ThreadLocal` 原理

### 7.`CAS` 和 `ABA` 问题

### 8.原子类的实现原理

### 9.说一下 `CurrentHashMap` 如何实现线程安全的

# JVM

## 知识体系

## Questions

### 1.`JVM` 内存划分

### 2.`GC` 垃圾收集器

### 3.垃圾收集算法，为什么新生代用标记-复制老年代用标记-整理

### 4.类加载流程

### 5.双亲委派机制，怎么样会打破双亲委派模型

### 6.`JVM` 1.7 和 1.8 的区别

# 数据库

## 知识体系

## Questions

### 1.`MySQL`的引擎了解吗？默认的是哪个？`InnoDB `和 `myISAM` 的区别？

`myISAM`:  支持表锁，适合读密集的场景，不支持外键，不支持事务，索引与数据在不同的文件，非聚簇索引

`InnoDB`:支持行、表锁，默认为行锁，适合并发场景，支持外键，支持事务，索引与数据同一文件，聚簇索引

### 2.介绍 `MVCC`

`MVCC` 是一种多版本并发控制机制，在大多数情况下代替行级锁，使用 `MVCC`，能降低其系统开销。`MVCC` 是通过保存数据在某个时间点的快照来实现的.。不同存储引擎的 `MVCC` 实现是不同的，典型的有乐观并发控制和悲观并发控制。`InnoDB` 的 `MVCC` 是通过在每行记录后面保存两个隐藏的列来实现的，这两个列，分别保存了这个行的创建时间，一个保存的是行的删除时间。这里存储的并不是实际的时间值，而是系统版本号(可以理解为事务的 `ID`)，每开始一个新的事务，系统版本号就会自动递增，事务开始时刻的系统版本号会作为事务的 `ID`。`InnoDB` 只会查找版本早于当前事务版本的数据行(也就是行的系统版本号小于或等于事务的系统版本号)，这样可以确保事务读取的行，要么是在事务开始前已经存在的，要么是事务自身插入或者修改过的。

### 3.`MySQL` 中一条 `SQL` 语句的执行过程

以查询语句为例

```mysql
select * from tb_student  A where A.age='18' and A.name='张三';
```

* 先检查该语句是否有权限，如果没有权限，直接返回错误信息，如果有权限，在 `MySQL8.0` 版本以前，会先查询缓存，以这条 `sql` 语句为 `key` 在内存中查询是否有结果，如果有直接缓存，如果没有，执行下一步
* 通过分析器进行词法分析，提取 `sql` 语句的关键元素，比如提取上面这个语句是查询 `select`，提取需要查询的表名为 `tb_student`,需要查询所有的列，查询条件是这个表的 `id='1'`。然后判断这个 `sql` 语句是否有语法错误，比如关键词是否正确等等，如果检查没问题就执行下一步
* 接下来就是优化器进行确定执行方案，上面的 `sql` 语句，可以有两种执行方案： a.先查询学生表中姓名为“张三”的学生，然后判断是否年龄是18。 b.先找出学生中年龄18岁的学生，然后再查询姓名为“张三”的学生。 那么优化器根据自己的优化算法进行选择执行效率最好的一个方案（优化器认为，有时候不一定最好）。那么确认了执行计划后就准备开始执行了
* 进行权限校验，如果没有权限就会返回错误信息，如果有权限就会调用数据库引擎接口，返回引擎的执行结果

### 4.如何查看 `sql` 语句的执行计划？用哪个关键字？使用这个关键字可以得到哪些信息？

用关键字 `explain` 来查看执行计划

![](https://i.loli.net/2021/07/01/xPIajcdQrysEJON.png)

![](https://i.loli.net/2021/07/01/k8TmrSEaK4M1lpc.png)

### 5.聚簇索引和非聚簇索引的区别，非聚簇索引是如何查询的？

聚簇索引就是按照每张表的主键构造一颗B+树，同时叶子节点中存放的就是整张表的行记录数据，也将聚集索引的叶子节点称为数据页。这个特性决定了索引组织表中数据也是索引的一部分，每张表只能拥有一个聚簇索引。`InnoDB` 通过主键聚集数据，如果没有定义主键，`InnoDB` 会选择非空的唯一索引代替。如果没有这样的索引，`InnoDB` 会隐式的定义一个主键来作为聚簇索引。

在聚簇索引之上创建的索引称之为辅助索引，辅助索引访问数据总是需要二次查找。辅助索引叶子节点存储的不再是行的物理位置，而是主键值。通过辅助索引首先找到的是主键值，再通过主键值找到数据行的数据页，再通过数据页中的 `Page Directory`找到数据行。`InnoDB` 辅助索引的叶子节点并不包含行记录的全部数据，叶子节点除了包含键值外，还包含了相应行数据的聚簇索引键。辅助索引的存在不影响数据在聚簇索引中的组织，所以一张表可以有多个辅助索引。在 `InnoDB` 中有时也称辅助索引为二级索引。

二级索引需要两次索引查找， 二级索引中保存的“行指针”的本质：不是物理地址的指针，而是行的主键值。所以通过二级索引查找行，引擎需要找到二级索引的子节点获得对应主键值，然后根据该值去聚簇索引找到对应行。 出现重复工作：两次B-Tree查找，而非一次。对于InnoDB，自适应哈希索引能够减少这样重复。

### 6.`MySQL`的 `ACID`，分别解释一下

**原⼦性：** 事务是最⼩的执⾏单位，不允许分割。事务的原⼦性确保动作要么全部完成，要么全不执行

**一致性：** 执⾏事务前后，数据保持⼀致，多个事务对同⼀个数据读取的结果是相同的

**隔离性：** 并发访问数据库时，⼀个⽤户的事务不被其他事务所⼲扰，各并发事务之间数据库是独⽴的

**持久性：** ⼀个事务被提交之后。它对数据库中数据的改变是持久的，即使数据库发⽣故障也不应该对其有任何影响

### 7.数据库索引的实现原理

`MySQL` 中默认的引擎为 `InnoDB`，`innoDB` 的索引使用的是 `B+` 树实现

**`B+` 树对比 `B树` 的好处**

* `IO` 次数少：`B+` 树的中间结点只存放索引，数据都存在叶结点中，因此中间结点可以存更多的数据，让索引树更加矮胖
* 范围查询效率更高：`B` 树需要中序遍历整个树，`B+` 树只需要遍历叶结点中的链表
* 查询效率更加稳定：每次查询都需要从根结点到叶结点，路径长度相同，所以每次查询的效率都差不多

**使用 `B` 树索引和哈希索引的比较**

哈希索引能以 `O(1)` 时间进行查找，但是只支持精确查找，无法用于部分查找和范围查找，无法用于排序与分组；`B` 树索引支持大于小于等于查找，范围查找。哈希索引遇到大量哈希值相等的情况后查找效率会降低。哈希索引不支持数据的排序

### 8.联合索引，最左前缀匹配规则

联合索引又叫复合索引。两个或更多个列上的索引被称作复合索引。对于复合索引：`MySQL` 从左到右的使用索引中的字段，一个查询可以只使用索引中的一部份，但只能是最左侧部分。当最左侧字段是常量引用时，索引就十分有效

**优点：**

* 避免回表
* 两个单列查询返回行较多，同时查返回行较少，联合索引更高效

**如何设计：**

* 等值查询中，查询条件a返回的条目比较多，查询条件b返回的条目比较多，而同时查询a、b返回的条目比较少，那么适合建立联合索引
* 对于有等值查询的列和范围查询的列，等值查询的列建在前、范围查询的列建在后比较实用
* 如果联合索引列的前置列与索引单列一致，那么单列查询可以用到索引，这样就避免了再建单列索引，因此联合索引的前置列应尽量与单列一致

**最左前缀匹配原则：**在MySQL建立联合索引时会遵守最左前缀匹配原则，即最左优先，在检索数据时从联合索引的最左边开始匹配

### 9.索引怎么优化

* `like` 语句的前导模糊查询不能使用索引
* **联合索引最左前缀原则**
* 不能使用索引中范围条件右边的列(范围列可以用到索引)，范围列之后列的索引全失效
* 不要在索引列上面做任何操作(计算、函数)，否则会导致索引失效而转向全表扫描
* 使用短索引，如果对长字符串列进行索引，应该指定一个前缀长度，这样能够节省大量索引空间
* 常查询数据建立索引或者组合索引，不要建立无意义的索引

### 10.事物隔离级别

- 读未提交（read uncommitted，RU） 一个事务还没提交，它的变更就能被其它事务看到
- 读已提交（read committed，RC） 一个事务提交后，其变更才会被其他事务看到
- 可重复读（repeatable read，RR） 一个事务执行过程中看到的数据，和该事务在启动时看到的数据一致。 自然未提交的变更对其他事务也是不可见的。一个事务启动时，能够看到所有已提交的事务结果。但之后的该事务执行期间，其他事务的更新对它就不可见了
- 串行化（serializable） 对同行记录，“写”加“写锁”，“读”加“读锁”。出现读写锁冲突时，后访问的事务必须等前一个事务执行完成

### 11.`binlog`、`redo log`、`undo log`

#### `binlog`

`binlog`用于记录数据库执行的写入性操作(不包括查询)信息，以二进制的形式保存在磁盘中。`binlog`是`mysql`的逻辑日志，并且由`Server`层进行记录，使用任何存储引擎的`mysql`数据库都会记录`binlog`日志

**使用场景：**

1. **主从复制**：在`Master`端开启`binlog`，然后将`binlog`发送到各个`Slave`端，`Slave`端重放`binlog`从而达到主从数据一致
2. **数据恢复**：通过使用`mysqlbinlog`工具来恢复数据

#### `redo log`

`redo log`包括两部分：一个是内存中的日志缓冲(`redo log buffer`)，另一个是磁盘上的日志文件(`redo log file`)。`mysql`每执行一条`DML`语句，先将记录写入`redo log buffer`，后续某个时间点再一次性将多个操作记录写到`redo log file`。这种**先写日志，再写磁盘**的技术就是`MySQL`里经常说到的`WAL(Write-Ahead Logging)` 技术

**区别**

|          | redo log                                                            | binlog                                                                          |
| -------- | ------------------------------------------------------------------- | ------------------------------------------------------------------------------- |
| 文件大小 | `redo log`的大小是固定的。                                          | `binlog`可通过配置参数`max_binlog_size`设置每个`binlog`文件的大小。             |
| 实现方式 | `redo log`是`InnoDB`引擎层实现的，并不是所有引擎都有。              | `binlog`是`Server`层实现的，所有引擎都可以使用 `binlog`日志                     |
| 记录方式 | redo log 采用循环写的方式记录，当写到结尾时，会回到开头循环写日志。 | binlog 通过追加的方式记录，当文件大小大于给定值后，后续的日志会记录到新的文件上 |
| 适用场景 | `redo log`适用于崩溃恢复(crash-safe)                                | `binlog`适用于主从复制和数据恢复                                                |

#### `undo log`

`undo log` 主要有两个作用：`回滚`和`多版本控制(MVCC)` 。在数据修改的时候，不仅记录了 `redo log`，还记录 `undo log`，如果因为某些原因导致事务失败或回滚了，可以用 `undo log` 进行回滚（保证了原子性） 。`undo log` 主要存储的是`逻辑日志`，用来回滚的`相反操作日志`。比如我们要 `insert` 一条数据了，那 `undo log` 会记录的一条对应的 `delete` 日志。我们要 `update` 一条记录时，它会记录一条对应相反的 `update` 记录。因为 `undo log` 存储着修改之前的数据，相当于一个前版本，``MVCC` 实现的是读写不阻塞，读的时候只要返回前一个版本的数据就行了

# Spring

## 知识体系

## Questions

### 1.`Sping IOC AOP` 的实现原理

### 2.`Spring` 事务的实现原理

### 3.`bean` 的生命周期

# Redis

## 知识体系

## Questions

### 1.基于 `redis` 的分布式锁是如何实现的

![](https://i.loli.net/2021/07/01/WwxOC1ULHkAdIfF.png)

**实现思想**：获取锁的时候，使用 `setnc` 加锁，并使用 `expire` 命令为锁添加一个超时时间，超过该时间则自动释放锁，锁的 `value` 值为一个随机生成的 `UUID`，通过此在释放锁的时候进行判断；获取锁的时候还设置一个获取的超时时间，若超过这个时间则放弃获取锁；释放锁的时候，通过 `UUID` 判断是不是该锁，若是该锁，则执行 `delete` 进行锁释放。因为 `redis` 是单线程的，这样的话，如果4个请求打过来的话，只有一个请求能获得锁，获得锁即为 `key` 设置一个值，如果这个 `key` 存在就设置失败，也就是只有一个请求可以设置成功，这个请求处理完之后，会把这个 `key` 释放掉，那么剩下等着的3个请求一定会有一个请求重新对这个 `key` 设置一个值再次获得锁，依次类推。这样即当不同机子上的请求打过来的时候能够保证某一时刻只能有一个请求去消费资源，间接地形成一种加锁的机制。

#### `Redisson` 实现 `Redis` 分布式锁

* 加锁机制：客户端1面对分布式集群下，首先根据 `hash` 节点选择一台机器，发送一段 `lua` 脚本（保证复杂业务的原子性），将 `key `加锁（如果 `key `不存在就进行加锁）、设置过期时间、客户端 `id`
* 锁互斥机制：如果客户端2执行同样的lua脚本，代码中则会判断该锁 `key` 已存在，紧接着判断锁 `key` 的 `hash` 数据结构中是否包含客户端2的 `id`，如果不包含则会获得一个返回的数字，代表这个锁 `key` 的剩余生存时间，紧接着客户端2会进入 `while` 循环不断尝试加锁
* `watch dog` 自动延期机制：客户端1加锁有默认生存时间，如果想继续持有，可以在加锁成功时启动一个 `watch dog`看门狗（后台线程），每10秒检查一下，如果客户端1还持有锁 `key`，就会不断的延长锁 `key` 的生存时间。**(是对1中如何设置有效期的优化)**
* 可重入加锁机制： `hash` 数据结构中的客户端 `id` 加锁次数+1**(是对锁只能加一次，不可重入的优化)**
* 释放锁机制：每次对数据结构中的加锁次数-1，当加锁次数为0，说明该客户端不持有锁，此时会从 `redis` 里删除这个 `key` ，另外的客户端可以尝试完成加锁。

**缺点**：如果采用这种方案，对某个 `redis master` 实例，写入 `mylock` 这种锁的 `value`，此时异步复制给对应的 `master slave` 实例，这个过程中一旦 `redis master` 宕机，`redis slave` 就变成了 `redis master` 会导致客户端2尝试加锁时，在新的`redis master `上完成了加锁，而客户端1也以为自己成功加了锁。导致多个客户端对一个分布式锁完成了加锁，导致各种脏数据产生。【`redis` 主从架构的主从异步复制导致 `redis` 分布式锁的最大缺陷】

### 2.跳表数据结构，`redis` 中哪里用到了跳表

跳跃表（[skiplist](http://en.wikipedia.org/wiki/Skip_list)）是一种随机化的数据， 由 William Pugh 在论文[《Skip lists: a probabilistic alternative to balanced trees》](http://www.cl.cam.ac.uk/teaching/0506/Algorithms/skiplists.pdf)中提出， 跳跃表以有序的方式在层次化的链表中保存元素， 效率和平衡树媲美 —— 查找、删除、添加等操作都可以在对数期望时间下完成， 并且比起平衡树来说， 跳跃表的实现要简单直观得多

![ilBuYTAOrxfEdSm](https://i.loli.net/2021/08/21/ilBuYTAOrxfEdSm.jpg)

跳跃表主要由以下部分构成：

- 表头（head）：负责维护跳跃表的节点指针
- 跳跃表节点：保存着元素值，以及多个层
- 层：保存着指向其他元素的指针。高层的指针越过的元素数量大于等于低层的指针，为了提高查找的效率，程序总是从高层先开始访问，然后随着元素值范围的缩小，慢慢降低层次
- 表尾：全部由 `NULL` 组成，表示跳跃表的末尾

`redis` 数据类型 `zset` 实现有序集合，底层使用的数据结构是跳表

### 3.出现缓存雪崩、击穿、穿透的情况及解决方法

#### 缓存雪崩

**缓存雪崩**指缓存同一时间大面积的失效，所以，后面的请求都会落到数据库上，造成数据库短时间内承受大量请求而崩掉。

**解决方案：**

- **Redis 高可用**，主从+哨兵，`Redis cluster`，避免全盘崩溃
- 本地 ehcache 缓存 + hystrix **限流&降级**，避免 `MySQL` 被打死
- 缓存数据的**过期时间设置随机**，防止同一时间大量数据过期现象发生
- **逻辑上永不过期**给每一个缓存数据增加相应的**缓存标记**，缓存标记失效则更新数据缓存
- **多级缓存**，失效时通过二级更新一级，由第三方插件更新二级缓存

#### **缓存穿透**

**缓存穿透**是指缓存和数据库中都没有的数据，导致所有的请求都落到数据库上，造成数据库短时间内承受大量请求而崩掉。

**解决方案：**

* **接口层增加校验**，如用户鉴权校验，id 做基础校验，id<=0 的直接拦截
* 从缓存取不到的数据，在数据库中也没有取到，这时也可以将**key-value 对写为 key-null**，缓存有效时间可以设置短点，如30秒。这样可以防止攻击用户反复用同一个id暴力攻击
* 采用**布隆过滤器**，将所有可能存在的数据哈希到一个足够大的 `bitmap` 中，一个一定不存在的数据会被这个 `bitmap` 拦截掉，从而避免了对底层存储系统的查询压力

#### **缓存击穿**

由于并发用户特别多，同时读缓存没读到数据，又同时去数据库去取数据，引起数据库压力瞬间增大，造成过大压力。和缓存雪崩不同的是，缓存击穿指并发查同一条数据，缓存雪崩是不同数据都过期了，很多数据都查不到从而查数据库

**解决方案：**

* 设置**热点数据永远不过期**，异步线程处理
* 加**写回操作加互斥锁**，查询失败默认值快速返回
* 缓存预热

```
系统上线后，将相关**可预期（例如排行榜）**热点数据直接加载到缓存
```

```
写一个缓存刷新页面，手动操作热点数据**（例如广告推广）**上下线
```

### 4.持久化策略

#### `RDB`

`RDB` 持久化是指在指定时间间隔内将内存中的数据集快照写入磁盘。实际上 `fork` 子线程，先将数据集写入临时文件，写入成功后，在替换之前的文件，用二进制压缩文件，`RDB` 是 `Redis` 默认的持久化方式，会在对应目录下生产一个`dump.rdb` 文件，重启会通过加载 `dump.rdb` 文件恢复数据

**优点**

- `RDB` 会生成多个数据文件，每个文件都代表了某时刻 `redis` 中的所有数据，这种方式非常适合做**冷备**，可将这种完整数据文件发送到云服务器存储，比如 `ODPS` 分布式存储，以预定好的备份策略来定期备份 `redis` 中的数据
- `RDB` 对 `Redis` 对外提供的读写服务，影响非常小，可让 `redis` 保持高性能，因为 `redis` 主进程只要 `fork` 一个子进程，让子进程执行 `RDB`
- 相对于 `AOF`，直接基于 `RDB` 文件重启和恢复 `redis`进程，更加快速

**缺点**

- `fork` 耗内存，`copy-on-write` 策略 `RDB `每次在 `fork` 子进程来执行 `RDB` 快照数据文件生成的时候，如果数据文件特别大，可能会导致对客户端提供的服务暂停数毫秒，或者甚至数秒
- 不可控，容易丢失数据 一般 `RDB` 每隔5分钟，或者更长时间生成一次，若过程中 `redis` 宕机，就会丢失最近未持久化的数据

#### `AOF`

`AOF`持久化是以日志的形式记录记录每一个增删操作然后追加到文件中。`AOF`的出现是为了弥补`RDB`备份的不足（数据不一致性）

**AOF的备份策略**

- `appendfsync always`：每次有数据修改发生时都会同步
- `appendfsync everysec`：每秒同步一次
- `appendsync no`：让操作系统决定何时进行同步

**优点**

- 更好避免数据丢失 一般`AOF`每隔1s，通过子进程执行一次`fsync`，最多丢1s数据
- `append-only`模式追加写 所以没有任何磁盘寻址的开销，写入性能高，且文件不易破损，即使文件尾部破损，也易修复
- 日志文件即使过大，出现后台重写操作，也不会影响客户端的读写 因为在`rewrite log`时，会压缩其中的指令，创建出一份需要恢复数据的最小日志。在创建新日志时，旧日志文件还是照常写入。当新的`merge`后的日志文件准备好时，再交换新旧日志文件即可

**缺点**

- 对于同一份数据，`AOF`日志一般比`RDB`快照更大，恢复慢
- `AOF`开启后，写`QPS`会比`RDB`的低，因为`AOF`一般会配置成每s `fsync`一次日志文件，当然，每s一次`fsync`，性能也还是很高的

### 5.主从复制原理

主从复制，是指将一台`redis`服务器的数据，复制到其他的`redis`服务器。前者称为主节点(`master`)，后者称为从节点(`slave`)。数据的复制是单向的，只能由主节点到从节点

**作用**

* 数据冗余：主从复制实现了数据的热备份，是持久化之外的一种数据冗余方式
* 故障恢复：当主节点出现问题时，可以由从节点提供服务，实现快速的故障恢复；实际上是一种服务的冗余
* 负载均衡：在主从复制的基础上，配合读写分离，可以由主节点提供写服务，由从节点提供读服务（即写`redis`数据时应用连接主节点，读`redis`数据时应用连接从节点），分担服务器负载；尤其是在写少读多的场景下，通过多个从节点分担读负载，可以大大提高`redis`服务器的并发量
* 高可用基石：除了上述作用以外，主从复制还是哨兵和集群能够实施的基础，因此说主从复制是`redis`高可用的基础

**原理**

* 通过从服务器发送到`PSYNC`命令给主服务器
* 如果是首次连接，触发一次**全量复制**。此时主节点会启动一个后台线程，生成 `RDB` 快照文件
* 主节点会将这个 `RDB` 发送给从节点，`slave` 会先写入本地磁盘，再从本地磁盘加载到内存中
* `master`会将此过程中的写命令写入缓存，从节点**实时同步**这些数据
* 如果网络断开了连接，自动重连后主节点通过命令传播**增量复制**给从节点部分缺少的数据

### 6.数据库缓存一致性

#### **先更新数据库，再更新缓存**

**线程安全**：同时有请求A和请求B进行更新操作

> 1. 线程A更新了数据库
> 2. 线程B更新了数据库
> 3. 线程B更新了缓存
> 4. 线程A更新了缓存

这就出现请求A更新缓存应该比请求B更新缓存早才对，但是因为网络等原因，B却比A更早更新了缓存。这就导致了脏数据，因此不考虑

**业务场景**：（1）如果你是一个写数据库场景比较多，而读数据场景比较少的业务需求，采用这种方案就会导致，数据压根还没读到，缓存就被频繁的更新，浪费性能；（2）如果你写入数据库的值，并不是直接写入缓存的，而是要经过一系列复杂的计算再写入缓存。那么，每次写入数据库后，都再次计算写入缓存的值，无疑是浪费性能的。显然，删除缓存更为适合

#### **先删除缓存，再更新数据库**

> 1. 请求A进行写操作，删除缓存
> 2. 请求B查询发现缓存不存在
> 3. 请求B去数据库查询得到旧值
> 4. 请求B将旧值写入缓存
> 5. 请求A将新值写入数据库

上述情况就会导致不一致的情形出现。而且，如果不采用给缓存设置过期时间策略，该数据永远都是脏数据。

#### **先更新数据库，再删除缓存**

> 1. 请求缓存刚好失效
> 2. 请求A查询数据库，得一个旧值
> 3. 请求B将新值写入数据库
> 4. 请求B删除缓存
> 5. 请求A将查到的旧值写入缓存

这样就出现脏数据了，然而，实际上出现的概率可能非常低，因为这个条件需要发生在读缓存时缓存失效，而且并发着有一个写操作。而实际上数据库的写操作会比读操作慢得多，而且还要锁表，而读操作必需在写操作前进入数据库操作，而又要晚于写操作删除缓存，所有的这些条件都具备的概率基本并不大，但是还是会有出现的概率。并且假如**第一步写数据库成功，第二步删除缓存失败，这样也导致脏数据**

#### **延时双删**

> 1. 先删除(淘汰)缓存
> 2. 再写数据库（这两步和原来一样）
> 3. 休眠1秒，再次删除(淘汰)缓存

或者

> 1. 先写数据库
> 2. 再删除(淘汰)缓存（这两步和原来一样）
> 3. 休眠1秒，再次删除(淘汰)缓存

这个1秒应该看你的业务场景，应该自行评估自己的项目的读数据业务逻辑的耗时，然后写数据的休眠时间则在读数据业务逻辑的耗时基础上，加几百ms即可，这么做确保读请求结束，写请求可以删除读请求造成的缓存脏数据

**为了性能更快，可以把第二次删除缓存可以做成异步的，这样不会阻塞请求了，如果再严谨点，防止第二次删除缓存失败，这个异步删除缓存可以加上重试机制，失败一直重试，直到成功**

**方案一**

> 1. 更新数据库数据
> 2. 缓存因为种种问题删除失败
> 3. 将需要删除的key发送至消息队列
> 4. 自己消费消息，获得需要删除的key
> 5. 继续重试删除操作，直到成功

![9rzmXuUeYCcOGQp](https://i.loli.net/2021/08/23/9rzmXuUeYCcOGQp.jpg)

该方案有一个缺点，对业务线代码造成大量的侵入，于是有了方案二，启动一个订阅程序去订阅数据库的Binlog，获得需要操作的数据。在应用程序中，另起一段程序，获得这个订阅程序传来的信息，进行删除缓存操作

**方案二**

> 1. 更新数据库数据
> 2. 数据库会将操作信息写入binlog日志当中
> 3. 订阅程序提取出所需要的数据以及key
> 4. 另起一段非业务代码，获得该信息
> 5. 尝试删除缓存操作，发现删除失败
> 6. 将这些信息发送至消息队列
> 7. 重新从消息队列中获得该数据，重试操作

![WcMTS4zq9A7E8sD](https://i.loli.net/2021/08/23/WcMTS4zq9A7E8sD.jpg)

### 7.`redis` 的内存淘汰策略

![](https://tva1.sinaimg.cn/large/008i3skNly1gtbx2jwtfnj61ct0l9dhm02.jpg)

* `volatile-ttl` 在筛选时会针对设置了过期时间的键值对，根据过期时间的先后进行删除，越早过期的越先被删除
* `volatiile-random` 在筛选时对设置了过期时间的键值对进行随机删除
* `volatile-lru` 使用 `lru` 算法筛选设置了过期时间的键值对
* `volatile-lfu` 使用 `lfu` 算法筛选设置了过期时间的键值对
* 优先使用 `allkeys-lru` 可以充分利用 `lru` 这一经典算法的优势，把最近最常访问的数据留在缓存中，提升应用的访问性能
* 如果业务中有置顶的需求，可以使用 `volatile-lru` 策略同时不给这些置顶数据设置过期时间

### 8.`redis` 的 `key` 的过期策略

* **定时过期**：每个设置过期时间的`key`都需要创建一个定时器，到过期时间就会立即清除。该策略可以立即清除过期的数据，对内存很友好；但是会占用大量的`CPU`资源去处理过期的数据，从而影响缓存的响应时间和吞吐量
* **惰性过期**：只有当访问一个`key`时，才会判断该`key`是否已过期，过期则清除。该策略可以最大化地节省`CPU`资源，却对内存非常不友好。极端情况可能出现大量的过期`key`没有再次被访问，从而不会被清除，占用大量内存
* **定期过期**：每隔一定的时间，会扫描一定数量的数据库的`expires`字典中一定数量的`key`，并清除其中已过期的`key`。该策略是前两者的一个折中方案。通过调整定时扫描的时间间隔和每次扫描的限定耗时，可以在不同情况下使得CPU和内存资源达到最优的平衡效果

### 9.`redis` 是单线程的吗，为什么这么快

**单线程**

- 没有了多线程上下文切换的性能损耗
- 没有了访问共享资源加锁的性能损耗
- 开发和调试非常友好，可维护性高

**纯内存操作**

`redis`是一个内存数据库，它的数据都存储在内存中，这意味着我们读写数据都是在内存中完成，这个速度是非常快的。`redis`是一个KV内存数据库，它内部构建了一个哈希表，根据指定的`KEY`访问时，只需要`O(1)`的时间复杂度就可以找到对应的数据。同时，`redis`提供了丰富的数据类型，并使用高效的操作方式进行操作，这些操作都在内存中进行，并不会大量消耗`CPU`资源，所以速度极快

**IO 多路复用技术**

`redis` 服务采用 `Reactor` 的方式来实现文件事件处理器（每一个网络连接其实都对应一个文件描述符）。文件事件处理器使用 I/O 多路复用模块同时监听多个 FD，当 `accept`、`read`、`write` 和 `close` 文件事件产生时，文件事件处理器就会回调 FD 绑定的事件处理器。虽然整个文件事件处理器是在单线程上运行的，但是通过 I/O 多路复用模块的引入，实现了同时对多个 FD 读写的监控，提高了网络通信模型的性能，同时也可以保证整个 `redis` 服务实现的简单

### 参考

* https://blog.csdn.net/Liu_Wd/article/details/108052428
* https://zhuanlan.zhihu.com/p/366972218
* https://houbb.github.io/2019/01/02/db-index-07-combine-index
* https://juejin.cn/post/6860252224930070536
* https://redisbook.readthedocs.io/en/latest/internal-datastruct/skiplist.html
* https://www.jianshu.com/p/53f0b27dc7e1
* https://sourcegraph.com/github.com/Wasabi1234/Java-Interview-Tutorial/-/blob/%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/Redis/Redis%E6%8C%81%E4%B9%85%E5%8C%96.md
* https://www.cnblogs.com/kismetv/p/9236731.html
* https://java.isture.com/redis/question/Redis%E4%BF%9D%E8%AF%81%E7%BC%93%E5%AD%98%E4%B8%8E%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8F%8C%E5%86%99%E6%97%B6%E7%9A%84%E6%95%B0%E6%8D%AE%E4%B8%80%E8%87%B4%E6%80%A7.html
* https://www.jianshu.com/p/8aa619933ebb

---

# 文件：开发\前端开发\【1】javascript.md

---

# 前端开发校招面试问题整理【1】——JavaScript

## 1、JavaScript 基础

### Q：介绍 js 的基本数据类型？

基本类型（值类型）：String，Number，Boolean，Null，Undefined，Symbol，BigInt。

数字类型包含整数与浮点数，整数精度为 2^53，浮点数值最高精度为 17 位小数。
另外注意的是，NAN 属于数字类型，代表非数字，即 1/0=NAN，NAN/1=NAN。
Infinity 代表超过了范围的数，有正负号。

undefined：表示变量已声明，但不含有值。
null：表示变量为空。

引用类型：对象，数组，函数（其实都是对象，都是一种引用）。

数据封装类对象：Object、Array、Boolean、Number、String
其他对象：Function、Arguments、Math、Date、RegExp、Error

基本类型被保存在栈内存中，引用类型被保存在堆内存中

### Q：js 中如何判断一个对象是什么类型？

使用 typeof 可以判断一个 javascript 基本对象的类型。

```javascript
typeof 'John' // 返回 string
typeof 3.14 // 返回 number
typeof NaN // 返回 number
typeof false // 返回 boolean
typeof [1, 2, 3, 4] // 返回 object
typeof { name: 'John', age: 34 } // 返回 object
typeof new Date() // 返回 object
typeof function () {} // 返回 function
typeof myCar // 返回 undefined (如果 myCar 没有声明)
typeof null // 返回 object
```

使用 constructor 可以返回一个构造器的类型，可以根据构造器名称来判断类型。

```javascript
"John".constructor // 返回函数 String() {[native code]}
(3.14).constructor // 返回函数 Number() {[native code]}
false.constructor // 返回函数 Boolean() {[native code]}
[1,2,3,4].constructor // 返回函数 Array() {[native code]}
{name:'John', age:34}.constructor // 返回函数 Object() {[native code]}
new Date().constructor // 返回函数 Date() {[native code]}
function () {}.constructor // 返回函数 Function() {[native code]}
```

使用 instanceof 可以检测引用类型，如：

```javascript
console.log(person instanceof Object) // 变量 person 是 Object 吗？
console.log(colors instanceof Array) // 变量 colors 是 Array 吗？
console.log(pattern instanceof RegExp) // 变量 pattern 是 RegExp 吗？
```

### Q：js 的原型链你是如何理解的？

![原型链](http://www.shadowingszy.top/images/prototype.png)

只要是对象就有原型, 并且原型也是对象, 因此只要定义了一个对象, 那么就可以找到他的原型, 如此反复, 就可以构成一个对象的序列, 这个结构就被成为原型链。

每个对象都会在其内部初始化一个属性，就是 prototype(原型)，当我们访问一个对象的属性时，如果这个对象内部不存在这个属性，那么他就会去 prototype 里找这个属性，这个 prototype 又会有自己的 prototype，于是就这样一直找下去，也就是我们平时所说的原型链的概念。

关系：instance.constructor.prototype = instance.proto

### Q：js 中 function.call 和 function.apply 的区别？

```javascript
function.apply(thisObj, [argArray])
function.call(thisObj, arg1, arg2, …, argN);
```

apply：调用一个对象的一个方法，用另一个对象替换当前对象。例如：B.apply(A, arguments);即 A 对象应用 B 函数，即将 B 上下文从初始的上下文改变为 A 的上下文。

call：调用一个对象的一个方法，用另一个对象替换当前对象。例如：B.call(A, args1,args2);即 A 对象调用 B 对象的方法。

### Q：js 中作用域你是如何理解的？

全局作用域：写在最外层的对象，或者写在内层但是没有用 var 声明的变量，作用域为全局，例如：

```javascript
var outerVar = "outer";
function fn() {
  innerVar = “inner”;
  console.log(outerVar);
}
fn(); //result:outer
console.log(innerVar); //result:inner
```

函数作用域：写在函数内层使用 var 声明的变量，例如：

```javascript
function fn() {
  var innerVar = 'inner'
}
console.log(innerVar) // ReferenceError: innerVar is not defined
```

作用域链：内部函数可以链式访问外部函数的变量，例如：

```javascript
name = 'aaa'

function b() {
  var name = 'bbb'

  function c() {
    var name = 'ccc'
    console.log(name) //ccc
  }

  function d() {
    console.log(name) //bbb
  }
  c()
  d()
}
b()
console.log(name) //aaa
```

### Q：js 中什么是闭包？

闭包是利用了 js 的作用域特性，使用返回值为函数的函数，使里面的函数能够访问到外面函数的变量。我们调用外面的函数，就可以调用返回值函数，对外面函数的变量进行修改。

闭包可以访问另一个函数作用域中变量的函数。
作用：
1、可以减少全局变量的对象，防止全局变量过去庞大，导致难以维护
2、防止修改常量。
3、读取函数内部的变量，让这些变量的值始终保持在内存中。

例如：

```javascript
const add = function () {
  let counter = 0
  return function () {
    counter++
    return counter
  }
}

const x = add()
console.log(x()) // 计数器为 1
console.log(x()) // 计数器为 2
console.log(x()) // 计数器为 3
```

## 2、JavaScript 流控制

### Q：介绍一下 js 中 Promise 的用法？

Promise 对象代表一个异步操作，其不受外界影响，有三种状态：
Pending // 进行中、未完成的
Resolved // 已完成，又称 Fulfilled
Rejected // 已失败

Promise 对象提供了如下四种方法：
then // 异步操作完成调用方法
catch // 异步操作失败调用方法
all // 所有函数操作完成后调用方法
race // 第一个完成或失败后调用方法

举个例子：

```javascript
function read(time) {
  console.log('正在阅读')
  let p = new Promise(function (resolve, reject) {
    setTimeout(function () {
      console.log('阅读完毕')
      time = time + 1
      resolve(time)
    }, 2000)
  })
  return p
}

function write(time) {
  console.log('正在写作')
  let p = new Promise(function (resolve, reject) {
    setTimeout(function () {
      console.log('写作完毕')
      time = time + 1
      resolve(time)
    }, 2000)
  })
  return p
}

function rest(time) {
  console.log('正在休息')
  let p = new Promise(function (resolve, reject) {
    setTimeout(function () {
      console.log('休息完毕')
      time = time + 1
      resolve(time) //把promise的状态设置为resolved，成功
    }, 2000)
  })
  return p
}

function badDream(time) {
  console.log('正在做梦')
  let p = new Promise(function (resolve, reject) {
    setTimeout(function () {
      console.log('做了噩梦')
      time = time + 1
      reject(time) //把promise的状态设置为rejected，失败
    }, 2000)
  })
  return p
}

read(0)
  .then(function (data) {
    return write(data)
  })
  .then(function (data) {
    return badDream(data)
  })
  .then(function (data) {
    return rest(data)
  })
  .then(function (data) {
    console.log(data)
  })
  .catch(function (data) {
    //处理失败的方法
    console.log('第' + data + '步出错')
  })

const p1 = new Promise(function (resolve, reject) {
  setTimeout(function () {
    console.log('p1完成')
    resolve(11)
  }, 1000)
})

const p2 = new Promise(function (resolve, reject) {
  setTimeout(function () {
    console.log('p2完成')
    resolve(22)
  }, 2000)
})

const p3 = new Promise(function (resolve, reject) {
  setTimeout(function () {
    console.log('p3完成')
    resolve(33)
  }, 3000)
})

//all，所有函数都执行结束后，回调方法
Promise.all([p1, p2, p3]).then(function (data) {
  console.log('全部完成', data)
})

//race，第一个函数执行结束后，回调方法
Promise.race([write(1), rest(2)]).then(function (results) {
  console.log('准备工作完毕：')
  console.log(results)
})
```

### Q：介绍一下 js 中 async 和 await 的用法？与 Promise 的区别？

async 作为一个关键字放到函数前面，用于表示函数是一个异步函数。
因为 async 就是异步的意思，异步函数也就意味着该函数的执行不会阻塞后面代码的执行。
async 会返回一个 Promise 对象，比如我们 console.log(timeout())，会得到一个 Promise 对象，如果 resolve 会返回 hello world，如果 reject 就会抛异常。
但这不是重点，async 的最大优势在于其内部可以使用 await，能够更简洁的处理 Promise 的 then 链。
如果 async 中没有 await，它其实就相当于一个 resolve 返回了 return 值的 Promise 对象罢了。

await 表达式会暂停当前 async function 的执行，等待 Promise 处理完成。若 Promise 正常处理(fulfilled)，其回调的 resolve 函数参数作为 await 表达式的值，继续执行 async function。若 Promise 处理异常(rejected)，await 表达式会把 Promise 的异常原因抛出。另外，如果 await 操作符后的表达式的值不是一个 Promise，则返回该值本身。

async 和 await 结合能够更直观地处理 Promise 的 then 链。

举个例子：
Promise 的写法：

```javascript
doSomething()
  .then(function (res) {
    return doNext(res)
  })
  .then(function (nextRes) {
    return doFinalThing(nextRes)
  })
  .then(function (finalResult) {
    console.log('Got the final result: ' + finalResult)
  })
  .catch(failureCallback)
```

async/await 的写法：

```javascript
try {
  const result = await doSomething()
  const nextRes = await doNext(result)
  const finalResult = await doFinalThing(newResult)
  console.log('Got the final result: ' + finalResult)
} catch (error) {
  failureCallback(error)
}
```

## 3、JavaScript 常用数据结构

### Q：说几个常用的 Array 的 api？

**Array 的 API 重点是会用，而不是知道这个 API 是干什么的**

```javascript
push(a)           // 末尾添加，返回数组长度
unshift(a)	      // 首位添加，返回数组长度
shift()	          // 删除第一个元素，并返回删除的元素
pop()	            // 删除最后一个元素，并返回删除的元素
join(a)	          // 把数组中的所有元素放到一个字符串中，分隔符为a
concat(a, b)	    // 将b连接到a后面
sort()	          // 数组排序
reverse()	        // 数组倒序
isArray(a)	      // a是否为数组
slice(a, b)	      // 根据输入的开始点和末尾点，截取出新的数组（包括前不包括后）
splice(a, b, c)	  // 从a位置开始，删除b长度的数组，插入c

indexOf(a)	      // 从前向后寻找值等于a的项，返回index
lastIndexOf(a)	  // 从后向前寻找等于a的项，返回index

every(function(item, index, array))	      // 对数组每一项运行一次某函数，若函数每一项都为true，返回true
some(function(item, index, array))	      // 对数组每一项运行一次某函数，若函数某一项为true，返回true
filter(function(item, index, array))	    // 对数组每一项运行一次某函数，返回返回值为true的项的数组
map(function(item, index, array))	        // 对数组每一项运行一次某函数，返回返回值的数组
forEach(function(item, index, array))	    // 对数组每一项运行一次某函数，无返回值
reduce(function(prev, cur, index, array)) //接收四个参数：前一个值、当前值、项的索引和数组对象，这个函数的任意返回值会作为第一个参数传给下一项。
```

### Q：for in 和 for of 遍历数组/对象的区别？

对于数组：for in 输出的是数组索引值（下标），for of 输出的是数组元素。
对于对象：for in 输出的是对象 key 值，for of 输出的是对象 value 值。

### Q：代码实现一下展平数组？

```javascript
const arr = [1, 3, [3, 4, [5, 6]]]

// 第一种方案：用现成api，不兼容低版本的浏览器
const arr1 = arr.flat(Infinity)

// 第二种方案：原生循环递归实现
function flat2(arr) {
  const result = []
  arr.forEach((item) => {
    if (Array.isArray(item)) {
      result.push(...flat(item))
    } else {
      result.push(item)
    }
  })
  return result
}
const arr2 = flat2(arr)

// 第三种方案：使用解构和递归实现
function flat3(arr) {
  const result = [].concat(...arr.map((x) => (Array.isArray(x) ? flat(x) : x)))
  return result
}
const arr3 = flat3(arr)
```

### Q：代码实现一下深拷贝对象？

```javascript
// 对象中如果均为基本类型
JSON.parse(JSON.stringify(obj))

// 对象中如果既有基本类型，还有Array和Object
function deepCopy(data) {
  let output
  if (data === null || !(typeof data === 'object')) {
    output = data
  } else {
    output = data.constructor.name === 'Array' ? [] : {}
    for (let key in data) {
      output[key] = deepCopy(data[key])
    }
  }
  return output
}
```

### Q："test" 和 new String("test")有什么区别，{}和 new Object()有什么区别？

"test"是一个 string 类型的常量，而 new String()会创建一个字符串对象。

```javascript
const obj1 = new String('abc') // 类型为object
const obj2 = 'abc' // 类型为string

obj1 == obj2 // true
obj1 === obj2 // false
```

而 {} 和 new Object() 本身都是创建对象，并返回这个对象的引用。

```javascript
const obj1 = new Object('abc') // 类型为object
const obj2 = {} // 类型为obejct

obj1 == obj2 // false
obj1 === obj2 // false
```

## 4、DOM/BOM api

### Q：addEventListener 用法？和 onxxx 的区别是什么？

onxxx 是 DOM0 的标准，所有浏览器支持，它会直接在 DOM 对象上注册事件名称。
例如：

```javascript
document.getElementById('click').onclick = function (event) {
  console.log(event.target)
}
```

addEventListener 是 DOM2 的标准，IE8 以上浏览器支持，它分为三个阶段：捕获阶段，目标阶段，冒泡阶段。
每个事件都会先由根传到目标元素，再传回根。

### Q：事件代理指的是什么？

事件代理是指为了给多个元素添加事件（比如 li），我们给元素的父级元素添加事件，当事件发生时，我们通过`e.target`捕获事件目标，判断目标后，执行对应的事件。

### Q：如何使用 js 访问 cookie？如果想要禁止用 js 访问 cookie，该怎么做？

使用 document.cookie 即可查看 cookie。cookie 储存形式是`key=value;key=value;`形式的字符串。

在 header 中设置 HttpOnly 后可以防止 js 访问 cookie。

### Q：如何使用 js 计算浏览器可视区域？

```javascript
// 获取浏览器窗口的可视区域的宽度
const width = document.documentElement.clientWidth || document.body.clientWidth

// 获取浏览器窗口的可视区域的高度
const height = document.documentElement.clientHeight || document.body.clientHeight
```

---

# 文件：开发\前端开发\【2】html.md

---

# 前端开发校招面试问题整理【2】——HTML

## 1、HTML 元素（element）

### Q：简单介绍下常用的 HTML 元素？

块状标签：元素独占一行，可指定宽、高。
常用的块状元素有：

```
<div>、<p>、<h1>-<h6>、<ol>、<ul>、<dl>、<table>、<form>
```

内联元素：元素在一行内，宽度与高度由内容决定，只有在内容超过 HTML 的宽度时，才会换行。
常用的内联元素有：

```
<a>、<span>、<i>、<em>、<strong>、<label>
```

内联块状元素同时具备内联元素、块状元素的特点，它和其他元素都在一行，但元素的高度、宽度、行高以及顶和底边距都可设置。常用的内联块状元素有：

```
<img>、<input>
```

### Q：语义化元素是指？

语义化元素是指元素本身传达了关于其内容类型的一些信息。这些元素让页面的内容结构化，结构更清晰，便于 SEO，容易阅读和维护。

常见的语义化元素：

```
<header>
<footer>
<nav>
<article>
<section>
<aside>

<h1>
<h2>
<h3>
<h4>
<h5>
<h6>

<strong>
<em>
```

### Q：HTML5 新增了哪些元素？

| 标签        | 说明                                                             |
| ----------- | ---------------------------------------------------------------- |
| ``  | 定义 section 或 document 的页眉。                                |
| ``  | 定义 section 或 document 的页脚。                                |
| ``     | 定义导航链接的部分。                                             |
| `` | 定义文章的内容。                                                 |
| `` | 定义文档中的段落。比如章节、页眉、页脚或文档中的其他部分。       |
| ``   | 定义 article 以外的内容。aside 的内容应该与 article 的内容相关。 |
| ``   | 定义声音。                                                       |
| ``  | 定义图形。                                                       |
| ``   | 定义视频，比如电影片段或其他视频流。                             |
| ``  | 为媒介元素（比如`` 和 ``）定义媒介资源。           |

## 2、HTML 事件

### Q：描述一下 HTML 的事件模型？事件捕获/事件冒泡指的是？

![事件模型](http://www.shadowingszy.top/images/event.png)

当页面触发一个事件的时候，浏览器主要做了三个阶段的事情，分别是：
1、捕获事件阶段
2、目标处理阶段
3、后续事件处理阶段

当事件被触发，从根节点传递事件对象到目标节点的过程，就是事件捕获。
当处理完成事件后，从目标节点反向的传递到根节点的过程，就是事件冒泡。

### Q：如何阻止事件冒泡？如何阻止元素默认行为？

```javascript
event.stopPropagation() // 阻止事件冒泡
event.preventDefault() // 阻止元素默认行为
```

---

# 文件：开发\前端开发\【3】css.md

---

# 校招前端面试常见问题【3】——CSS

## 1、盒模型

### Q：请简述一下 CSS 盒模型？

![盒模型](http://www.shadowingszy.top/images/box.png)

W3C 模式：盒子宽=width+padding+border+margin
怪异模式：盒子宽=width+margin

### Q：inline、block、inline-block 元素的区别？

inline（行内元素）:
使元素变成行内元素，拥有行内元素的特性，即可以与其他行内元素共享一行，不会独占一行。
不能更改元素的 height，width 的值，大小由内容撑开。
可以使用 padding 上下左右都有效，margin 只有 left 和 right 产生边距效果，但是 top 和 bottom 就不行。

block（块级元素）:
使元素变成块级元素，独占一行，在不设置自己的宽度的情况下，块级元素会默认填满父级元素的宽度。
能够改变元素的 height，width 的值。
可以设置 padding，margin 的各个属性值，top，left，bottom，right 都能够产生边距效果。

inline-block（融合行内于块级）:
结合了 inline 与 block 的一些特点，结合了上述 inline 的第 1 个特点和 block 的第 2,3 个特点。
用通俗的话讲，就是不独占一行的块级元素。

## 2、选择器

### Q：请列举出你用过的 CSS 选择器？

普通选择器：

| 选择器  | 例子       | 描述                            |
| ------- | ---------- | ------------------------------- |
| .class  | .intro     | 选择 class="intro" 的所有元素。 |
| #id     | #firstname | 选择 id="firstname" 的元素。    |
| \*      | \*         | 选择所有元素                    |
| element | p          | 选择所有 元素                |

层次选择器

| 选择器            | 例子    | 描述                               |
| ----------------- | ------- | ---------------------------------- |
| element.class     | p.intro | 选择 class="intro" 的所有 元素  |
| element,element   | div, p  | 选择所有 元素和所有  元素  |
| element element   | div p   | 选择 元素内的所有  元素    |
| element>element   | div > p | 选择父元素是 的所有  元素  |
| element+element   | div + p | 选择紧跟 元素的首个  元素  |
| element1~element2 | p ~ ul  | 选择前面有 元素的每个  元素 |

属性选择器

| 选择器            | 例子            | 描述                                       |
| ----------------- | --------------- | ------------------------------------------ |
| [attribute]       | [target]        | 选择带有 target 属性的所有元素。           |
| [attribute=value] | [target=_blank] | 选择带有 target="\_blank" 属性的所有元素。 |

伪类选择器

| 选择器         | 例子           | 描述                                           |
| -------------- | -------------- | ---------------------------------------------- |
| :active        | a:active       | 选择活动链接。                                 |
| ::after        | p::after       | 在每个 的内容之后插入内容。                 |
| ::before       | p::before      | 在每个 的内容之前插入内容。                 |
| :first-child   | p:first-child  | 选择属于父元素的第一个子元素的每个 元素。   |
| :focus         | input:focus    | 选择获得焦点的 input 元素。                    |
| :fullscreen    | :fullscreen    | 选择处于全屏模式的元素。                       |
| :hover         | a:hover        | 选择鼠标指针位于其上的链接。                   |
| :link          | a:link         | 选择所有未访问过的链接。                       |
| :not(selector) | :not(p)        | 选择非 元素的每个元素。                     |
| :nth-child(n)  | p:nth-child(2) | 选择属于其父元素的第二个子元素的每个 元素。 |
| :visited       | a:visited      | 选择所有已访问的链接。                         |

### Q： CSS 选择器的权重是什么样的？

| 样式       | 权重      |
| ---------- | --------- |
| !important | 权重最大  |
| 内联样式   | 权重 1000 |
| 类选择器   | 权重 10   |
| id 选择器  | 权重 100  |
| 派生选择器 | 权重 1    |

## 3、常见规则

### Q：position 的值有哪几种，布局方式是什么样的？

| 值       | 描述                                                                                                                                                                                                     |
| -------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| static   | 元素框正常生成。块级元素生成一个矩形框，作为文档流的一部分，行内元素则会创建一个或多个行框，置于其父元素中。                                                                                             |
| relative | 相对于其正常位置进行定位，元素仍保持其未定位前的形状，它原本所占的空间仍保留。                                                                                                                           |
| absolute | 元素框从文档流完全删除，相对于值不为 static 的第一个父元素进行定位。元素原先在正常文档流中所占的空间会关闭，就好像元素原来不存在一样。元素定位后生成一个块级框，而不论原来它在正常流中生成何种类型的框。 |
| fixed    | 元素框的表现类似于将 position 设置为 absolute，不过其包含块是视窗本身。                                                                                                                                  |

### Q： 简单描述下 flex 布局？

使用 flex 布局的元素会成为容器（flex container），它内部的元素自动成为 flex 项目（flex item）。
容器拥有两根隐形的轴，水平的主轴（main axis），和竖直的交叉轴。
此外，需注意使用 flex 容器内元素，即 flex item 的 float，clear、vertical-align 属性将失效。

![盒模型](http://www.shadowingszy.top/images/flex.png)

flex 的各种属性：

```
1、flex-direction
属性决定主轴的方向（即项目的排列方向）。

row（默认值）：主轴为水平方向，起点在左端。
row-reverse：主轴为水平方向，起点在右端。
column：主轴为垂直方向，起点在上沿。
column-reverse：主轴为垂直方向，起点在下沿。
```

```
2、flex-wrap
默认情况下，项目都排在”轴线”上。本属性定义如果一条轴线排不下，如何换行。

nowrap（默认值）：不换行。
wrap：换行，第一行在上方。
wrap-reverse：换行，第一行在下方。
```

```
3、align-items
定义弹性盒子在交叉轴上如何对齐。

flex-start：交叉轴的起点对齐。
flex-end：交叉轴的终点对齐。
center：交叉轴的中点对齐。
baseline: 项目的第一行文字的基线对齐。
stretch（默认值）：如果项目未设置高度或设为 auto，将占满整个容器的高度。
```

```
4、justify-content
定义弹性盒子在主轴方向上的对齐方式。

flex-start：容器开头对齐。
flex-end：容器终点对齐。
center：容器中点对齐。
```

## 4、常见概念

### Q：FC 是什么？BFC 和 IFC 是什么？

FC：格式化模型。

FC 会根据 CSS 盒子模型将文档中的元素转换为一个个的盒子，每个盒子的布局由以下因素决定：
1、盒子的尺寸：精确指定、由约束条件指定或没有指定
2、盒子的类型：行内盒子（inline）、行内级盒子（inline-level）、原子行内级盒子（atomic inline-level）、块盒子（block）
3、定位方案：普通流定位、浮动定位或绝对定位
4、文档树中的其它元素：即当前盒子的子元素或兄弟元素
5、视窗尺寸与位置
6、包含的图片的尺寸
7、其他的某些外部因素

BFC：块级格式化上下文。

1、在 BFC 中，盒子从顶端开始垂直地一个接一个地排列，两个盒子之间的垂直的间隙是由它们的 margin 值所决定的。在一个 BFC 中，两个相邻的块级盒子的垂直外边距会产生折叠。
2、在 BFC 中，每一个盒子的左外边缘会触碰到容器的左边缘。
3、浮动元素、绝对定位元素，以及设置了 overflow 属性（除了 visible）的元素不是块级盒子的块容器，因此会为他们的内容创建新的 BFC。

IFC：行内级格式化上下文。

1、在 IFC 中，盒子一个接着一个地水平放置。这些盒子会通过不同的方式进行对齐，如底部对齐，顶部对齐，文字基线对齐。
2、矩形区域包含着来自一行的盒子叫做盒行盒（line box）。
3、line box 的宽度由浮动情况和它的包含块决定。line box 的高度由 line-height 计算决定（也就是说，由其内部的块撑开）。

### Q：如何清除浮动？

浮动可以理解为让某个 div 元素脱离标准流，漂浮在标准流之上。
一个浮动元素会尽量向左或向右移动，直到它的外边缘碰到包含框或另一个浮动框的边框为止。

清除浮动可以理解为打破横向排列。清除浮动的关键字是 clear，其取值有以下几种：
1、none，默认值。允许两边都可以有浮动对象
2、left，不允许左边有浮动对象
3、right，不允许右边有浮动对象
4、both，不允许有浮动对象
对于 CSS 的清除浮动(clear)，一定要牢记：这个规则只能影响使用清除的元素本身，不能影响其他元素。

### Q：什么是回流？什么是重绘？

当页面中的一部分(或全部)因为元素的规模尺寸，布局，隐藏等改变而需要重新绘制，这就称为回流。每个页面至少需要一次回流，就是在页面第一次加载的时候。

当页面中的一些元素需要更新属性，而这些属性只是影响元素的外观，风格，而不会影响布局的，比如 background-color。则就叫称为重绘。

任何对页面中元素的操作都会引起回流或者重绘，比如：

1、添加、删除元素(回流+重绘)
2、隐藏元素，display:none(回流+重绘)，visibility:hidden(只重绘，不回流)
3、移动元素，比如改变 top,left(重绘+回流)。
4、对 style 的操作(对不同的属性操作，影响不一样)。
5、还有一种是用户的操作，比如改变浏览器大小，改变浏览器的字体大小等(回流+重绘)

### Q：如何开启 GPU 加速？其优缺点是什么？

当页面中某个 DOM 元素应用了某些 CSS 规则时就会开启 GPU 加速，如 3D 变换：

```css
.cube {
  -webkit-transform: translate3d(250px, 250px, 250px) rotate3d(250px, 250px, 250px, -120deg) scale3d(0.5, 0.5, 0.5);
}
```

如果不想对元素用 3D 变换但是还想要开 GPU 加速，就可以：

```css
.cube {
  -webkit-transform: translateZ(0);
  -moz-transform: translateZ(0);
  -ms-transform: translateZ(0);
  -o-transform: translateZ(0);
  transform: translateZ(0);
}
```

但是，一定要注意：不要随意使用 GPU 加速，如果的确能够显著提高性能，可以尝试使用 GPU 加速。但是另一方面，使用 GPU 可能会导致严重的性能问题，因为它增加了内存的使用，而且它会减少移动端设备的电池寿命。

---

# 文件：开发\前端开发\【4】网络及浏览器.md

---

# 校招前端面试常见问题【4】——网络及浏览器

## 1、网络相关

### Q：请简述一下 HTTP 协议，以及 HTTP1.0/1.1/2.0/3.0 的区别？

HTTP 协议：超文本传输协议，使用 TCP/IP 协议传输数据。是一个应用层的协议。

HTTP1.0：HTTP 1.0 规定浏览器与服务器只保持短暂的连接，浏览器的每次请求都需要与服务器建立一个 TCP 连接，服务器完成请求处理后立即断开 TCP 连接，服务器不跟踪每个客户也不记录过去的请求。因此 HTTP 1.0 存在很大的性能缺陷——当访问一个包含有许多资源文件的网页时，每次请求和响应都需要建立一个单独的连接，每次连接只是传输一个文档和图像，请求之间完全分离。即使图像文件都很小，但是客户端和服务器端每次建立和关闭连接却是一个相对比较费时的过程，会严重影响到性能。

HTTP 1.1：支持长连接的 HTTP 协议，在一个 TCP 连接上可以传送多个 HTTP 请求和响应。一个包含有许多图像的网页文件的多个请求和应答可以在一个连接中传输，但每个单独的网页文件的请求和应答仍然需要使用各自的连接。

HTTP2.0：支持多路复用的 HTTP 协议。 HTTP 2.0 允许同时通过单一的 HTTP 2.0 连接发起多重的请求-响应消息。所有通信都在一个连接上完成，这个连接可以承载任意数量的双向数据流。由于 TCP 有慢启动的特点，如果 HTTP 连接很多，就会十分低效。HTTP/2 通过让所有数据流共用同一个连接，可以更有效地使用 TCP 连接。

HTTP3.0：也就是 QUIC (quick udp internet connection)协议，是由 google 提出的使用 udp 进行多路并发传输的协议。通过使用 UDP 协议，省去了 TCP 握手和慢启动的时间，拥有极低的建立连接延时。

### Q：请简述一下 HTTPS 协议？

HTTPS 在传输数据之前需要客户端与服务器之间进行一次握手，在握手过程中将确立双方加密传输数据的密码信息。TLS/SSL 协议不仅仅是一套加密传输的协议，TLS/SSL 中使用了非对称加密，对称加密以及 HASH 算法。

握手过程的简单描述如下：
1、客户端将自己支持的加密规则发送给服务器。

2、网站从中选出加密算法和 HASH 算法，将证书发回给浏览器。证书里面包含了网站地址，加密公钥，以及证书的颁发机构等信息。

3、获得证书之后客户端要做以下工作：
a) 验证证书（颁发证书的机构是否合法，证书中包含的网站地址是否与正在访问的地址一致等）。
b) 如果通过验证，浏览器会生成一串随机数的密码，并用证书中提供的公钥加密
c) 计算握手信息的 HASH，然后将握手信息也一并加密，最后将所有信息发送给网站。

4、网站接收浏览器发来的数据之后要做以下的操作：
a) 使用自己的私钥将信息解密取出密码，使用密码解密握手消息，判断 HASH 是否一致。
b) 计算握手的 HASH，并使用密码加密握手消息，发送给浏览器。

5、浏览器解密并计算握手消息的 HASH，如果与服务端发来的 HASH 一致，此时握手过程结束，之后所有的通信数据将由之前浏览器生成的随机密码进行加密。

### Q：请简述一下 HTTP 协议中的缓存策略？

HTTP 的缓存策略有两种：强缓存和协商缓存。

强缓存是利用 http 头中的 Expires 和 Cache-Control 两个字段来控制的，用来表示资源的缓存时间。强缓存中，普通刷新会忽略它，但不会清除它，需要强制刷新。

例如：
cache-control: max-age=691200 （HTTP 1.1）
expires: Fri, 14 Apr 2017 10:47:02 GMT （HTTP 1.0）

协商缓存主要涉及到两个 header 字段：E-Tag 和 Last-Modified。每次读取数据时客户单都会跟服务器通信，并且会增加缓存标识。在第一次请求服务器时，服务器会返回资源，并且返回一个资源的缓存标识，一起存到浏览器的缓存数据库。当第二次请求资源时，浏览器会首先将缓存标识发送给服务器，服务器拿到标识后判断标识是否匹配，如果不匹配，表示资源有更新，服务器会将新数据和新的缓存标识一起返回到浏览器；如果缓存标识匹配，表示资源没有更新，并且返回 304，浏览器就读取本地缓存服务器中的数据。

例如：
E-Tag: 123456abcd
Last-Modify: Thu,31 Dec 2037 23:59:59 GMT。

## 2、浏览器相关

### Q：请列举一下你知道的浏览器内核的种类？

Trident：IE6、IE7、IE8、IE9、IE10、360 浏览器和猎豹浏览器。
Gecko：firefox 浏览器。
Blink：opera 浏览器。
Webkit：sarfari 和 chrome 浏览器。

### Q：浏览器内核中的有哪些线程？

内核主要分成五部分：

![内核](http://www.shadowingszy.top/images/browser.png)

GUI 渲染线程：负责渲染浏览器界面，解析 HTML，CSS，构建渲染树，布局和绘制等。当界面需要重绘或回流时，该线程就会执行。注意，GUI 渲染线程与 JS 引擎线程是互斥的，

JS 引擎线程：解析和执行 javascript。

事件触发线程：归属于浏览器而不是 JS 引擎，用来控制事件循环。

定时器触发线程：setInterval 与 setTimeout 所在线程。浏览器定时计数器并不是由 JavaScript 引擎计数的，因此通过单独线程来计时并触发定时（计时完毕后，添加到事件队列中，等待 JS 引擎空闲后执行）。

异步 http 请求线程：在 XMLHttpRequest 在连接后是通过浏览器新开一个线程请求，将检测到状态变更时，如果设置有回调函数，异步线程就产生状态变更事件，将这个回调再放入事件队列中，再由 JavaScript 引擎执行。

### Q：请简述一下浏览器的渲染流程？

![渲染流程](http://www.shadowingszy.top/images/render.png)

接收到文档后，渲染引擎会对 HTML 文档进行解析生成 DOM 树、对 CSS 文件进行解析生成布局树；同时执行页面中的 JavaScript 代码；最终根据 DOM 树和布局树，计算样式生成渲染树，渲染树中，只会包含即将显示在页面中的元素及其样式信息（如 head 元素、display 为 hidden 的元素就不会包含在渲染树中）；根据渲染树需要进行布局来计算每个元素在页面上的位置；

接下来渲染引擎开始进行绘制（paint），这一步分为若干阶段：
1、根据渲染树绘制每层的各个元素。
2、栅格化绘制出的图像（将渲染树中的节点转换成屏幕上的实际像素）
3、显示在屏幕上。
每一层的绘制是由浏览器来完成的；最后的合成是由 GPU 来完成；而栅格化过程取决于浏览器的设置，chrome 默认开启 GPU 栅格化，否则由 CPU 进行。

### Q：浏览器从输入请求到呈现页面有哪几步？

1、URL 解析
2、DNS 查询
3、TCP 连接
4、处理请求
5、接受响应
6、渲染页面

### Q：localstorage、sessionstorage 的区别，以及使用场景是什么？

localStorage：生命周期是永久的，关闭页面或浏览器之后 localStorage 中的数据也不会消失。localStorage 除非主动删除数据，否则数据永远不会消失（只会存储 string）。

sessionStorage：生命周期是在仅在当前会话下有效。sessionStorage 引入了一个“浏览器窗口”的概念，sessionStorage 是在同源的窗口中始终存在的数据。只要这个浏览器窗口没有关闭，即使刷新页面或者进入同源另一个页面，数据依然存在。但是 sessionStorage 在关闭了浏览器窗口后就会被销毁。同时独立的打开同一个窗口同一个页面，sessionStorage 也是不一样的。

使用方法：

```javascript
window.localStorage
window.sessionStorage
```

API：

```javascript
setItem(key, value) // 保存数据，以键值对的方式储存信息。
getItem(key) // 获取数据，将键值传入，即可获取到对应的 value 值。
removeItem(key) // 删除单个数据，根据键值移除对应的信息。
clear() // 删除所有的数据
key(index) // 获取某个索引的 key
```

---

# 文件：开发\前端开发\【5】前端框架及打包工具.md

---

# 校招前端面试常见问题【5】——前端框架及常用工具

## React

#### Q：请简述一下虚拟 DOM 的概念？

基于 React 进行开发时所有的 DOM 构造都是通过虚拟 DOM 进行，每当数据变化时，React 都会重新构建整个 DOM 树，然后 React 将当前整个 DOM 树和上一次的 DOM 树进行对比，得到 DOM 结构的区别，然后仅仅将需要变化的部分进行实际的浏览器 DOM 更新。

React 在构建 DOM 的时候，是使用 javascript 的对象模拟 DOM 的，针对 js 的对象进行比较要比针对浏览器 DOM 进行比较的开销小很多。

#### Q：请简述一下 React 的生命周期？

![react生命周期](http://www.shadowingszy.top/images/react.png)

#### Q：请简述一下 React Fiber 的概念？

在页面元素很多，且需要频繁刷新的场景下，React 15 会出现掉帧的现象。那么为什么会出现掉帧问题呢？其根本原因，是大量的同步计算任务阻塞了浏览器的 UI 渲染。默认情况下，JS 运算、页面布局和页面绘制都是运行在浏览器的主线程当中，他们之间是互斥的关系。如果 JS 运算持续占用主线程，页面就没法得到及时的更新。当我们调用 setState 更新页面的时候，React 会遍历应用的所有节点，计算出差异，然后再更新 UI。如果页面元素很多，整个过程占用的时机就可能超过 16 毫秒，就容易出现掉帧的现象。而原因就是 React 15 采用的是递归的方式遍历整颗组件树。

react16 将底层更新单元的数据结构改成了链表结构。以前的协调算法是递归调用，通过 react dom 树级关系构成的栈递归。而 fiber 是扁平化的链表的数据存储结构，通过 child 找第一个子节点，return 找父节点，sibling 找兄弟节点。遍历从递归改为循环。

这是 React 核心算法的一次大的更新，重写了 React 的 reconciliation 算法。reconciliation 算法是用来更新并且渲染 DOM 树的算法。以前 React 15.x 的版本使用的算法称为“stack reconciliation”，现在称为“fiber reconciler”。

fiber reconciler 主要的特点是可以把更新流程拆分成一个一个的小的单元进行更新，并且可以中断，转而去执行高优先级的任务或者浏览器的动画渲染等，等主线程空闲了再继续执行更新。

对于流畅度问题，我们很容易想到一个 api：requestldleCallback ， 这个 api 可以在浏览器空闲的时候执行回调，我们把复杂的任务分片在浏览器空闲的时间执行，就不会影响浏览器的渲染等工作。这个就可以解决复杂任务长时间霸占主线程导致渲染延迟。

但是可能由于兼容性的考虑，react 团队放弃了这个 api，转而利用 requestAnimationFrame 和 MessageChannel pollyfill 了一个 requestIdleCallback

当前帧先执行浏览器的渲染等任务，如果当前帧还有空闲时间，则执行任务，直到当前帧的时间用完。如果当前帧已经没有空闲时间，就等到下一帧的空闲时间再去执行。

#### Q：React setState 的时机？

使用 setState 时不会直接更新数据，而是会直接将其挂到更新队列中。
更新的时机是：当前宏任务结束后，微任务开始前。

```javascript
this.state = {
  a: 1,
}

// 这种情况只会+1，因为它相当于Object.assign(oldState, {count: XXX}, {count: XXX})
this.setState({ count: this.state.count + 1 })
this.setState({ count: this.state.count + 1 })

console.log(this.state.count) // 这时候会取到原来的state，也就是1

// 进行改造，这样就一定会+2了
this.setState((state, props) => {
  return { count: state.count + 1 }
})
this.setState((state, props) => {
  return { count: state.count + 1 }
})
```

## Vue

#### Q：什么是 mvvm 模式？

M: 模型 => 数据，业务逻辑，验证逻辑，模型常常包含业务逻辑。
V: 视图 => 交互界面，是模型数据的可视化呈现，视图可能包含展示逻辑。
VM：视图和模型的中间人。

数据双向绑定：V 的变动直接反映在了 VM 上，M 的变化也直接反映在了 VM 上。

#### Q：请简述一下 vue 响应式数据的原理？

响应式数据的关键在于：data 如何更新 view，以及 view 如何更新 data。

1、view 更新 data 可以通过事件监听，比如 input 标签监听 'input' 事件就可以实现了。

2、而 data 更新 view 的重点是如何知道数据变了。这时候我们就通过`Object.defineProperty()`对属性设置一个 set 函数，当数据改变了就会来触发这个函数，所以我们只要将一些需要更新的方法放在这里面就可以实现 data 更新 view 了。

Object.defineProperty 的具体用法：

```javascript
Object.defineProperty(obj, prop, descriptor)
obj：要在其上定义属性的对象。
prop：要定义或修改的属性的名称。
descriptor：将被定义或修改的属性描述符。

descriptor 具有以下两种可选值：
get：给属性提供 getter 的方法，如果没有 getter 则为 undefined。当访问该属性时，该方法会被执行，方法执行时没有参数传入，但是会传入 this 对象。
set：给属性提供 setter 的方法，如果没有 setter 则为 undefined。当属性值修改时，触发执行该方法。该方法将接受唯一参数，即该属性新的参数值。
```

一个简单的响应式数据的例子：

```html
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <title>defineProperty</title>
  </head>
  <body>
    <div id="app">
      <input type="text" id="txt" />
      <p id="show"></p>
    </div>

    <script>
      let obj = {}

      Object.defineProperty(obj, 'txt', {
        get: function () {
          return obj
        },
        set: function (newValue) {
          document.getElementById('txt').value = newValue
          document.getElementById('show').innerHTML = newValue
        },
      })
      document.addEventListener('keyup', function (e) {
        obj.txt = e.target.value
      })
    </script>
  </body>
</html>
```

#### Q：请简述一下 Vue 的生命周期？

![vue生命周期](http://www.shadowingszy.top/images/vue.png)

#### Q：请简述一下 Vue router 的原理？

Vue router 有两种模式：hash 模式和 history 模式，分别对应了两种原理：

hash 模式：

```
hash("#")符号的本来作用是加在 URL 指示网页中的位置，例如：
http://www.example.com/index.html#print

#本身以及它后面的字符称之为 hash 可通过 window.location.hash 属性读取。
hash 虽然出现在 url 中，但不会被包括在 http 请求中，对服务器端完全无用，因此，改变 hash 不会重新加载页面。

我们可以为 hash 的改变添加监听事件：
window.addEventListener("hashchange",funcRef,false)

每一次改变 hash，我们都会重新注入对应的组件，就可以来实现前端路由"更新视图但不重新请求页面"的功能了。
```

history 模式：

```
从HTML5开始，History interface提供了2个新的方法：pushState(),replaceState()使得我们可以对浏览器历史记录栈进行修改。

window.history.pushState(stateObject, title, URL)
window.history.replaceState(stateObject, title, URL)

stateObject: 当浏览器跳转到新状态时，触发popState事件，该事件将携带stateObject参数的副本
title: 所添加记录的标题
URL: 所添加记录的URL

我们可以为window.history的改变添加监听事件：
window.addEventListener("popstate",funcRef,false)

在监听事件中，重新注入对应的组件，就可以来实现前端路由"更新视图但不重新请求页面"的功能了。

用 HTML5 实现，单页路由的 url 就不会多出一个#，变得更加美观。但因为没有 # 号，所以当用户刷新页面之类的操作时，浏览器还是会给服务器发送请求，可能会造成404。
```

## 打包工具

#### Q：介绍一下 webpack？

webpack 是一个模块打包工具，在 webpack 中，一切文件都是模块，webpack 能做的就是将它们打包在一起。

webpack 在配置时主要有如下常用属性：

1、entry 以及 output：
入口起点(entry point)指示 webpack 应该使用哪个模块，来作为构建其内部依赖图的开始。进入入口起点后，webpack 会找出有哪些模块和库是入口起点（直接和间接）依赖的。

output 属性告诉 webpack 在哪里输出它所创建的 bundles，以及如何命名这些文件，默认值为 ./dist。基本上，整个应用程序结构，都会被编译到你指定的输出路径的文件夹中。你可以通过在配置中指定一个 output 字段，来配置这些处理过程。

```javascript
const path = require('path')
module.exports = {
  entry: './path/to/my/entry/file.js',
  output: {
    path: path.resolve(__dirname, 'dist'),
    filename: 'my-first-webpack.bundle.js',
  },
}
```

2、loader：
loader 让 webpack 能够去处理那些非 JavaScript 文件。loader 可以将所有类型的文件转换为 webpack 能够处理的有效模块，然后你就可以利用 webpack 的打包能力，对它们进行处理。
本质上，webpack loader 将所有类型的文件，转换为应用程序的依赖图（和最终的 bundle）可以直接引用的模块。

```javascript
const path = require('path')
const config = {
  module: {
    rules: [{ test: /\.txt$/, use: 'raw-loader' }],
  },
}
```

3、plugins：
插件相比于 loader，可以用于执行范围更广的任务，比如压缩打包，优化等。想要使用一个插件，你只需要 require() 它，然后把它添加到 plugins 数组中。

```javascript
const HtmlWebpackPlugin = require('html-webpack-plugin') // 通过 npm 安装
const webpack = require('webpack') // 用于访问内置插件
const config = {
  module: {
    rules: [{ test: /\.txt$/, use: 'raw-loader' }],
  },
  plugins: [new HtmlWebpackPlugin({ template: './src/index.html' })],
}
```

---

# 文件：开发\前端开发\【6】nodejs.md

---

# 校招前端面试常见问题【6】——NodeJS

## NodeJS

#### Q：NodeJS 的 IO 模型特点是什么？与多线程同步 IO 有什么不同？

NodeJS 的 IO 模型（更准确的说是 js 的执行环境，也就是 v8）的特点是“单线程异步非阻塞”。

而与多线程同步 IO，两者各有优劣，应该根据实际应用场景来做取舍。

在传统的观点里，异步 IO 的好处是 IO 本身并不需要占用太多的资源，缺点在于非线性代码带来的复杂度和难以理解维护，而多线程同步 IO 的缺点在于性能资源的开销和线程管理的问题。

所以很显然，在相同的机器资源里面，异步 IO 的并发量肯定是要高于多线程同步 IO 的；但是服务器程序本身肯定不只是由 IO 组成，还有逻辑运算的部分，过重的逻辑运算依旧会影响性能。换句话说，密集型 CPU 任务会阻塞 js 的执行，导致异步 IO 得不到处理，极大地影响到 node 处理响应的时间。

总之，node 的 IO 模型更适合处理 IO 密集型的任务。多线程同步 IO 更适合处理计算密集型的任务。

#### Q：V8 引擎垃圾回收机制是什么样的？

1、如何判断是否可以回收
（1）标记清除
当变量进入环境（例如，在函数中声明一个变量）时，就将这个变量标记为“进入环境”。从逻辑上讲，永远不能释放进入环境的变量所占用的内存，因为只要执行流进入相应的环境，就可能会用到它们。而当变量离开环境时，则将其标记为“离开环境”。

具体做法：
垃圾收集器在运行的时候会给存储在内存中的所有变量都加上标记（当然，可以使用任何标记方式）。
然后，它会去掉运行环境中的变量以及被环境中变量所引用的变量的标记
此后，依然有标记的变量就被视为准备删除的变量，原因是在运行环境中已经无法访问到这些变量了。
最后，垃圾收集器完成内存清除工作，销毁那些带标记的值并回收它们所占用的内存空间。

（2）引用计数
引用计数的含义是跟踪记录每个值被引用的次数。
当声明了一个变量并将一个引用类型值赋给该变量时，则这个值的引用次数就是 1。
如果同一个值又被赋给另一个变量，则该值的引用次数加 1。
相反，如果包含对这个值引用的变量又取得了另外一个值，则这个值的引用次数减 1。
当这个值的引用次数变成 0 时，就可以将其占用的内存空间回收回来，这样，当垃圾收集器下次再运行时，它就会释放那 些引用次数为零的值所占用的内存。
但这样会有循环引用的问题。

2、V8 垃圾回收策略
将内存分为两个生代：新生代（new generation）和老生代（old generation）。
新生代中的对象为存活时间较短的对象，老生代中的对象为存活时间较长或常驻内存的对象，分别对新老生代采用不同的垃圾回收算法来提高效率，对象最开始都会先被分配到新生代（如果新生代内存空间不够，直接分配到老生代），新生代中的对象会在满足某些条件后，晋升到老生代。

新生代主要使用 Scavenge 进行管理，将内存平均分为两块，使用空间叫 From，闲置空间叫 To，新对象都先分配到 From 空间中，在空间快要占满时将存活对象复制到 To 空间中，然后清空 From 的内存空间，此时，调换 From 空间和 To 空间，继续进行内存分配，当满足晋升条件时对象会从新生代晋升到老生代。

对象晋升的条件主要有两个：
如果一个对象是第二次经历从 From 空间复制到 To 空间，那么这个对象会被移动到老生代中。
当要从 From 空间复制一个对象到 To 空间时，如果 To 空间已经使用了超过 25%，则这个对象直接晋升到老生代中。（设置 25%这个阈值的原因是当这次 Scavenge 回收完成后，这个 To 空间会变为 From 空间，接下来的内存分配将在这个空间中进行。如果占比过高，会影响后续的内存分配）

老生代主要采用 Mark-Sweep 和 Mark-Compact 算法，一个是标记清除，一个是标记整理。两者不同的地方是，Mark-Sweep 在垃圾回收后会产生碎片内存，而 Mark-Compact 在清除前会进行一步整理，将存活对象向一侧移动，随后清空边界的另一侧内存，这样空闲的内存都是连续的，但是带来的问题就是速度会慢一些。在 V8 中，老生代是 Mark-Sweep 和 Mark-Compact 两者共同进行管理的。

#### Q：实现一个 EventEmitter？

实现：

```javascript
class EventEmitter {
  constructor() {
    this._events = {}
  }

  subscribe(type, handler) {
    if (this._events.hasOwnProperty(type)) {
      this._events[type].push(handler)
    } else {
      this._events[type] = [handler]
    }
  }

  unsubscribe(type, handler) {
    if (this._events.hasOwnProperty(type)) {
      const index = this._events[type].indexOf(handler)
      if (index > -1) {
        this._events[type].splice(index, 1)
      }
    }
  }

  once(type, handler) {
    let fired = false
    let _this = this
    function magic() {
      _this.unsubscribe(type, magic)

      if (!fired) {
        fired = true
        handler.apply(_this, arguments)
      }
    }
    this.subscribe(type, magic)
  }

  emit(type, args) {
    if (this._events.hasOwnProperty(type)) {
      this._events[type].forEach((fn) => fn(args))
    }
  }
}

module.exports = EventEmitter
```

使用：

```javascript
const EventEmitter = require('./myEventEmitter')

const eventEmitter = new EventEmitter()

const fn = (args) => {
  console.log('good args', args)
}
const fn2 = (args) => {
  console.log('good args 2', args)
}
const fn3 = (args) => {
  console.log('good args 3', args)
}

eventEmitter.subscribe('good', fn)
eventEmitter.subscribe('good2', fn2)

eventEmitter.emit('good', 11111)
eventEmitter.emit('good2', 22222)

eventEmitter.unsubscribe('good', fn)

eventEmitter.emit('good2', 22222)

eventEmitter.once('good3', fn3)
eventEmitter.emit('good3', 33333)

eventEmitter.emit('good3', 33333)
```

#### Q：es6 模块化、commonjs 模块化的区别？

es6 模块化：

```
在es6规范中，使用import和export可以使js文件模块化。
每个import的js文件都是单例，如果再次import，就直接在内存中进行读取。

导出方式1：
//lib.js 文件
let foo = "stringFoo";
let fn0 = function() {
    console.log("fn0");
};
export{foo, fn}

//main.js文件
import {foo, fn} from "./lib";
console.log(bar+"_"+foo);
```

commonjs 模块化：

```
Node 应用由模块组成，采用 CommonJS 模块规范。

每个文件就是一个模块，有自己的作用域。在一个文件里面定义的变量、函数、类，都是私有的，对其他文件不可见。如果要定义全局变量，需要global属性。

CommonJS规范规定，每个模块内部，module变量代表当前模块。
这个变量是一个对象，它的exports属性（即module.exports）是对外的接口。加载某个模块，其实是加载该模块的module.exports属性。
为了方便，Node为每个模块提供一个exports变量，指向module.exports。

例如：
var test = function () {
	console.log(123);
};
module.exports.test = test;

使用require('XXX')加载模块。
require命令的基本功能是，读入并执行一个JavaScript文件，然后返回该模块的exports对象。如果没有发现指定模块，会报错。
```

## NodeJS 相关框架

#### Q：请简述一下 Koa 的洋葱模型？

koa 洋葱模型是指 koa 中每个中间件的执行顺序。
koa 在执行多个中间件中的逻辑时，会先执行第一个中间件的逻辑，执行到 next()函数后会执行第二个中间件的逻辑，以此类推，直到最后一个中间件。当最后一个中间件执行完毕后，会跳回执行倒数第二个中间件 next()函数后面的代码，以此类推，直到第一个中间件 next()函数后面的代码执行完毕。

![洋葱模型](http://www.shadowingszy.top/images/koa.png)

举例来说：

```javascript
const Koa = require('koa')

const app = new Koa()
const PORT = 3000

// #1
app.use(async (ctx, next) => {
  console.log(1)
  await next()
  console.log(1)
})
// #2
app.use(async (ctx, next) => {
  console.log(2)
  await next()
  console.log(2)
})

app.use(async (ctx, next) => {
  console.log(3)
})

app.listen(PORT)
console.log(`http://localhost:${PORT}`)
```

访问 http://localhost:3000，控制台打印：

```
1
2
3
2
1
```

---

# 文件：开发\大数据\mapreduce.md

---

## 海量数据处理常用技术概述

> 如今互联网产生的数据量已经达到PB级别，如何在数据量不断增大的情况下，依然保证快速的检索或者更新数据，是我们面临的问题。
> 所谓海量数据处理，是指基于海量数据的存储、处理和操作等。因为数据量太大无法在短时间迅速解决，或者不能一次性读入内存中。

在解决海量数据的问题的时候，我们需要什么样的策略和技术，是每一个人都会关心的问题。今天我们就梳理一下在解决大数据问题
的时候需要使用的技术，但是注意这里只是从技术角度进行分析，只是一种思想并不代表业界的技术策略。
常用到的算法策略

1. 分治：多层划分、MapReduce
2. 排序：快速排序、桶排序、堆排序
3. 数据结构：堆、位图、布隆过滤器、倒排索引、二叉树、Trie树、B树，红黑树
4. Hash映射：hashMap、simhash、局部敏感哈希

### 海量数据处理－－从分而治之到Mapreduce

**分治**

> 分治是一种算法思想，主要目的是将一个大问题分成多个小问题进行求解，之后合并结果。我们常用到的有归并排序:*先分成两部分进行排序，之后在合并*，
> 当然还有其他的很多应用，就比如是我们上篇文章中提到的Top K问题，就是将大文件分成多个小文件进行统计，之后进行合并结果。这里我们对分治进行抽象，
> 依然从上述提到的Top K频率统计开始出发。定义如下：有M多个Query日志文件记录，要求得到Top K的Query。
> 我们可以抽象成几个步骤：

1. 多个文件的输入，我们叫做**input splits**
2. 多进程同时处理多个文档，我们叫做**map**。
3. **partition** *从上文中我们知道。因为我们要将相同的Query映射的一起*
4. 多进程处理划分或的文件，我们叫做**reduce**
5. 合并过个文件的结果，我们叫做**merge**

> 上面的这四个步骤是我们从Top K问题抽象出来的，为什么我们对每一步进行一个取名字？因为这就是最简单的MapReduce的原理。我们现在就可以认为之前已经
> 用过Mapreduce的思想了，它就是这么简单，当然中的很多问题我都没有提出来，但是主要的思想就是这样，很成熟的MapReduce的实现，有Hadoop和CouchDB等。
> 我给出一张图片来表示这个过程。
> ![](http://images0.cnblogs.com/blog/508066/201506/151059199826134.jpg)

**MapReduce**

> MapReduce是一种编程模式、大数据框架的并行处理接口和分布式算法计算平台，主要用于大规模数据集合的并行计算。一个Mapreduce的程序主要有两部分组成: map和reduce. 它主要借鉴了函数式编程语言和矢量编程语言特性。
> MapReduce最早是由Google公司研究提出的一种面向大规模数据处理的并行计算模型和方法。Google公司设计MapReduce的初衷主要是为了解决其搜索引擎中大规模网页数据的并行化处理。

**MapReduce组成**

> 1. **Map:**
>    用户根据需求设置的Map函数，每一个工作节点(主机)处理本地的数据，将结果写入临时文件，给调用Reduce函数的节点使用。

> 2. **Shuffle:**
>    在MapReduce的编程模式中，我们要时刻注意到数据结构是(key, value)对，Shuffle就是打乱数据，也是我们之前提到过的Partition处理，主要目的是将相同的key的数据映射到同一个Reduce工作的节点（这是主要的功能，当然还有其他的功能）。

> 3. **Reduce:**
>    Reduce函数，并行处理相同key的函数，返回结果。

Mapreduce模式这么流行，现在几乎所有的大公司都在使用Hadoop框架，当然可能会有一些优化，不过主要的思想依然是MapReduce模式。在公司中或者个人的使用的时候，我们一般会先搭建Hadoop环境，之后最简单的使用就是提供Map函数和Reduce函数即可，语言可以使用C++、Java、Python等。例如我们提到的Top k问题的伪代码的例子：

```
map(String key, String values):
    // key: 文档名字
    // values: 文档内容
    for each line in values:
        EmitIntemediate(line, "1")

..... // 这中间的省略号，表示还可以加一些代码，
..... // 不加也不影响结果，只是效率问题，后面会提到

reduce(String key, Iterator values):
    // key: a query
    // values: a lists of counts
    int result = 0;
    for each v in values:
        result += ParseInt(v)
    Emit(AsString(result))
```

**代码抽象**

> map:　　　 (k1, v1) 　　   --->　　 list(k2, v2)
> reduce: 　　(k2, list(v2)) ---> 　　list(v３)

MapReduce支持的数据格式，从上述的代码中，我们可以看到MapReduce的输入和输出都是(k, v)对的格式。当然这只是转换之后的格式，一般来书我们的输入文件都是文件，MapReduce认为第一个分隔符之前的字段是key，后面的values，(values可以不存在，例如我们的Top k问题就没有values)。所有在使用的时候，我们只需要用分隔符空格将key和values分开，每一行代表一个数据，提供我们需要的Map和Reduce函数即可。

文章到此应该已经可以结束了，我们可以在任何MapReduce框架下，根据需求写出map函数和reduce函数。对于想用使用MapReduce的程序员来说，在写函数的时候只需要注意key和value怎么设置，如何编写map和reduce函数，因为中间的细节，运行的框架已经帮我们封装的很好的，这就是为什么Mapreduce在业界流行。这种编程模式很简单，只要提map和reduce函数，对于那些没有并行计算和分布式处理经验的程序员，MapReduce框架帮我们处理好了并行计算、错误容忍、本地读取优化和加载平衡的细节，我们只需要关注业务，不用关心细节，还有就是这么编程模式可以简单的解决很多常见的问题，例如: linux中的grep命令，Sort，Top K，倒排索引等问题。

知其然而知其所以然，不仅更能帮助我们写出更优的代码，更重要的是如何在改进现有的技术，使其更好的应用到我们的业务上，因为很多大公司都会重写这种代码，使其在公司内部更好的应用。

### 浅谈技术细节

MapReduce模式下我们需要关注的问题如下(参考论文)：

1. **数据和代码如何存储?**

> 设置一个Master，拷贝代码文件，分配给节点进行处理，指定Map或者Reduce已经输入和输出文件的路径。所有Master节点是一个管理节点负责调度。

2. **如何Shuffle？**

> 在MapReduce中都是(key, values)数据，输入的M个文件直接对应M的Map，产生的中间结果key2，通过哈希函数，
> hash(key) % R(R是Reduce的个数)。当然我们需要设置一个好的hash函数，保证任务不平衡分到不同的Reduce节点上。

3. **节点之间如何通信？**

> Master负责调度和通信，其他节点之和Master节点通信，master监控所有节点的信息，比如是map或者reduce任务，是否运行结束，占用的资源、文件读写速度等，master会重新分配那些已经完成的节点任务，对所有的错误的节点重新执行。

4. **节点出现错误如何解决？**

> 因为有master的存在，可以重新执行出现错误的运行节点，注意的是对于出错的map任务，其分配到的reduce任务也要重新执行。节点运行bug，我们可以修改代码，使其更鲁棒，但是有时候我们必须使用try-catch操作跳过一些错误的bad lines.

5. **Map和Reduce个数如何设置？**

> 这个设置和集群的个数和经验有很大关系，建议我们每一个map任务的输入数据16-64MB, 因此map的个数 = 总的文件大小 / 16-64MB. reduce的个数建议大于节点的个数，这样可以保证更好的并行计算。

6. **怎么控制负载平衡？**

> master会监控所有节点的运行状态，并且要对所有的运行完成的节点重新分配任务，来保证负载均衡，需要注意的是这里的并行计算是map和reduce的分别并行计算，必须保证map执行之后才能执行reduce(因为你有shuffle操作)。

7. **技巧**

> + map任务运行时候尽可能的读取本地或者当前局域内的文件，减少文件传输的网络带宽
> + M和R的设置会对master的监督有一定的影响，因为要监督所有的状态
> + 备份运行状态很重要，可以知道那台节点运行的缓慢，可能出现异常，可以让其他节点代替它运行任务
> + shuffle操作的hash函数真的很重要，可以有效的解决负载均衡
> + map生成的中间文件要根据key进行排序，也可以便于划分
> + map和reduce之间有时候需要加合并(combiner)操作,可以起到加速作用

### 参考

1. [MapReduce wikipedia ](https://en.wikipedia.org/wiki/MapReduce)
2. [MapReduce Paper](https://static.googleusercontent.com/media/research.google.com/en//archive/mapreduce-osdi04.pdf)

---

# 文件：开发\大数据\questions.md

---

# 面试题目

## 1. 相同URL

> **题目**: 给定a、b两个文件，各存放50亿个url，每个url各占64字节，内存限制是4G，让你找出a、b文件共同的url？

> 方案1：估计每个文件的大小为50G×64=320G，远远大于内存限制的4G。所以不可能将其完全加载到内存中处理。考虑采取分而治之的方法。
> <img src="../assert/big-data1.png">
> 遍历文件a，对每个url求取 hash(url)%1000[比如ASCII码值求和], 然后根据所取得的值将url分别存储到1000个小文件(记为a0, a1, … , a999)中。这样每个小文件的大约为300M。
> 
> 遍历文件b，采取和a相同的方式将url分别存储到1000个小文件(记为b0, b1, … , b999)。
> 
> 这样处理后，所有可能相同的url都在对应的小文件(a0 vs b0, a1 vs b1, … , a999 vs b999)中，不对应的小文件不可能有相同的url。然后我们只要求出1000对小文件中相同的url即可。
> 
> 求每对小文件中相同的url时，可以把其中一个小文件的url存储到hash_set中。然后遍历另一个小文件的每个url，看其是否在刚才构建的hash_set中，如果是，那么就是共同的url，存到文件里面就可以了。

> 方案2：如果允许有一定的错误率，可以使用Bloom filter，4G内存大概可以表示340亿bit。将其中一个文件中的url使用Bloom filter映射为这340亿bit，然后挨个读取另外一个文件的url，检查是否与Bloom filter，如果是，那么该url应该是共同的url（注意会有一定的错误率）。

## 2. Query排序

> **题目**: 有10个文件，每个文件1G，每个文件的每一行存放的都是用户的query，每个文件的query都可能重复。要求你按照query的频度排序。
> 
> 方案1：
> 顺序读取10个文件，按照hash(query)%10的结果将query写入到另外10个文件(r1,r2…r10)中。这样新生成的文件每个的大小大约也1G（假设hash函数是随机的）。
> 
> [2G左右的机器] 对r1,r2…r10用hash_map(query, query_count)来统计每个query出现的次数。利用快速/堆/归并排序按照出现次数进行排序。将排序好的query和对应的query_cout输出到文件中。这样得到了10个排好序的文件(r1,r2…r10).
> 
> 对(r1,r2…r10)这10个文件归并排序(内排序和外排序结合)
> <img src="../assert/bigdata2.png"/>
> 方案2：
> 一般query的总量是有限的，只是重复的次数比较多而已，可能对于所有的query，一次性就可以加入到内存了。这样，我们就可以采用trie树/hash_map等直接来统计每个query出现的次数，然后按出现次数做快速/堆/归并排序就可以了。
>  

## 3. Top k 单词

**题目**: 有一个1G大小的一个文件，里面每一行是一个词，词的大小不超过16字节，内存限制大小是1M。返回频数最高的100个词。

> 1. 顺序读文件，对每个词x，取hash(x)%5000，然后按照该值存到5000个小文件(x0, x1, … x4999)中。这样每个文件大概是200k左右，如果有的文件超过了1M大小，还可以按照类似的方法继续往下分，直到分解得到的小文件的大小都不超过1M。
> 2. 对每个小文件，统计每个文件出现的词及相应的频率（可以采用trie树/hash_map等），并取出现频率最大的100个词（可以用含100个结点的最小堆），并把100词及相应的频率存入文件，这样又得到了5000个文件。
> 3. 下一步就是把这5000个文件进行归并（类似与归并排序）的过程了。

## 4. IP统计 

> **题目**: 海量日志数据，提取出某日访问百度次数最多的那个IP。
> 
> 1. 定位到某日，并把访问百度的日志中的IP取出来，逐个写入到大文件中。注意IP是32位，最多有2^32个IP。-
> 2. 采用映射的方法，比如模1000，把整个大文件映射为1000个小文件
> 3. 找出每个小文件出现频率最大的IP（可以采用hash_map进行频率统计，然后再找出频率最大的几个）及相应的频率。
> 4. 然后再在这1000个最大的IP中，找出那个频率最大的IP，即为所求。

## 5. 不重复的整数

> **题目**: 在2.5亿个整数中找出不重复的整数，内存不足以容纳这2.5亿个整数。
> 
> 方案1：
> 采用2-Bitmap（每个数分配2bit，00表示不存在，01表示出现一次，10表示多次，11无意义）进行，共需内存2^32*2bit = 1G内存，还可以接受。
> 扫描这2.5亿个整数，查看Bitmap中相对应位，如果是00变01，01变10，10保持不变。所描完事后，查看bitmap，把对应位是01的整数输出即可。
> 
> 方案2：
> 
> 采用上题类似的方法，进行划分小文件的方法。然后在小文件中找出不重复的整数，并排序。然后再进行归并，注意去除重复的元素。

## 6. Top K

> **题目**: 海量数据分布在100台电脑中，想个办法高校统计出这批数据的TOP10。

> mapreduce还没有使用,是不是应该使用下mapreduce, 找key,定value.

# 参考

1. https://blog.csdn.net/u012289441/article/details/45192775
2. https://blog.csdn.net/v_july_v/article/details/6685962

---

# 文件：开发\大数据\Technology.md

---

# 相关技术

在解决海量数据的问题的时候使用的技术，但是注意这里只是从技术角度进行分析，只是一种思想并不代表业界的技术策略。
常用到的算法策略.

1. 分治：多层划分、MapReduce
2. 排序：快速排序、桶排序、堆排序
3. 数据结构：堆、位图、布隆过滤器、倒排索引、二叉树、Trie树、B树，红黑树
4. Hash映射：hashMap、simhash、局部敏感哈希

## 排序

> 排序:
> 　　将一组无序的集合，根据某个给定的条件，将其变成有序的方法就是排序。从这个我给出的不严谨的定义中排序是方法，目的是让原来无序的集合满足条件有序。
> 　　这里我们基于海量数据的考虑重新思考排序，不会详述每一种排序方法的原理，主要面向的是如何在海量数据情况下使用排序方法。

> 常用的排序方法:
> 　　插入排序，选择排序，冒泡排序，希尔排序，快速排序，归并排序，堆排序，桶排序，计数排序，基数排序。
> 下面给出几种排序算法的简单介绍图。
> 　　　　　<a href="https://ibb.co/cFNAoL"><img src="https://preview.ibb.co/in9ZF0/Screenshot-from-2018-11-05-14-04-03.png" alt="Screenshot-from-2018-11-05-14-04-03" border="0"></a>

既然有这么多的排序方法，我们可以直接读取数据到内存中直接调用语言中封装好的排序方法即可。但是数据量很大，不能将数据同时读入内存。
这就出现了所有的外排序，我们可以用归并排序的思想来解决这个问题，也可以基于数据范围用"计数排序"的思想来解决。排序真的很重要吗？我一直相信一句话:没有排序解决不了的问题。这里给出几个需求，例如:

+ 取最大的ｋ个数，直接降序排序取前ｋ个即可；
+ 推荐、搜索业务，我们也可以直接排序(精度不高)
+ 二分查找之前也要求数据有序

## 堆排序

> 在top k中我们用到了一个数据结构堆(有最大堆和最小堆)，这里就先介绍一下这个数据结构的性质，基于最大
> 堆进行介绍。堆是一个完全二叉树，对于任意的节点，我们可以使用数据来表示最大堆，设置下标从0开始, 满足以下性质:

+ root > left && root > right. (左右节点存在)
+ 根节点:root_index; 左孩子节点:left_index; 右孩子节点:right_index
+ left_index = root_index * 2 + 1
+ right_index = root_index * 2 + 2
+ root_index = (*_index - 1) / 2

在堆的数据结构进行增删改查的过程中，我们始终维护堆的数据结构，定义MaxheapFy(int *A, int i)表示维护第i个
节点满足最大堆的性质，注意这里没有考虑到泛型编程，正常应该提供一个比较方法的函数，让使用者自己设置比较方式。
从下面的伪代码中，我们可以知道对于一个大小为n的堆，维护一次堆的性质，最坏时间为O(logn)，但是必须保证
在改变之前，他是满足堆的性质的。

```
void MaxheapFy(int *A,int i) {
    // i 要在A的范围之内，
    assert(i >= 0);
    assert(i < n) // 堆的大小
    l = LEFT(i), r = RIGHT(i); // 得到左右子节点，如果存在
    now = i;

    // 找到左右孩子的最大值
    if(l<=heapsize&&A[l]>A[now]){
        now=l;//交换A[l]和A[i]，并递归维护下一个当前结点now
    }
    if(r<=heapsize&&A[r]>A[now]){
        now=r;//交换A[l]和A[i]，并递归维护下一个当前结点now
    }

    if(now != i) { // 交换，递归维护
        swap(A[i], A[now]);
        MaxheapFy(A, now);
    }
}
```

基于上面的这个维护的性质，我们可以直接对于长度为n的数组建立最大堆，我们知道当只有一个元素的时候，一定满足最大堆的性质，
基于这个性质，我们对于长度为n的数组A，从 n / 2向前维护每一个节点的性质，就可以得到最大堆.从下面给出的最大堆
的构建代码，我们可以分析建堆的时间复杂度是O(nlogn).因为每次维护是O(logn),维护n次，(这里计算时间复杂度的时候，忽略常数系数)。

```
void BuildMaxHeap(int *A,int n){//A[1..n]
    heapsize=n;//全局变量，表示最大堆的大小
    for(int i=n/2;i>=1;i--){//从n/2..1维护堆中每个节点的最大堆性质：结点的值大于起孩子的值
        MaxheapFY(A,i);
    }
}
```

建成最大堆之后，从最大堆的性质我们知道，A[0]一定是最大值，如果要堆A升序排序，就可以swap(A[0], A[n-1]);
继续维护A[0],直到堆中只是一个元素，这就完成了堆排序。从这个思路出发，对于top k问题，我们为什么要维护一个
最小堆呢，因为我们要过滤所有的数据，保证每次弹出一个最小值，之后剩下的k个一定是top k的最大值，但是这k个不一定
有序，如果需要我们可以堆这k进行任何排序，因为我们通过过滤，数据已经很少了，时间复杂度就是从n个中过滤出来k个。
首先任选k个构建最小堆, 时间复杂度O(klogk), 用最小堆过滤n-k个数字，每次维护堆的性质，时间O((n-k)logk).
总的时间复杂度O(klogk + (n-k)logk)。(注意当k多大时，我们不在使用堆的数据结构，这里留给读者计算)。

```
void HeapSort(int *A,int n){
    BuildMaxHeap(A,n);//建立最大堆
    for(int i=n;i>=2;i--){
        //cout<<A[1]<<" ";
        swap(A[1],A[i]);//交互A[1]和A[i],使得A[i]中为当前最大的元素
        heapsize--;//堆大小减去1，便于下次操作去掉已经排好序的元素
        MaxheapFY(A,1);//此时A[1]不一定满足最大堆的性质，重新维护下标1的最大堆的性质
    }
}
```

## 快速排序

> 快速排序是对冒泡排序的改进。通过一趟排序将要排序的数据分割成独立的两部分，其中一部分的所有数据都比另外一部分的所有数据都要小，然后再按此方法对这两部分数据分别进行快速排序，整个排序过程可以递归进行，以此达到整个数据变成有序序列。

**伪代码**

```
int partition(array,left,right){
    index = left;
    i = left+1, j = right;
    while(1) {
        while(i<j && array[i]<array[index]) ++i;
        while(i<j && array[j]>array[index]) ++i;
        if(i>j) break;
        else{
            swap(array[i],array[j]);
            ++i; --j;
        }
    }
    swap(array[index], array[j]);
    return j
}

// Qsort(array,0,n-1)
void Qsort(array, left, right){
  
    if(left < right) {
        mid = partition(array, left, right);
        Qsort(array, left, mid-1);
        Qsort(array, mid+1, right);
    }
}
```

快速排序在海量数据处理的过程中，一般不会直接使用，因为快速排序在基于内存的排序时，性能很好，是最常用的方法，例如我们对大数据进行划分后，可以对单个小文件应用快速排序。其实应用多还有就是快速排序中的一次划分很重要，比如我们有很多性别{男，女}，请将所有的女性放到男性的前面，我们只需要才有划分思想就OK了。

## 桶排序

> 桶排序的工作原理是将数据分装到有限数量的桶里，对每个桶分别进行排序，如果能将数据均匀分配，排序的速度将是很快的。

**伪代码**

```
bucket_sort(array):
    buckets[10]; // 申请10个桶
    for d in array:
        index = function(d) // 将d划分到每一个桶中
        buckets[index].append(index)
  
    // 对每一个桶分别进行排序
    for i in {1...10}:
        sort(buckets[i])
  
    // concat所有结果，这里是连接不是归并，
    // 我们划分的时候保证buckets[i] < buckets[i+1]
```

## 位图排序

> 使用提示，如何想到使用计数排序或者在海量数据处理方面使用计数排序的思想呢？如果我们知道所有的数字只出现一次，我们就可以只使用计算排序中的记录函数，将所有存在的值对应的位置设置为1，否则对应为0，扫描整个数组输出位置为1对应的下标即可完成排序。这种思想可以转为位图排序。

> 我们使用一个位图来表示所有的数据范围，01位串来表示，如果这个数字出现怎对应的位置就是1，否则就是0.例如我们有一个集合S = {1,4,2,3,6,10,7}; 注意到最大值10，用位图表示为1111011001，对应为1的位置表示这个数字存在，否则表示这个数字不存在。

**伪代码**

```
// step 1, 初始化为0
for(i = 0; i < n; i++){
    bit[i] = 0;
}

// step 2, 读取数据，对应设置为1
for d in all file_read:
    bit[d] = 1

// step3, 对应为1的位置写入文件
for(i = 0; i < n; i++) {
    if(bit[i] == 1) {
        write i to sort_file
    }
}
```

## 归并排序

> 归并排序: 建立在归并操作上的一种排序算法，该方法采用分治法的一个非常典型应用，一般我们都是使用二路归并，就是将数据划分成两部分进行处理，但是注意我们可以是多路归并，不要让二路归并排序限制我们的思想。
> 从下面的伪代码中，我们可以很容易看到二路归并排序只有两个部分，一个是递归划分，一个是归并操作，这就是我们最长用到的归并排序。但是在海量数据的排序过程中，我们可以使用二路归并，当然我也可以选择多路归并排序。

**伪代码:**

```
// 归并
merge(array, left, mid, right):
    tmp = new int[right - left + 1] // 申请复制空间
    i = left, j = mid+1, k = 0;
    while(i <= mid && j <= right) {
        if(array[i] < array[j]) tmp[k++] = array[i++];
        else tmp[k++] = array[j++];
    }
    // 处理尾部，可能会有一部分没有处理结束
    while(i <= mid) tmp[k++] = array[i++];
    while(j <= right) tmp[k++] = array[j++];
  
    // copy回到原来的数据, tmp -> array
    copy(array[left, right], tmp[0,k-1])

// 调用
merge_sort(array, left, right):
    if(left < right) {
        mid = (left + right) / 2;
        merge_sort(array, left, mid);
        merge_sort(array, mid+1, right);

        // 调用归并函数
        merge(arr, left, mid, right);
    }
```

## 倒排索引

> 倒排索引是一种索引方法，常用在搜索引擎中，这个数据结构是根据属性值来确定记录的位置。对于一批文档，我们的属性值就是关键字，对应值是包含该属性的文档的ID或者文化的位置。

例如:

---

T0 = {a,b,c}
T1 = {a,d}
T2 = {a,b,c,e}
----

*构建倒排索引*

a: {0,1,2}
b: {0,2}
c: {0,2}
d: {1}
e: {1}
--

> 检索的时候可以根据关键字的交集或者并集进行检索，可以看出，倒排索引就是正向索引的相反。原理其实很简单，可以通过学习或者问题的性质，来发现什么时候使用倒排碎索引，最重要的倒排索引怎么优化，在内存中和文件上如何分配，才能满足快速的检索。倒排索引的构建可以根据自己的业务，决定需要存储什么信息，但是属性值是确定的，对应的集合中可以保留出现的次数等信息。

## 字典树

> 字典树，Trie树是一种前缀树，我们之前也有介绍过，一般应用在快速查询中，例如搜索提示，当你输入前半部分，会提示后半部分的内容。字典树用一句话表示就是根据字符串的前缀构成的树结构。
> **格式定义**

```
template<typename T>
struct TreeNode {
    int flag; // {1,0}1:表示存在，0:表示不存在
    int count: // 表示这个字符串出现的次数
    struct TreeNode **childs; // 索引的孩子节点
    T value;
};
```

> 搜索字典项目的方法为：(来自百度百科)
> 
> 1. 从根结点开始一次搜索；
> 2. 取得要查找关键词的第一个字母，并根据该字母选择对应的子树并转到该子树继续进行检索；
> 3. 在相应的子树上，取得要查找关键词的第二个字母,并进一步选择对应的子树进行检索。
> 4. 迭代过程……
> 5. 在某个结点处，关键词的所有字母已被取出，则读取附在该结点上的信息，即完成查找。

<center><a href="https://imgbb.com/"><img src="https://image.ibb.co/jXM9qL/d62a6059252dd42a745cc2c2033b5bb5c9eab806.jpg" alt="d62a6059252dd42a745cc2c2033b5bb5c9eab806" border="0"></a><br /><a target='_blank' href='https://deleteacc.com/match'></a></center>

## 面试问题

1. 求top k, 可以用到堆数据结构.
   例如我们有100个有序(降序)的数组，现在从这个100个数组中找到最大的k个元素。这就是上述问题的抽象。使用100路归并(后面的归并排序)。
   用一个大小为k的最大堆，每次弹出一个最大值，记录是那个队列中的值，直到出现k个数，就结束。这里里面的两个思想，

+ 归并，不能处理的大问题，分成多个小问题并行处理，之后归并结果，比如外排序
+ 堆，帮助我们找到top k，k要相对n较小。

<a href="https://ibb.co/fLfEiL"><img src="https://preview.ibb.co/kDtpHf/Screenshot-from-2018-11-05-16-53-45.png" alt="Screenshot-from-2018-11-05-16-53-45" border="0"></a>

2. 中位数

> 在一个大小为10GB的文件中有一堆整数，乱序排列，要求找出中位数。内存限制2GB。
> 这个问题，我们可以使用外排序，并且记录元素的各种，最后得到中位数即可。这里我们使用**桶排序**的思想。

+ 将所有的数据根据前8位进行分桶，最多有255个桶，并且记录每个桶中元素的格式。这里的桶是文件表示。
+ 根据划分性质，我们有buckets[i] < buckets[i+1]; count[i]:个数
+ 如果sum{count{1,k}} < sum(count{1,n}) / 2 <= sum{count(1,k+1)},得到中位数在k+1个桶中，
  +将k+1个桶中读取内存(假设小于2GB，否则要根据次8位进行继续分桶)，找到第m个数字，sum{count{1,k}}+m对应的是中位数的下标。<center><a href="https://imgbb.com/"><img src="https://image.ibb.co/iTjffA/Screenshot-from-2018-11-08-16-46-28.png" alt="Screenshot-from-2018-11-08-16-46-28" border="0"></a><center>

3. 基于位图的排序

> 给你一个文件，里面有n个不重复的正整数，而且每一个数都小于等于n(10^7)。请最多使用1M的内存空间，对这个文件进行排序。
> 可以使用归并排序，但是时间应该慢，我们这里使用位图排序，$10^7 / 8 = 1.25Mb$, 我们只有1M内存空间，这里可以分成两个读取文件，$(1, 5*10^6)$和$(5*10^6, 10^7)$进行分开使用位图，空间占用0.625Mb.
> 
> <center><a href="https://imgbb.com/"><img src="https://image.ibb.co/hL92cq/Screenshot-from-2018-11-08-15-07-23.png" alt="Screenshot-from-2018-11-08-15-07-23" border="0"></a><center>

4. 大文件排序
   海量数据排序，使用归并排序的思想进行排序，例如我们现在有一个5G的数据文件，每一行有一个32位的正整数，现在要求只能使用1G的内存空间，对这个文件排序。
   我们有大数据处理的经验都知道，内存放不下，只能将大文件分成几个小文件，这就是划分，之后对每个文件进行排序，最后归并这几个小文件的排序结果，叫做多路归并。上述的过程可以叫做外排序，即借助外部的文件进行排序。

> 从这个题目出发我们使用之前介绍过的大数据处理技术完成这个排序过程。
> 
> 1. 划分成5个小文件，5G / 1G = 5
> 2. 将单个文件读入内存，进行排序，写入文件
> 3. 使用5路归并，将每个文件作为一路排序，归并最后得到结果

<center> <a href="https://ibb.co/ma0ifA"><img src="https://image.ibb.co/hzxktV/Screenshot-from-2018-11-07-17-49-48.png" alt="Screenshot-from-2018-11-07-17-49-48" border="0"></a> </center>

5. 串的快速检索
   
   > 给出N个单词组成的熟词表，以及一篇全用小写英文书写的文章，请你按最早出现的顺序写出所有不在熟词表中的生词。在这道题中，我们可以用数组枚举，用哈希，用字典树，先把熟词建一棵树，然后读入文章进行比较，这种方法效率是比较高的。
6. “串”排序
   
   > 给定N个互不相同的仅由一个单词构成的英文名，让你将他们按字典序从小到大输出用字典树进行排序，采用数组的方式创建字典树，这棵树的每个结点的所有儿子很显然地按照其字母大小排序。对这棵树进行先序遍历即可。
7. 最长公共前缀
   
   > 对所有串建立字典树，对于两个串的最长公共前缀的长度即他们所在的结点的公共祖先个数，于是，问题就转化为当时公共祖先问题。
8. 搜索引擎
   
   > 一个搜索引擎执行的目标就是优化查询的速度：找到某个单词在文档中出现的地方。以前，正向索引开发出来用来存储每个文档的单词的列表，接着掉头来开发了一种反向索引。
   
   > 正向索引的查询往往满足每个文档有序频繁的全文查询和每个单词在校验文档中的验证这样的查询。实际上，时间、内存、处理器等等资源的限制，技术上正向索引是不能实现的。为了替代正向索引的每个文档的单词列表，能列出每个查询的单词所有所在文档的列表的反向索引数据结构开发了出来。随着反向索引的创建，如今的查询能通过立即的单词标示迅速获取结果（经过随机存储）。随机存储也通常被认为快于顺序存储。

# 参考

1. <a href="https://blog.csdn.net/fool_ran/article/details/44487137">数据结构: 构建和使用堆</a>
2. 《算法导论》第六章：堆排序
3. 《编程之美：面试与算法心得》

---

# 文件：数学\统计学\logic.md

---

# 逻辑题目

逻辑题目现在也是面试中常考的题目,也不清楚面试出这种题目的意义,可能就是考察
面试人员是否逻辑清晰. 这种题目没有什么好的方法,除非你见过原题,否则,只能根据
所给出的条件慢慢分析,尽量不要用常规思路,希望大家要跳跃思维. 如果实在不行就
给出一种解法,可能不是最优的,至少表示我们有逻辑.

## 1. 猜数字

> **题目**:
> 两人玩游戏,在脑门上贴数字(正整数>=1),只看见对方的,看不见自己的,而且两人的数字相差1.

```
以下是两人的对话：
A：我不知道
B：我也不知道
A：我知道了
B：我也知道了
问A头上的字是多少，B头上的字是多少.
```

**解析**: 只看见对方的,看不见自己的.

1. 假设A头上的数字是x(x>=1); B头上的数字是y(y>=1).
2. 条件1, A看到B头上的y,说不知道自己的是多少.
   确定y>=2, 这样的话x可以取两个值y+1,和y-1.
3. 条件2: B看到x后,说我也不知道,同理可以确定,x>=2, y取值是x-1和x+1.
4. 条件3, A说我知道了, 因为A的头上x取值是y-1和y+1, 尽然A说自己知道了, 肯定要排除y-1和y+1的其中的一个,现在有 x>=2, y>=2, 只有y=2的时候,y-1=1, 此时x只能是y+1=2, 如果x=1, B就可以直接知道自己的是2, A确定自己是3,之后,
5. 条件4, B说他也知道了, A是3, B是2和4,只有自己是2的时候,A才可能知道自己是3.

## 2. 握手

> **题目**:
> 五队夫妇甲、乙、丙、丁、戊举行家庭聚会,每一个人都可能和其他人握手,但夫妇之间绝对不握手,聚会结束时,甲先生问其他人: 各握了几次手?
> 得到的答案是: 0、1、2、3、4、5、6、7、8，试问：甲太太握了几次手? 确定一点每一对夫妇一定会有一个人和其他夫妇握手.

**解析**:
每一对夫妇一定会有一个人和其他夫妇握手. 所有一对夫妇握手的次数和一定是8.
记0、1…8这9个人分别为A0、A1…A8。
首先，A8和A0是夫妇。因为A0没有和其他任何人握手，而A8握了别家的所有人的手。
继续推导，A1和A7是夫妇。因为A1已经和A8握过1次手，A7必须和除了A0和自己配偶以外的所有人握手，因此，A1和A7只能是夫妇。
同理，A2和A6是夫妇，A3和A5是夫妇，
最后，A4和甲是夫妇。题目中4只出现一次，因而甲和甲的夫人都握了4次手。

## 3. 找出毒药

> **题目**:
> 实验室里有8瓶饮料，已知其中有且仅有一瓶有毒，小白鼠喝了有毒的饮料后，将会在24小时后毒发身亡。实验室的小李需要在24小时后知道有毒的饮料是哪瓶，他可以使用小白鼠试喝饮料，请问，小李最少需要用几只小白鼠试喝饮料?

**解析**:

将8个瓶子进行如下编码：
(000)_2=0
(001)_2=1
(010)_2=2
(011)_2=3
(100)_2=4
(101)_2=5
(110)_2=6
(111)_2=7
编码后的0/1位表示一个老鼠，0-7表示8个瓶子。按照3个二进制位中每位是否为1分类，即最低位为1的1、3、5、7号瓶子的药混起来给老鼠1吃，次低位为1的2、3、6、7号瓶子的药混起来给老鼠2吃，最高位为1的4、5、6、7号瓶子的药混起来给老鼠3吃.
24小时后，哪个老鼠死了，相应的位标为1。如最低老鼠1死了、次低老鼠2死了、最高老鼠3没死，那么就是011=5号瓶子有毒。
即：n只老鼠可以最多检验2^n个瓶子。所有8个饮料最多用三个小白鼠.

## 4. 坏鸡蛋

> **题目**: 有十二个鸡蛋，有一个是坏的(重量与其余鸡蛋不同)请问用天平最少称几次,才能称出哪个鸡蛋是坏的?

**解析**：

题目中没有说明坏的蛋是比好的蛋重还是轻。本题可以将鸡蛋分成三份，每份四只。为表述方便，将鸡蛋编号为1到12。
第一次，取1234放在天平的左端，5678放在天平的右端。天平有两种情况，平衡或不平衡。
1）先分析天平平衡的情况：若平，则重量不同的蛋在剩下的4个中。
第二次用天平，任意取3个1到8号中的蛋放在天平的左端，从9到12号蛋中任意取3个（例如9，10，11）放在另右端，又有两种情况，平衡或不平衡
若平衡，则12号蛋为重量不同的蛋，第三次用天平，把12号蛋和其他任意一蛋比较，可以知道是轻还是重.
若不平衡，则可知重量不同的蛋在9，10，11这3个蛋中，并且可以知道他比其他蛋重还是轻，第三次用天平，任意取其中2蛋（例如9，10）放在天平两端，若平衡，则剩下的蛋（11号蛋）为要找的蛋，若不平衡，根据前面判断的该蛋是比较轻还是重可以判断天平上的其中一个蛋为要找的蛋.
2）下面分析第一次天平不平衡的情况。那么有左端重或者右端重两种情况，不妨假设左端重（如果是右端重也是一样的）。
现在第二次用天平，从左端任意拿下3个蛋（例如123），从右端拿3个蛋（例如567）放到左端，再从第一次称时剩下的4个蛋中任意拿3个（例如9，10，11）到右端，这时天平会出现3种情况：a）左端重，b）平衡，c）右端重。我们一个一个来分析。
a）左端重，那么要找的蛋肯定是4号蛋或者8号蛋。第三次用天平，把其中一蛋（例如4号蛋）放在天平左端，任意取其余10个蛋中的一个蛋放在右端，又有3种情况：
一）若平衡，则8号蛋为要找的蛋，并且根据第二次用天平的结果，可知比其余蛋轻。
二）若左端重，则4号蛋为要找的蛋，并且比其余蛋重。
三）若右端重，则4号蛋为要找的蛋，并且比其余蛋轻。
b）平衡，那么要找的蛋在从左端拿下的三个蛋（1，2，3）中，由于第一次用天平左端重，所以可知这个蛋比其余的蛋重，接下了来的分析和前面的一样，不再重复。
c）右端重，那么要找的蛋在从右端移到左端的3个蛋（5，6，7）中，并且由天平第一次左端重，第二次右端重可知，该蛋比其他蛋轻，接下来的分析同前面一样。
所以，需要称重三次。

## 5. 测半径

> **题目**: 一个球、一把长度大约是球的直径2/3长度的直尺.你怎样测出球的半径?

**解析**:

<img src="../assert/r.jpe">

## 6. 过河

> **题目**: 有A、B、C、D四个人,要在夜里过一座桥。他们通过这座桥分别需要耗时1、2、5、10分钟,只有一支手电筒，并且同时最多只能两个人一起过桥. 请问,最短需要几分钟四人都能过桥?

**解析**:

A: 1
B: 2
C: 5
D: 10

1. AB过去(花费2分钟),A回来(花费1分钟),共1+2=3
2. CD过去,让花费时间相近的人一起走,可以降低时间的浪费(花费10分钟), B回来(花费2分钟),共10+2=12
3. AB一起过去(花费2分钟),ABCD全部过来共花费3+12+2=17分钟.

## 7. 称石头

> **题目**: 给你8颗小石头和一架天平，其中有7颗石头重量一样，另外一个比这7颗略重。请问在最坏情况下，最少要称重几次，才能把这颗较重的石头找出来?

**解析**:

分为332.进行称重

首先任取8个石子中的6个进行称重,天平两边都是3个石子.

1. 如果重量相等
   再称剩下的两个石子即可找出重的.(2次)
2. 如果不相等.
   取较重的一边的任意2个称重,如果相等则剩下的1个是重的,如不相等则较重的一个是要找的石子.(2次)

最少两次称重可以找出重的石头.

## 8. 倒水

> **题目**: 假设有一个池塘,里面有无穷多的水. 现有2个空水壶,容积分别为5升和6升. 问题是如何只用这2个水壶从池塘里取得3升的水.

**解析**:

1. 6升容器装满水, 将水把5升容器倒满, 则6升容器中剩下1升水.
2. 清空5升容器,并将6升容器中的1升水倒入5升容器中.
3. 6升容器装满水, 将水把5升容器倒满, 则6升容器中剩下2升水.
4. 清空5升容器,并将6升容器中的2升水倒入5升容器中.
5. 6升容器装满水, 将水把5升容器倒满, 则6升容器中剩下3升水.

## 9. 绳子时间

> **题目**: 烧一根不均匀的绳子要用一个小时，如何用它来判断半个小时? 烧一根不均匀的绳子，从头烧到尾总共需要1个小时。现在有若干条材质相同的绳子，问如何用烧绳子的方法来计时45分钟呢?:

**解析**:

1. 如何判断半个小时
   将根绳子两头同时点燃,绳子全部烧完,就是半个小时.
2. 如何计时45分钟,
   选择使用两个绳子A和B,将绳子A两头点燃,绳子B一头点燃.
   当绳子A烧完已经过去30分钟,此时点燃绳子B的另一端,直到绳子B烧完一共是45分钟.

## 10. 植树

> **题目**: 怎么样种植4棵树木,使其中任意两棵树的距离相等?

**解析**：　

从三维空间考虑,画出一个空间正四面体,使其所有的边的长度相同．
<img src="../assert/simain.jpg">

# 参考(copy)

1. https://blog.csdn.net/linjcai/article/details/80868385
2. https://www.julyedu.com/question/select/kp_id/1
3. https://www.cnblogs.com/pang951189/p/7439670.html

---

# 文件：数学\统计学\probability.md

---

# 概率题目

现在的面试中,大部分公司都会问道概率相关的问题,我们现在给出几道常见的概率问题.

## 1. 三角形问题

> **题目**: 给你一根铅笔,将铅笔折两次,组成三角形的概率是多大.

> **解析**:

设: 铅笔长度是1, 折两次之后,得到三条边,对应的长度分别是x,y,1-x-y.

1. 得到条件:
   0 < x < 1
   0 < y < 1
   0 < 1-x-y < 1
   计算得到面积是: S=1/2
2. 根据两边之和大于第三边,进行计算:
   x + y > 1-x-y => x + y > 1/2
   x + (1-x-y) > y => y < 1/2
   y + (1-x-y) > x => x < 1/2
   计算得到面积是: A=1/8
   做线性规划求解:
   第一步,根据1中的所有条件,画出中的取值面积S,
   第二步,根据2中的不等式,画出满足条件的面积A.
   最后的概率=A/S=(1/8) / (1/2) = 1/4.

方法二: (思路来自网友Summer)
排除存在的可能性，

第一次，x+y=1，假设y>x，如果选择y作为一条边肯定不满足，这时就排除了1/2，只能选x作为一个边。

第二次，从y中折出两条边，一定满足三边只和大于第三边，只能根据两边只差＞第三边进行排除。因为y＞x，一定是从y中的两个边之差＞x。假设从y中折一个a，一个y-a。计算，
y-a-a＞x，得到y＞x+2a，又因为x＜1/2，y＞1/2，
根据三个不等式得到排除概率1/4。

1-1/2-1/4，

## 2. 排列组合

> **题目**: 20个阿里巴巴B2B技术部的员工被安排为4排，每排5个人，我们任意选其中4人送给他们一人一本《effective c++》，那么我们选出的4人都在不同排的概率是多少?

> **解析**:

1. 从20个人中,任选4个,是C(20,4).
2. 4个人在不同排,即从每排中选中一个C(5,1)*C(5,1)*C(5,1)*C(5,1)
3. 所以四个人在不同的概率是 C(5,1)^4 / C(20,4)

## 3. 男女比例

> **题目**: 在一个世世代代都重男轻女的村庄里，村长决定颁布一条法律，村子里没有生育出儿子的夫妻可以一直生育直到生出儿子为止，假设现在村子上的男女比例是1:1，这条法律颁布之后的若干年村子的男女比例将会多少?

> **解析**:

还是1:1.
先验性的认为生男生女的自然概率相同，都是0.5；由于生育儿子后就不再生，所以，每个家庭都有且只有一个儿子。假定家庭数目为1，则S(男)=1。
有0.5的家庭一胎生男就停止生育；剩下的0.5的家庭，有0.25二胎生男则停止生育……，从而，每个家庭的女孩数目为：

$$
S(女)=\sum_{i=1}^{m}(\frac{1}{2})^i(i-1)=1
$$

## 4. 取球问题

> **题目**: 袋中有红球，黄球，白球各一个，每次任意取一个又放回，如此连续抽取3次，求下列概率值:
> 
> 1. 颜色不全相同
> 2. 颜色全相同
> 3. 颜色全不同
> 4. 颜色无红色

> **解析**:

1. 每次都取红球的概率是1/3, 如果都是3次都是红色概率则是: (1/3)*(1/3)*(1/3)=1/27
   所有颜色全相同的概率是3*(1/3)*(1/3)*(1/3)=1/9.
2. 颜色不全相同的概率: 1-颜色全相同的概率=8/9.
3. 颜色全不同:
   假设三次依次是红,黄,白: 概率是(1/3)*(1/3)*(1/3)=1/27
   颜色全排列是A(3,3)=6
   所有颜色全不同的概率是6*1/27 = 2/9
4. 无红色的概率:
   (2/3)*(2/3)*(2/3)=8/27

## 5. 等概率器

> **题目**: 已知一随机发生器，产生0的概率是p，产生1的概率是1-p，现在要你构造一个发生器，使得它产生0和1的概率均为1/2。(或者是非等概率硬币,也是一样的情况).

> **解析**:

找到等概率事件. 考虑连续产生两个随机数，结果只有四种可能：00、01、10、11，其中产生01和产生10的概率是相等的，均为p*(1-p)，于是可以利用这个概率相等的特性等概率地产生01随机数。
比如把01映射为0,10映射为1。于是整个方案就是：
产生两个随机数，如果结果是00或11就丢弃重来，如果结果是01则产生0，结果是10则产生1。

## 6. 再谈等概率器

> **题目**: 给你一个不均匀的骰子,1-6出现的概率都不相同,你也不知道每个面出现的概率,现在让你用这个骰子构造一个01发生器,使得01出现的概率都是1/2.

> **解析**:

方法1:
找到一个等概率事件,因为每一个面出现的概率都不知道,现在我们假设扔6次骰子,1-6分别出现一次为事件p,那么p这个序列的概率就是(p1*p2*p3*p4*p5*p6), 我们将这样构造

1. 所有以(1,2,3)开头的这样的序列p对应0;
2. 所有以(4,5,6)开头的这样的序列p对应1;
3. 每6次作为一个事件,不满足p序列的要求,这次实验就作废.

看起来0和1产生的概率都是1/2,都是有一个问题,我们需要扔很多次才能得到一次0或1.这种方法理论上可行,实际中不好用.

方法2:
0101:大于小于.
我们将扔两次骰子作为一个时间,假设第一是x,第二次是y.

1. x > y: 对应0
2. x < y: 对应1
3. x == y: 当x属于[1,2,3]时对应0, 否则对应1.

各个面出现的概率不同,这个满足要求吗?
11 12 13 14 15 16
21 22 23 24 25 26
31 32 33 34 35 35
41 42 43 44 45 46
51 52 53 54 55 56
61 62 63 64 65 66

可以看出,左下对应0,右上对应1. 而且出现的次数相同.

## 7. 吃苹果

> **题目**: 有一苹果两个人抛硬币来决定谁吃这个苹果先抛到正面者吃。问先抛者吃到苹果的概率是多少？

> **解析**:
> 先抛者A吃苹果, 后者是B:
> A(第一次)吃: 1/2
> A(第二次)吃: 1/2(!A)*1/2(!B)*1/2(A)=1/8
这是一个等比数列,公比是1/4, 首项是1/2.
求解的(1/2)*(1-(1/4^n)) / (1-1/4) = (1/2)/(3/4) = 2/3.

## 8. 蚂蚁爬三角形

> **题目**: 一个三角形， 三个端点上有三只蚂蚁，蚂蚁可以绕任意边走，问蚂蚁不相撞的概率是多少?

> **解析**:

1.每个蚂蚁在方向的选择上有且只有2种可能，共有3只蚂蚁，所以共有2的3次方种可能
2.不相撞有有2种可能，即全为顺时针方向或全为逆时针方向。
不相撞概率=不相撞/全部=2/8

## 9. 正确的概率

> **题目**: 甲乙两个人答对一道题的概率分别为90%和80%，对于一道判断题，他们都选择了“正确”，问这道题正确的概率.

> **解析**:

设:
甲的选择是"正确"的,是事件A.
乙的选择是"正确"的,是事件B.
这道题是正确的是事件C.
则有:

$$
P(A|C)=0.9 \tag{1}
$$

$$
P(B|C)=0.8 \tag{2}
$$

目标是求: P(C|AB), 根据贝叶斯公式有:

$$
P(C|AB)=\frac{P(AB|C)*P(C)}{P(AB|C)*P(C)+(AB|\bar{C})*P(\bar{C})} \tag{3}
$$

可以认为A和B是独立事件.则有:

$$
P(AB|C)=P(A|C)*P(B|C)=0.72
$$

$$
P(AB|{C})=P(A|{C})*P(B|\bar{C})=(1-0.9)*(1-0.8)=0.02
$$

根据实际情况,一道题对或者错的概率是0.5. 则公式3的结果是:

$$
\frac{0.72*0.5}{0.72*0.5+0.02*0.5}=\frac{36}{37}
$$

## 10. 和超过1的个数

> **题目**: 从(0,1)中随机取数,期望情况下取多少个数才能让和超过1.

> **解析**:

<img src="../assert/prb10.gif">

# 参考

1. https://www.julyedu.com/question/selectAnalyze/kp_id/6/cate/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1
2. https://blog.csdn.net/huazhongkejidaxuezpp/article/details/73662357
3. https://www.cnblogs.com/sunflower627/p/4839031.html
4. http://www.voidcn.com/article/p-afkjgouj-qm.html
5. https://blog.csdn.net/rudyalwayhere/article/details/7349957

---

# 文件：数据结构与算法\Array.md

---

# 数组（Array）

> 面试中最常见的就是围绕数组进行出题，主要原则数组可以随机读取，一般遇到数组相关的题目，都不是直观看到的那样。第一步暴力解法，第二步是否可以排序，是否可以二分，是否可以使用数据结构（哈希表，队列，栈等）。
> 要时刻注意一个数组中有两列数，一列是给定的数组的值，另一个是数组的下标。

## 1. two sum

> **题目**: 给你一个数组arr，和一个目标值target，找到一组下标（i，j）使得arri = target。
> 进阶: 数组中有重复的值，找到所有可能的下标组合。
> 例如arr = [1, 2, 3, 3, 2, 4] target = 5。 return {(0, 5), (1, 2), (1, 3), (2, 4), (3, 4)}。
> **公司**: 各大公司

```
pair<int, int> twosum(int *arr, int n, int target) {
    map<int, int> hash;
    pair<int, int> result;

    for(int i = 0; i < n; i++) {
        if(hash.find(arr[i]) == hash.end()) {
            hash[target - arr[i]] = i;
        }
        else {
            result = make_pair(hash[arr[i]], i);
            break;
        }
    }
    return result;
}

vector<pair<int, int> > twosumplus(int *arr, int n, int target) {
    map<int, vector<int> > hash;
    for(int i = 0; i < n; i ++) {
        if(hash.find(arr[i]) == hash.end()) {
            vector<int> tmp;
            tmp.push_back(i);
            hash[arr[i]] = tmp;
        }
        else {
            hash[arr[i]].push_back(i);
        }
    }
    vector<pair<int, int> > results;
    for(int i = 0; i < n; i ++) {
        map<int, vector<int>>::iterator it = hash.find(target - arr[i]);
        if(it != hash.end()) {
            for(int j = 0; j < hash[target-arr[i]].size(); j ++) {
                for(int k = 0; k < hash[arr[i]].size(); k ++) {
                    // 去除 3 + 3 = 6，使用两次同一个3
                    if(target - arr[i] == arr[i] && k <= j) {
                        continue;
                    }
                    int x = min(hash[target-arr[i]][j], hash[arr[i]][k]);
                    int y = max(hash[target-arr[i]][j], hash[arr[i]][k]);
                    results.push_back(make_pair(x, y));
                }
            }
            hash.erase(it);
        }
    }
    return results;
}
```

## 2. 查找旋转数组

> **题目**: 排序数组（没有重复元素）根据未知旋转轴排序（例如，0 1  2 3 4 5 变成 3 4 5 0 1  2。给定一个目标值进行搜索，如果存在返回下标，不存在返回-1。
> **公司**: 百度，头条等

```
int find(vector<int> &arr, int l, int r, int target) {
    if(l > r || arr.size() == 0) return -1;
    int index = -1;
    while(l <= r) {
        int mid = (l + r) / 2;
        if(arr[mid] == target) {
            index = mid;
            break;
        }
        else if(arr[mid] < target) {
            if(arr[r] >= target) l = mid + 1;
            else r = mid - 1;
        }
        else { // arr[mid] > target
            if(arr[l] <= target) right = mid - 1;
            else left = mid + 1;
        }
    }
    return index;
}
```

## 3 主元素

> **题目**: 给你一个证书数组，其中有一个数字出现了超过1/2，这个数就是主元素，请找出这个数字。
> 扩展1: 找到一个主元素，它出现的次数严格大于数组个数的1/3.
> **公司**: 百度，京东

```
// 每次去掉两个数，剩下的那个就是主元素
int MainElem(int *arr, int n) {
    int mainelem = arr[0];
    int cnt = 1;
    for(int i = 1; i < n; i ++) {
        if(mainelem == arr[i]) {
            cnt ++;
        }
        else {
            cnt --;
            if(cnt == 0) {
                mainelem = arr[i];
                cnt = 1;
            }
        }
    }
    return mainelem;
}

// 每次去掉三个数，选择两个候选集合
int MainElemP(int *arr, int n) {
    int maina, mainb, cnta, cntb;
    cnta = cntb = 1;
    maina = arr[0];
    mainb = arr[1];
    for(int i = 2; i < n; i ++) {
        if(arr[i] == maina) cnta ++;
        else if(arr[i] == mainb) cntb ++;
        else if(cnta == 0) {
            maina = arr[i];
            cnta = 1;
        }
        else if(cntb == 0) {
            mainb = arr[i];
            cntb = 1;
        }
        else { // 去掉三个数
            cnta --;
            cntb --;
        }
    }
    int cnt = 0;
    for(int i = 0; i < n; i ++) {
        if(minas == arr[i]) cnt ++;
    }
    if(cnt >= n / 3) return maina;
    else return mainb;
}
```

## 4. 落单的数

> **题目**: 2n+1个数，其中只有一个数出现了一次，其他都出现了两次，求出这个出现一次的数。
> 扩展1: 2n+2个数，其中有两个出现一次，其他出现两次，求这两个出现一次的数。
> 扩展2: 3n+1个非负数，只有一个数出现了一次，其他都出现了三次，求出现一次的数。        **公司**: 常见的题目

```
// xor: a ^ 0 = a, a ^ a = 0
int SingleNumber(int *arr, int n) {
	int ans = 0;
	for(int i = 0; i  < n; i ++) {
		ans = ans ^ arr[i]
	}
	return ans;
}
// 找到a和b, 和上一个类似，
pair<int, int> SingleNumberP(int *arr, int n) {
    int c = 0;
    for(int i = 0; i  < n; i ++) {
        c = c ^ arr[i]
    }
    // c = a ^ b, 确定c的位数是1的位置。
    int k = 0;
    while(c) {
        if(c & 1) break;
        k ++;
        c =>> 1;
    }
    // 根据1的位置不同确定a和b
    int a = 0, b = 0;
    for(int i = 0; i < n; i ++) {
        if((arr[i]>>k) & 1) {
            a ^= arr[i];
        }
        else {
            b ^= arr[i];
        }
    }
    return make_pair(a, b);
}
// 非负整数，记录所有位的1的个数，对3取余即可
int SingleNumberPP(int *arr, int n) {
    int res = 0;
    for(int j = 0; j < 32; j ++) {
        int bits = 0;
        for(int i = 0; i < n; i ++) {
            bits += (arr[i] >> j) & 1;
        }
        ans |= (bits % 3) << j;
    }
    return ans;
}
```

## 5. 中位数

> **题目**: 给定两个有序数组，找到这两个数组合并排序后的中位数。
> **公司**: 360，阿里

```
double findMedianSortedArrays(vector<int> &A, vector<int> &B) {
    // do A.size() < B.size()
    if(A.size() > B.size()) swap(A, B);
    int lena = A.size();
    int lenb = B.size();
    int la = 0, ra = lena, ma, mb, madian;
    while(la <= ra) {
        ma = (la + ra) / 2;
        mb = (lena + lenb + 1) / 2 - ma;
        if(ma < lena && mb > 0 && B[mb-1] > A[ma]) {
            la = ma + 1;
        }
        else if(ma > 0 && mb < lenb && B[mb] < A[ma-1]) {
            ra = ma - 1;
        }
        else {
            if(ma == 0) median = B[mb - 1];
            else if(mb == 0) median = A[ma - 1];
            else {
                median = max(A[ma - 1], B[mb - 1]);
            }
            break;
        }
    }
    if((lena + lenb) % 2 == 1) {
        return double(median);
    } 
    if(ma == lena) {
        return (median + B[mb]) / 2.0;
    }
    if(mb == lenb) {
        return (median + A[ma]) / 2.0;
    }
    return (median + min(A[ma], B[mb])) / 2.0;
}
```

## 6. 二维数组中的查找

> **题目**: 给你一个每一行每一列都有序的二维数组，给定一个target，查找这个值是否在二维数组中。
> 扩展1: 计算target出现的次数。
> **公司**: 常见的题目

```
// 查找是否存在
bool SearchMatrix(vector<vector<int> > matrix, int target) {
    if(matrix.size() == 0) return false;
    int m = matrix.size(), n = matrix[0].size();
    int mid, low = 0, high = n * m - 1;
    while(low <= high) {
        mid = (low + high) / 2;
        int r = mid / n;
        int c = mid % n;
        if(matrix[r][c] == target) {
            return true;
        }
        else if(matrix[r][c] < target) {
            low = mid + 1;
        } 
        else {
            high = mid - 1;
        }
    }
    return false;
}

// 查找次数
int SearchMatrixP(vector<vector<int> > matrix, int target) {
    if(matrix.size() == 0) return 0;
    int m = matrix.size(), n = matrix[0].size();
    int i = m - 1, j = 0, cnt = 0;
    while(i >= 0 && j < n) {
        if(target == matrix[i][j]) {
            cnt ++;
            j ++;
        }
        else if(target > matrix[i][j]) {
            j ++;
        }
        else {
            i --;
        }
    }
    return cnt;
}
```

## 7. 构建乘积数组

> **题目**: 给定一个数组A0, 1, ..., n-1, 其中B中的元素Bix...xAi-1x...xA[n-1]。
> **公司**: 某创业公司

```
// 从计算中可以直接使用数组的前缀和后缀乘积
vector<int> multiply(const vector<int>& arr1) {
    vector<int> arr2(arr1.size(), 0);
    // 计算前缀
    arr2[0] = 1;
    for(int i = 1; i < arr1.size(); i ++) {
        arr2[i] = arr2[i - 1] * arr1[i];
    }
    // 计算后缀
    int temp = 1;
    for(int i = arr1.size() - 2; i>= 0; i --) {
        temp *= arr1[i+1];
        arr2[i] *= temp;
    }
    return arr2;
}
```

## 8. 滑动窗口的最大值

> **题目**: 给定一个数组array和滑动的大小k，求所有滑动窗口里的最大值。
> **公司**: 头条

```
// 滑动窗口最大值
vector<int> MaxSildingWindow(vector<int> nums, int k) {
    deque<int> q;
    vector<int> ans;
    if(k <= 0) return ans;
    if(k == 1) return nums;

    for(int i = 0; i < k; i ++) {
        while(!q.empty() && nums[q.back()] <= nums[i]) {
            q.pop_back();
        }
        q.push_back(i);
    }
    for(int i = k; i < nums.size(); i ++) {
        ans.push_back(nums[q.front()]);
        while(!q.empty() && nums[q.back()] <= nums[i]) {
            q.pop_back();
        }
        if(!q.empty() && i - q.front >= k) { // 超过窗口大小
            q.pop_front();
        }
        q.push_back(i);
    }
    // 处理结尾
    ans.push_back(nums[q.front()]);
    return ans;
}
```

## 9. 第k小(大)的数

> **题目**: 给你一个无序数组，找出第k小的数。
> **公司**: 常见题目

```
int randPartition(int *arr,int l,int r){
    int x = arr[l];
    int i=l+1,j=r;
    while(i<j){
        while(arr[i]<x && i<r) i++;
        while(arr[j]>=x && j>l) j--;
        if(i<j){
            swap(arr[i],arr[j]);
            i++;
            j--;
        }
    }
    // 交换最后一个位置
    swap(arr[l],arr[j]);
    return j;
}
int kthSmallest(int *arr,int l,int r,int k){
    if(k<0 || k > r-l+1) return -1;
    int pos = randPartition(arr,l,r);
    // 使用相对位置
    if(pos-l+1 == k) return arr[pos];
    else if(pos-l+1 > k) return kthSmallest(arr,l,pos-1,k);
    else{//在右边，因为使用的是相对位置，所以k要减去左边丢弃的数的个数
        return kthSmallest(arr,pos+1,r,k-(pos-l+1));
    }
}
int findKElem(int *arr, int n, int k) {
    // 根据快速排序的思想，使用快排的一次划分
    if(n < 0 || arr == NULL) return -1;
    return kthSmallest(arr, 0, n-1, k);
}
```

## 10. 奇偶排序

> **题目**: 给你一个数组，将所有的偶数排列奇数的前面。
> **公司**: 搜狐

```
void oddEvenSort(vector<int> &arr) {
    int i = 0, j = arr.size() - 1;
    while(1) {
        while(i < j && a[i] % 2 == 0) i ++;
        while(i < j && a[j] % 2 == 1) j --;
        if(i > j) break;
        swap(arr[i], arr[j]);
        i ++, j --;
    }
}
```

---

# 文件：数据结构与算法\binaryTree.md

---

# 二叉树(bibary Tree)

> 二叉树是面试中最容易被问道的问题，这里同样给出高频而且有代表性的10道题目。二叉树介绍:
> 
> 1. <a href="https://baike.baidu.com/item/%E4%BA%8C%E5%8F%89%E6%A0%91">百度百科:二叉树</a>
> 2. <a href="https://en.wikipedia.org/wiki/Binary_tree">wikipedia: binary Tree</a>

定义二叉树:

```
struct TreeNode {
    int data;
    TreeNode *left, *right;
    TreeNode(){}
    TreeNode(int _data, TreeNode* _left, TreeNode* _right):data(_data), left(_left), right(_right){}
};
```

## 1. 二叉树的遍历

> **题目**: 给出二叉树的层次遍历, 前序, 中序, 后序 遍历.
> **扩展**: 前序遍历的迭代形式,希望大家自行手写中序和后序的迭代代码, 很多公司会问道非递归代码.

```
// 前序遍历: 根 -> 左 -> 右 [感谢wbzhang233(https://github.com/wbzhang233)同学提出问题]
void printPostorder(struct TreeNode* node) { 
    if (node == NULL) 
        return;   
    // 根
    cout << node->data << "";
    // 左
    printPostorder(node->left); 
    // 右
    printPostorder(node->right);   
} 
// 中序遍历: 左 -> 根 -> 右  
void printInorder(struct TreeNode* node) { 
    if (node == NULL) 
        return; 
    // 左
    printInorder(node->left);   
    // 根
    cout << node->data << " ";   
    // 右
    printInorder(node->right); 
} 
// 后序遍历: 左 -> 右 -> 根 [感谢wbzhang233(https://github.com/wbzhang233)同学提出问题]
void printPreorder(struct TreeNode* node) { 
    if (node == NULL) 
        return; 
    // 左
    printPreorder(node->left);  
    // 右
    printPreorder(node->right);
    // 根
    cout << node->data << " ";
}  

// 层次遍历 
void printLevelOrder(struct TreeNode* node) {
    queue<TreeNode *> q;
    if(!node) q.push(node);
    while(!q.empty()) {
        // 当前的长度是上一层的个数,这一点很重要,可以解决很多层次遍历相关的问题
        int len = q.size(); 
        for(int i = 0; i < len; i ++) {
            TreeNode * tmp = q.top();
            q.pop();
            cout << tmp->data << " ";
            if(tmp->left) q.push(tmp->left);
            if(tmp->right) q.push(tmp->right);
        }
    } 
}

// 迭代的前序遍历, root left right
void iterativePreorder(struct TreeNode *root) {
    if(root == NULL) return;
    stack<TreeNode *> sta;
    sta.push(root);
    while(!sta.empty()) {
        // 注意先进后出, 所以先right后left
        TreeNode * tmp = sta.top();
        sta.pop();
        cout << tmp->data << " ";
        if(tmp->right) sta.push(tmp->right);
        if(tmp->left) sta.push(tmp->left);
    }
}
```

## 2. 二叉树的Z型遍历

> **题目**: 二叉树的Z型遍历.
> **扩展**: 层次遍历的从下到上遍历, 层次遍历的奇数层遍历, 层次遍历的从右到左遍历等,都可以使用这个代码进行变形

```
/***
    3
   / \
  9  20
    /  \
   15   7
Z型遍历: 3, 20, 9, 15, 7
**/
vector<int> printLevelOrder(struct TreeNode* node) {
    queue<TreeNode *> q;
    vector<int> ans;
    stack<int> sta;
    if(!node) q.push(node);
    int k = 1;
    while(!q.empty()) {
        // 当前的长度是上一层的个数,这一点很重要,可以解决很多层次遍历相关的问题
        int len = q.size(); 
        for(int i = 0; i < len; i ++) {
            TreeNode * tmp = q.top();
            q.pop();
            if(k % 2 == 1) {
                ans.push_back(tmp->data);
            }
            else {
                sta.push(tmp->data);
            }
            if(tmp->left) q.push(tmp->left);
            if(tmp->right) q.push(tmp->right);
        }
        if(k % 2 == 0) {
            while(!sta.empty()) {
                ans.push_back(sta.top());
                sta.pop();
            }
        }
        k ++;
    } 
    return ans;
}
```

## 3. 平衡二叉树

> **题目**: 给出一个二叉树,判断是否是平衡二叉树.
> 一棵高度平衡的二叉树的定义是：一棵二叉树中每个节点的两个子树的深度相差不会超过1。
> **扩展**: 二叉树的最大高度, 也是使用类似的递归思想, 二叉树的最大宽度是使用层次遍历,

```
// 二叉树的最大高度
int getHeight(TreeNode *root) {
    if(root == NULL) return 0;  
    return max(getHeight(root->left),getHeight(root->right))+1;      
} 
bool isBalanced(TreeNode *root) {
    if(root == NULL) return true; 
    if(abs(getHeight(root->left) - getHeight(root->right))>1){
        return false;
    }
    return isBalanced(root->left) && isBalanced(root->right);   
}
```

## 4. 前序遍历的第k个结点

> **题目**: 给个二叉树, 找到其前序遍历的第k个结点.
> **扩展**: 中旬的遍历的第k个结点, 前序遍历的结点a的前一个结点 等

```
TreeNode* KthPostordernode(struct Node* root, int k) { 
    static int flag = 0; 
    if (root == NULL) 
        return; 
    if (flag <= k) { 
        kthPostordernode(root->left, k); 
        kthPostordernode(root->right, k); 
        flag++; 
        if (flag == k) return root;   
    }
}
```

## 5. 二叉树的对角线遍历

> 题目: 根据对角线顺序遍历二叉树.
> 扩展: 根据垂线从左到右遍历二叉树.

> 输入:
> <img src="../assert/d1-1.png"/>

```
输出: 
 8 10 14
 3 6 7 13
 1 4
```

> 我们从右上向左下看进行层次划分,可以看出, root和root->right都是同一层, root->left是下一层, 我们可以使用map,将层数作为key, 每一层对应的节点作为vector<TreeNode *>作为values, 最后打印出来map中的值即可.

```
void diagOrderUtil(Node* root, int d, map<int, vector<int>> &diagVec) { 
    if (!root) 
        return; 
    diagVec[d].push_back(root->data); 
    diagOrderUtil(root->left, d+1, diagVec); 
    diagOrderUtil(root->right, d, diagVec) 
}
void diagOrder(Node* root) { 
    map<int, vector<int> > diagVec; 
    diagOrderUtil(root, 0, diagVec);   
    cout << "Diagonal Traversal of binary tree: \n";
    for (auto it = diagVec.begin(); it != diagVec.end(); ++it) { 
        for (auto itr = it->second.begin(); itr != it->second.end(); ++itr) {
            cout << *itr  << ' '; 
        }  
        cout << 'n'; 
    } 
}
```

## 6. 构造二叉树

**题目**: 给出二叉树的前序和中序遍历,构造二叉树.
**扩展**: 其他几种构造二叉树的方式,建议多熟练掌握.
中旬: [1,2,3]
前序: [2,1,3]
return {2,1,3}.
知道一点,前序的第一个A是根节点, 在中序中找到前序的第一个节点A,就可以将中序分成左右两个子树,只有进行递归即可,

```
class Solution {
    /**
     *@param preorder : A list of integers that preorder traversal of a tree
     *@param inorder : A list of integers that inorder traversal of a tree
     *@return : Root of a tree
     */
public:
    typedef vector<int>::iterator Iter;
    TreeNode *buildTreeRecur(Iter istart, Iter iend, Iter pstart, Iter pend)
    {
        if(istart == iend)return NULL;
        int rootval = *pstart;
        Iter iterroot = find(istart, iend, rootval);
        TreeNode *res = new TreeNode(rootval);
        res->left = buildTreeRecur(istart, iterroot, pstart+1, pstart+1+(iterroot-istart));
        res->right = buildTreeRecur(iterroot+1, iend, pstart+1+(iterroot-istart), pend);
        return res;
    }
    TreeNode *buildTree(vector<int> &preorder, vector<int> &inorder) {
        return buildTreeRecur(inorder.begin(), inorder.end(), preorder.begin(), preorder.end());
    }
};
```

## 7. 对称二叉树

> **题目**: 判断给定的二叉树是否是对称二叉树.
> **扩展**: 二叉树的镜像

```
1
   /   \
  2     2
 / \   / \
3   4 4   3
是对称的, True

    1
   / \
  2   2
   \   \
   3    3
不是对称的, False
```

```
// 比较两个二叉树是否互为镜像
bool isMirror(struct Node *root1, struct Node *root2) { 
    if (root1 == NULL && root2 == NULL) 
        return true;   
    if (root1 && root2 && root1->key == root2->key) 
        return isMirror(root1->left, root2->right) &&
               isMirror(root1->right, root2->left); 
    return false; 
} 

// 是不是自身互为镜像,就是对称的二叉树  
bool isSymmetric(struct Node* root) 
{ 
    return isMirror(root, root); 
}
```

## 8. 最近公共祖先

> **题目**: 给定两个节点a和b,找出a和b的最近公共祖先.
> 最近公共祖先有很多方法,这里给出最好理解一个方法, 分别找到a和
> b所有祖先, 进行比较. 这里我们需要找到从root到A的路径,和从root到B的路径, 进行比较即可.
> **扩展**: 两个节点a和b的最近距离.
> a和b之间的距离, 我们可以先从a--root, root--b, 主要这个时候,多走了很多无用路径, 其实可以 a--lca(a,b)--b,这是最短路径, a--lca(a,b)--b = a--root--b - 2*lca(a,b)

```
bool findPath(Node *root, vector<int> &path, int k) { 
    if (root == NULL) return false;   
    path.push_back(root->key);   
    if (root->key == k) 
        return true; 
    if ( (root->left && findPath(root->left, path, k)) || 
         (root->right && findPath(root->right, path, k)) ) 
        return true; 
    path.pop_back(); 
    return false; 
} 
int findLCA(Node *root, int a, int b) { 
    vector<int> patha, pathb; 
    if (!findPath(root, patha, n1) || !findPath(root, pathb, n2)) return -1;   
    int i; 
    for (i = 0; i < path1.size() && i < pathb.size(); i++) {
        if(patha[i] != pathb[i]){
            return patha[i-1];
        }
    } 
    return -1;
}
```

## 9. 寻找树中最左下结点的值

> **题目**: 给定一棵二叉树，找到这棵树最中最后一行中最左边的值.
> **扩展**: 最右边的值.
> **解释**: 使用深度优先搜索dfs，当我们第一次访问一个深度为depth的节点x（之前只访问过深度小于depth的节点）时，x一定是depth深度的最左节点，用这个节点更新Ans。即我们维护一个最大深度，当遍历到一个点的深度大于最大深度时，用这个节点来更新答案，并更新最大深度即可。时间复杂度O(n)。

```
int findBottomLeftValue(TreeNode * root) {
    int ans_data = 0, ans_depth = 0;
    return findBottomLeftValue(root, 1, ans_data, ans_depth);
}
int findBottomLeftValue(TreeNode * root, int depth, int &ans_data, int &ans_depth) {
    if (ans_depth < depth) {
        ans_data = root->val;
        ans_depth = depth;
    }
    if (root->left) findBottomLeftValue(root->left, depth+1, ans_data, ans_depth);
    if (root->right) findBottomLeftValue(root->right, depth+1, ans_data, ans_depth);
    return ans_data;
}
```

## 10. 二叉树的最长连续子序列

> **题目**: 给一棵二叉树，找到最长连续路径的长度。
> 这条路径是指 任何的节点序列中的起始节点到树中的任一节点都必须遵循 **父-子** 联系。最长的连续路径必须是从父亲节点到孩子节点（不能逆序）。

```
样例1:

输入:
{1,#,3,2,4,#,#,#,5}
输出:3
说明:
这棵树如图所示
   1
    \
     3
    / \
   2   4
        \
         5
最长连续序列是3-4-5，所以返回3.
样例2:

输入:
{2,#,3,2,#,1,#}
输出:2
说明:
这棵树如图所示：
   2
    \
     3
    / 
   2  
  / 
 1
最长连续序列是2-3，而不是3-2-1，所以返回2.
```

```
void longestConsecutiveUtil(Node* root, int curLength, int expected, int& res) { 
    if (root == NULL) 
        return;   
    if (root->data == expected) 
        curLength++; 
    else
        curLength = 1;   
    res = max(res, curLength);   
    longestConsecutiveUtil(root->left, curLength, 
                           root->data + 1, res); 
    longestConsecutiveUtil(root->right, curLength, 
                           root->data + 1, res); 
}
```

**扩展**: 给定一棵二叉树，找到最长连续序列路径的长度(节点数)。
路径起点跟终点可以为二叉树的任意节点。

```
例1:

输入:
{1,2,0,3}
输出:
4
解释:
    1
   / \
  2   0
 /
3
0-1-2-3
例2:

输入:
{3,2,2}
输出:
2
解释:
    3
   / \
  2   2
2-3
```

```
class Solution {
public:
    int longestConsecutive2(TreeNode * root) {
        // write your code here
        int res = 0;
        helper(root, root, res);
        return res;
    }  
    pair<int, int> helper(TreeNode* node, TreeNode* parent, int& res) {
        if (!node) return {0, 0};
        auto left = helper(node->left, node, res);
        auto right = helper(node->right, node, res);
        res = max(res, left.first + right.second + 1);
        res = max(res, left.second + right.first + 1);
        int inc = 0, dec = 0;
        if (node->val == parent->val + 1) {
            inc = max(left.first, right.first) + 1;
        } else if (node->val + 1 == parent->val) {
            dec = max(left.second, right.second) + 1;
        }
        return {inc, dec};
    }
};
```

## 11. 左边看到的二叉树结点

> 题目: 给你一个二叉树,打印出来从左边视角看到的所有结点.

```
Input 1: 
                 1
               /   \
              2     3
             / \     \
            4   5     6           
Output 1: 1 2 4

Input 2:
        1
      /   \
    2       3
      \   
        4  
          \
            5
             \
               6
Output 2: 1 2 4 5 6
```

> 解析:
> 方法一: 用我们上面提到的层次遍历, 打印出来每一层的第一个结点即可.
> 方法二: 维护一个从左到右的最大等级, 如果当前等级大于最大等级,则是左边看到的,否则不是.
> 这里只给出第二种方法的代码.

```
// leftView(root, 1, 0, ans)
void leftView(struct node *root, int level, int &max_level, vector<int> &ans) { 
    if (root==NULL)  return;   
    if (max_level < level) { 
        ans.push_back(root->data);
        max_level = level; 
    } 
    leftView(root->left, level+1, max_level, ans); 
    leftView(root->right, level+1, max_level, ans); 
}
```

## 参考

1. http://www.cnblogs.com/grandyang/p/6864398.html
2. https://www.jiuzhang.com/solution/
3. https://www.geeksforgeeks.org/binary-tree-data-structure/
4. https://www.geeksforgeeks.org/print-left-view-binary-tree/

---

# 文件：数据结构与算法\dp.md

---

# 动态规划(DP)

动态规划是面试中最常被问道的题目,但是一般情况下的都是常见的一些题目.

1. [百度百科](https://baike.baidu.com/item/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/529408?fr=aladdin)
2. [wikipedia](https://en.wikipedia.org/wiki/Dynamic_programming)

## 1. 最长上升子序列

> **题目**: 最长上升子序列问题是在一个无序的给定序列中找到一个尽可能长的由低到高排列的子序列，这种子序列不一定是连续的或者唯一的.

> **解析**:

dp[j]: 表示以j结尾的最长子序列的长度,
dp[j] = max(dp[j], dp[i]+1) if(a[i]<d[j]) {i in [1,j]}

return max(dp[1-n])
使用二分查找可以得到O(nlog)的算法,这里就不给出,思路也很简单,读者自行查询.

```
int lis(vector<int> &nums) {
    if(nums.size() == 0) return 0;
    memset(dp,0,sizeof(dp));
    int ans = 1;
    dp[0] = 1;
    for(int i=0;i<nums.size();i++){
        dp[i] = 1;
        for(int j=0;j<i;j++){
            if(nums[j] < nums[i]){
                dp[i] = max(dp[i],dp[j]+1);
            }
        }
        ans = max(ans,dp[i]);     
    }
    return ans;
}
```

## 2. 最长公共子序列

> **题目**: 给出两个字符串，找到最长公共子序列(LCS)，返回LCS的长度。

> **解析**:

dp[i][j]: 表示以i和j结尾的最长序列的长度.
dp[i][j] = max(dp[i-1][j], dp[i][j-1]) if(a[i] != b[j])
dp[i][j] = dp[i-1][j-1] + 1 if(a[i]==b[j])

```
int lcs(string &A, string &B) {
        int dp[A.size()+1][B.size()+1] = {0};   
        for(int i=1;i<=A.size();i++){
            for(int j=1;j<=B.size();j++){
                dp[i][j] = max(dp[i-1][j],dp[i][j-1]);
                if(A[i-1] == B[j-1]){
                    dp[i][j] = max(dp[i][j],dp[i-1][j-1]+1);
                }
            }
        }
        return dp[A.size()][B.size()];
    }
```

## 3. 最长整除子集

> **题目**: 给定一个n个正整数的数组, 找出最长的子序列,使得序列中每一个较小的数都能整除较大的数.

> Example:

Input : arr[] = {10, 5, 3, 15, 20}
Output : 3
最长子序列: 10, 5, 20.
因为: 20能被整除10, 10能被5整除.

> **解析**: 这个可以参考最长上升子序列, 首先排序数组.

dp[i]: 表示下标i结尾的,最长的子序列长度
if(a[j] % a[i] == 0) dp[j] = max(dp[j], dp[i]+1) j in [i+1, n]

```
int largeSubset(int a[], int n) {
    sort(a, a+n);
    int dp[n] = {0};
    dp[0] = 1;
    for(int j = 1; j < n; j++) {
        for(int i = 0; i < j; i++) {
            if(a[j] % a[i] == 0) {
                dp[j] = max(dp[j], dp[i]+1);
            }
        }
    }
    return *max_element(dp, dp+n);
}
```

## 4. 背包问题

> **题目**: 在n个物品中挑选若干物品装入背包，最多能装多满？假设背包的大小为m，每个物品的大小为A[i].

> **解析**:

表示dp[m]能否装满, dp[m] = dp[m] | dp[m-A[i]])

```
int backPack(int m, vector<int> A) {
    int dp[m+1];
    memset(dp,0,sizeof(dp));
    dp[0]=1;
    //背包问题的循环顺序很重要
    for(int i=0;i<A.size();i++){
        for(int j=m;j>=A[i];j--){
            dp[j] |= dp[j-A[i]];//注意一下这个语句，类似于，
            //if(dp[j-A[i]]==1) dp[j]=1;
        }
    }
    int ans;
    for(ans=m;!dp[ans];--ans);   
    return ans;
}
```

## 5. 编辑距离

> **题目**: 给出两个单词word1和word2，计算出将word1 转换为word2的最少操作次数。
> 你总共三种操作方法：

插入一个字符
删除一个字符
替换一个字符

> **解析**:

分别表示插入,删除,修改
dp[i][j] = min(dp[i-1][j], dp[i][j-1], dp[i-1][j-1])+1; // a[i-1] != b[j-1]
dp[i][j] = dp[i-1][j-1] // a[i-1] == b[j-1]

```
int minDistance(string &word1, string &word2) {
    int dp[word1.size()+1][word2.size()+1];
    dp[0][0] = 0;
    for(int i=1;i<=word1.size();i++) dp[i][0] = i;
    for(int i=1;i<=word2.size();i++) dp[0][i] = i;  
    for(int i=1;i<=word1.size();i++){
        for(int j=1;j<=word2.size();j++){
            if(word1[i-1] == word2[j-1]){
                dp[i][j] = dp[i-1][j-1];
            }
            else{
                dp[i][j] = min(min(dp[i-1][j],dp[i][j-1]),dp[i-1][j-1]) + 1;
            }
        }
    }
    return dp[word1.size()][word2.size()];
}
```

## 6. 矩阵链乘

> **题目**: 给你一个矩阵序列, 找到有效的方式把这些数相乘到一起.
> Example:

Input: p[] = {40, 20, 30, 10, 30}
Output: 26000

表示四个矩阵,分别是A:40x20, B:20x30, C;30x10, D:10x30.
最优的方式是: (A(BC))D -->
20*30*10 + 40*20*10 + 40*10*30

> **解析**:

dp[i][j]: 表示[i,j]区间上最小值.
dp[i][j] = min(dp[i][j], dp[i][k]+dp[k+1][j]+p[i-1]*p[k]*p[l]) k in [i,j-1]

```
int MatrixChainOrder(int p[], int n) {  
    int dp[n][n]; 
    int i, j, k, L, q;   
    for (i=1; i<n; i++) { 
        dp[i][i] = 0; 
    }
    // L is chain length. 
    for (L=2; L<n; L++) { 
        for (i=1; i<n-L+1; i++) { 
            j = i+L-1; 
            dp[i][j] = INT_MAX; 
            for (k=i; k<=j-1; k++) { 
                q = dp[i][k] + dp[k+1][j] + p[i-1]*p[k]*p[j]; 
                if (q < dp[i][j]) dp[i][j] = q; 
            } 
        } 
    } 
    return m[1][n-1]; 
}
```

## 7. 回文划分

> **题目**:给定字符串 s, 需要将它分割成一些子串, 使得每个子串都是回文串.
> 最少需要分割几次?

> **Example**:

样例 1:
输入: "a"
输出: 0
解释: "a" 本身就是回文串, 无需分割

样例 2:
输入: "aab"
输出: 1
解释: 将 "aab" 分割一次, 得到 "aa" 和 "b", 它们都是回文串.

> **解析**:

可以看作序列型动态规划问题, 设定 dp[i] 表示原串的前 i 个字符最少分割多少次可以使得到的都是回文子串.

如果 s 前 i 个字符组成的子串本身就是回文串, 则 dp[i] = 0, 否则:

dp[i] = min{dp[j] + 1} (j < i 并且 s[j + 1], s[j + 2], ... , s[i] 是回文串)

```
int minCut(string s) {
    int n = s.length();
    int f[n + 1];
    vector<vector<bool>> isPalin(n, vector<bool>(n, false));
    for (int i = 0; i < n; i++) {
        isPalin[i][i] = true;
        if (i + 1 < n) {
            isPalin[i][i + 1] = (s[i] == s[i + 1]);
        }
    }
    for (int i = n - 1; i >= 0; i--) {
        for (int j = i + 2; j < n; j++) {
            isPalin[i][j] = isPalin[i + 1][j - 1] && (s[i] == s[j]);
        }
    }
    f[0] = -1;
    for (int i = 1; i <= n; i++) {
        f[i] = i - 1;
        for (int j = 0; j < i; j++) {
            if (isPalin[j][i - 1]) {
                f[i] = min(f[i], f[j] + 1);
            }
        }
    }
    return f[n];
}
```

## 8. 丑数

> **题目**:设计一个算法，找出只含素因子2，3，5 的第 n 小的数。

> **解析**: 使用2,3,5进行组合,得到第n个丑数.

```
int dp[100000];  
int MIN(int x,int y,int z){
    return min(min(x,y),z);
  
}
int nthUglyNumber(int n) {
    dp[1] = 1;
    int i2,i3,i5; // 分别表示2,3,5的对应的数，目标是使用前面的数字构造后面的数字，
    // 不能使用2,3,5的倍数进行构造，否则会出现错误   
    i2 = i3 = i5 = 1;
    int i=2;
    while(i<=n){
        int m2 = dp[i2] * 2;
        int m3 = dp[i3] * 3;
        int m5 = dp[i5] * 5;
        int minv = MIN(m2,m3,m5);
        dp[i++] = minv;      
        if(minv == m2) i2++;
        if(minv == m3) i3++;
        if(minv == m5) i5++;
    }
    return dp[n];
}
```

## 9. 最小花费路径

> **题目**: 给定一个矩阵,求出从左上角到右下角的最小路径的和.

> **解析**:

dp[i][j]: (0,0)到(i,j)的最小路径的和.
dp[i][j] = min(dp[i-1][j],dp[i][j-1],dp[i-1][j-1])+a[i][j]

```
int minCost(vector<vector<int>> cost){ 
     int i, j, m, n; 
     int m = cost.size();
     int n = cost[0].size();
     int dp[m+1][n+1];   
     dp[0][0] = cost[0][0];   
     for (i = 1; i <= m; i++) { 
        dp[i][0] = dp[i-1][0] + cost[i][0];
    }
    for (j = 1; j <= n; j++) {
        dp[0][j] = dp[0][j-1] + cost[0][j]; 
    }
    for (i = 1; i <= m; i++) {
        for (j = 1; j <= n; j++) { 
            dp[i][j] = min(dp[i-1][j-1],  
                        dp[i-1][j],  
                        dp[i][j-1]) + cost[i][j]; 
        }
    }
    return dp[m][n]; 
} 
int min(int x, int y, int z) { 
   if (x < y) 
      return (x < z)? x : z; 
   else
      return (y < z)? y : z; 
}
```

## 10. 最大矩阵和

> **题目**: 给定一个由整数组成二维矩阵（r*c），现在需要找出它的一个子矩阵，使得这个子矩阵内的所有元素之和最大，并把这个子矩阵称为最大子矩阵。

> Example:

例子：
0 -2 -7 0
9 2 -6 2
-4 1 -4 1
-1 8 0 -2
其最大子矩阵为：
9 2
-4 1
-1 8
其元素总和为15。

> **解析**: 将矩阵进行求和压缩到一维形式,之后使用一维数组的最大子段和进行计算.

```
int a[101][101],s[101],ma[101];
int maxSum(int s[],int ma[],int m){//最大子序列的和
    ma[0]=s[0];
    for(int i=1;i<m;i++){
       if(ma[i-1]>=0) ma[i]=ma[i-1]+s[i];
       else ma[i]=s[i];
    }
    int sum=ma[0];
    for(int i=1;i<m;i++){
        if(sum<ma[i]) sum=ma[i];
    }
    return sum;
}
int maxMatrixSum(int n, int m) {
    int res=INT_MIN;//注意序列的最小值
    for(int i=0;i<n;i++){
        memset(s,0,sizeof(s));
        for(int j=i;j<n;j++){
            int sum=0;
            for(int k=0;k<m;k++){
                s[k]+=a[j][k];//转化为一维数组
            }
            sum=maxSum(s,ma,m);
            if(sum>res) res=sum;
        }
    }
    return res;
}
```

## 11. 最大正方形面积

> **题目**: 给你一个二维矩阵，权值为False和True，找到一个最大的正方形，使得里面的值全部为True，输出它的面积.

> Example:

输入:
[
[1, 1, 0, 0, 1],
[0, 1, 0, 0, 1],
[0, 0, 1, 1, 1],
[0, 0, 1, 1, 1],
[0, 0, 0, 0, 1]
]
输出: 4

> **解析**:

构造辅助数组,dp[m][n],

用m[i][j]表示右下角的1.
if m[i][j]=1 then
dp[i][j]=min(dp[i][j-],dp[i-1][j], dp[i-1][j-1]) + 1
else:
dp[i][j] = 0

```
int MaxSubSquare(vector<vector<bool>> &matrix) {
    int R=matrix.size(), C=matrix[0].size();
    vector<vector<int>> dp(matrix.size(), vector<int>(matrix[0].size(), 0));
  
    int i, j;         
    for(i = 0; i < R; i++) { 
        dp[i][0] = matrix[i][0];  
    }
    for(j = 0; j < C; j++) { 
        dp[0][j] = matrix[0][j];  
    }
    int res = 0;  
    for(i = 1; i < R; i++) {  
        for(j = 1; j < C; j++) {  
            if(matrix[i][j] == 1)  
                dp[i][j] = min(dp[i][j-1],min(dp[i-1][j],dp[i-1][j-1])) + 1;  
            else
                dp[i][j] = 0;  
            res = max(res, dp[i][j]);
        }  
    }  
    return res;
}
```

## 12. 二进制串个数

> **题目**: 求长度为n的01组成的二进制串中,没有连续1的串的个数.

> **解析**: 分别用a[i]和b[i],表示长度为i,分别0结尾和1结尾的串的个数. 那么

a[i+1] = a[i] + b[i] // 在后面加0
b[i+1] = a[i] // 只能在结尾是0的后面加1

```
int countStrings(int n) { 
    int a[n], b[n]; 
    a[0] = b[0] = 1; 
    for (int i = 1; i < n; i++) { 
        a[i] = a[i-1] + b[i-1]; 
        b[i] = a[i-1]; 
    } 
    return a[n-1] + b[n-1]; 
}
```

## 13. 交叉字符串

> **题目**: 给出三个字符串:s1、s2、s3，判断s3是否由s1和s2交叉构成。
> **解析**:

dp[i][j]: s1[1,i] 和 s2[1,j] 是否能够组成s3[i+j]

dp[i][j] = dp[i][j] || dp[i-1][j]  if s1[i] == s3[i+j-1]

dp[i][j] = dp[i][j] || dp[i][j-1]  if s2[j] == s3[i+j-1]

```
bool isInterleave(string &s1, string &s2, string &s3) {
    if(s1.size() + s2.size() != s3.size()) return false;
    int dp[s1.size() + 1][s2.size() + 1] = {0};
    dp[0][0] = 1;
    int ok = 1;
    // 初始化
    for(int i = 1; i <= s1.length(); i++) {
        dp[i][0] = dp[i - 1][0] && s1[i - 1] == s3[i - 1];
    }  
    for(int i = 1; i <= s2.length(); i++) {
        dp[0][i] = dp[0][i - 1] && s2[i - 1] == s3[i - 1];
    }
    // dp转化
    for(int i = 1; i <= s1.size(); i++) {
        for(int j = 1; j <= s2.size() ;j++) {
            if(s3[i+j - 1] == s1[i - 1]) {
                dp[i][j] = dp[i][j] || dp[i-1][j];
            }
            if(s3[i+j - 1] == s2[j - 1]) {
                dp[i][j] = dp[i][j] || dp[i][j - 1];
            }
          
        }
    }
    return dp[s1.size()][s2.size()];
}
```

## 14. 乘积最大子序列

> **题目**: 找出一个序列中乘积最大的连续子序列（至少包含一个数）。
> **解析**: 这里可以借鉴和最大的子序列,但是需要每次保存两个值,一个最大值和最小值,(因为存在负负得正).

```
int maxProduct(vector<int> &nums) {
    int premin, premax, ans;
    premin = premax = ans = nums[0];
    for(int i=1;i<nums.size();i++){
        // 每次更新最大最小值,保证负负得正
        // 这里使用滚动变量表示dp
        int curmax = max(max(premax*nums[i],premin*nums[i]),nums[i]);
        int curmin = min(min(premax*nums[i],premin*nums[i]),nums[i]);
        premax = curmax;
        premin = curmin;
        ans = max(curmax, ans);
    }
    return ans;
  
}
```

## 15. k个数之和

> **题目**: 给定 n 个不同的正整数，整数 k（k <= n）以及一个目标数字 target。在这 n 个数里面找出 k 个数，使得这 k 个数的和等于目标数字，求问有多少种方案？
> **解析**: dp[j][s]比碍事j个数组合s的个数,
> dp[j][s] += dp[j-1][s-A[i]] {i: [0,n)}

```int
int ans = 0;
    int dp[1000][1000] = {0};
    dp[0][0] = 1;  
    for(int i=0;i<A.size();i++){
        for(int j=k;j>0;j--){
            for(int s=target;s>=A[i];s--){
                dp[j][s] += dp[j-1][s-A[i]];
            }
        }
    }   
    ans = dp[k][target];
    return ans;
}
```

# 参考

1. https://www.lintcode.com/problem/?tag=dynamic-programming
2. https://www.geeksforgeeks.org/dynamic-programming/

---

# 文件：数据结构与算法\graph.md

---

# Graph(图)

在面试的过程中,一般不会考到图相关的问题,因为图相关的问题难,而且描述起来很麻烦.
但是也会问道一下常见的问题,比如,最短路径,最小支撑树,拓扑排序都被问到过.

1. 图常用的表示方法有两种: 分别是邻接矩阵和邻接表.
   邻接矩阵是不错的一种图存储结构,对于边数相对顶点较少的图,这种结构存在对存储空间的极大浪费.
   因此,找到一种数组与链表相结合的存储方法称为邻接表.

+ 邻接矩阵表示的无向图

<img src="../assert/g-1.png">

+ 邻接表表示的无向图

<img src="../assert/g-2.png">

## 1. 最短路径

+ Dijkstra

1. 维护一个最短路径的的集合(sptSet)和最短距离数组, 直到遍历所有的点, 初始化起始点的距离是0, 集合为空.
2. 初始化起始点s到所有的点的距离是INF, 注意s到s的距离是0.
3. while sptSet 不包含所有的顶点:
   + 选择当前能到达点的最小距离的点u,加入 sptSet
   + 使用u作为中间顶点,更新所有点的距离,选择最小距离的替换
   + dist[u]+graph[u][v] < dist[v]

[百度百科](https://baike.baidu.com/item/%E8%BF%AA%E6%9D%B0%E6%96%AF%E7%89%B9%E6%8B%89%E7%AE%97%E6%B3%95/4049057?fr=aladdin)
[wikipedia](https://en.wikipedia.org/wiki/Dijkstra%27s_algorithm)

```
int minDistance(vector<int> dist, set<int> sptSet) {
    int min_d = INT_MAX, u;
    for(int i = 0, i < dist.size(); i ++) {
        if(sptSet.find(i) == sptSet.end() && dist[v] < min_d) {
            min_d = dist[i], u = i;
        }
    }
    return u;
}
// 使用vector 表示的邻接矩阵, return 起始点到所有点的最小距离
// 没有边的用0填充
vector<int> dijstra(vector<vector<int>> graph, set<int> &sptSet,int src) {
    int V = graph.size();
    vector<int> dist(V, 0);
    for(int i = 0;i < V; i ++) {
        if(i != src) dist[i] = INT_MAX;
    }
    while(sptSet.size() < V-1) {
        // pick mininum distance u
        int u = minDistance(dist, sptSet); 
        sptSet.insert(u);
        // 使用u更新距离
        for(int v = 0; v < V; v ++) {
            if(sptSet.find(v)==sptSet.end() && graph[u][v] 
                        && dist[u] != INT_MAX
                        && dist[u]+graph[u][v] < dist[v]) {
                dist[v] = dist[u] + graph[u][v];
            }
        }
    }
    return dist;
}
```

+ Floyd Warshall
  Floyd算法是使用动态规划算法, dist[i][j]表示i-->j的最短距离,
  那么是否存在i-->k-->j的路径小于dist[i][j],于是就有了下面的更新公式,
+ if dist[i][k] + dist[k][j] < dist[i][j]:
  dist[i][j] = dist[i][k] + dist[k][j]

[百度百科](https://baike.baidu.com/item/floyd-warshall%E7%AE%97%E6%B3%95/9705345)
[wikipedia](https://en.wikipedia.org/wiki/Floyd%E2%80%93Warshall_algorithm)

```
void floydWarshall(vector<vector<int> > graph, vector<vector<int>> &dist, vector<vector<int> > &path) {
    int V = graph.size();
    // 参数dist和path需要初始化大小, 确定是否已经初始化
    vector<vector<int> > tmp(V, vector<int>(V));
    dist = path = tmp;
    for(int i = 0; i < V; i ++) {
        for(int j = 0; j < V; j ++) {
            dist[i][j] = graph[i][j];
            path[i][j] = j;
        }
    }
    for(int i = 0; i < V; i++) {
        for(int j = 0; j < V; j++) {
            for(int k = 0; k < V; k++) {
                if(dist[i][j] > dist[i][k] + dist[k][j]) {
                    dist[i][j] = dist[i][k] + dist[k][j];
                    pre[i][j] = pre[i][k];
                }
            }
        }
    }
}
//打印最短路径 u ---> v
int pfpath(int u, int v, vector<vector<int> > path) { 
    while(u != v) {
        cout << u  << " ";
        u = path[u][v];
    }
    cout << u << endl;
}
```

## 2. 最小支撑树

+ Prim Algorithm

1. 用一个集合mstSet维护已经满足要求的顶点
2. 使用dist表示从mstSet集合某个点到u的最小距离为INF, 初始点Src的距离是0.
3. while mstSet doesn't include all vertices:
   + 选择一个不在mstSet中, 并且在dist中距离最小的顶点u, 加入到mstSet
   + 使用u更新dist距离, 表示从mstSet某个点到达为使用的点的最小距离

[百度百科](https://baike.baidu.com/item/Prim/10242166)
[wikipedia](https://en.wikipedia.org/wiki/Prim%27s_algorithm)

```
int minDistance(vector<int> dist, set<int> mstSet) {
    int min_d = INT_MAX, u;
    for(int i = 0, i < dist.size(); i ++) {
        if(mstSet.find(i) == mstSet.end() && dist[v] < min_d) {
            min_d = dist[i], u = i;
        }
    }
    return u;
}
// 使用vector 表示的邻接矩阵, return 起始点到所有点的最小距离
// 没有边的用0填充
vector<int> dijstra(vector<vector<int>> graph, set<int> &mstSet,int src) {
    int V = graph.size();
    vector<int> dist(V, 0);
    int parent[V]; // 每个顶点的相邻的点
    parent[src] = -1;
    for(int i = 0;i < V; i ++) {
        if(i != src) dist[i] = INT_MAX;
    }
    while(mstSet.size() < V-1) {
        // pick mininum distance u
        int u = minDistance(dist, sptSet); 
        mstSet.insert(u);
        // 使用u更新距离
        for(int v = 0; v < V; v ++) {
            if(mstSet.find(v)==mstSet.end() && graph[u][v] 
                        && graph[u][v] < dist[v]) {
                dist[v] = graph[u][v];
                parent[v] = u;
            }
        }
    }
    return dist;
}
```

+ Kruskal Algorithm

1. 根据权重排序所有的边
2. 选择一个小权重的边,如果它没有和最小支撑顶点形成环,就加入这个边
3. 重复2,知道包含V-1个边

[百度百科](https://baike.baidu.com/item/%E5%85%8B%E9%B2%81%E6%96%AF%E5%85%8B%E5%B0%94%E6%BC%94%E7%AE%97%E6%B3%95)
[wikipedia](https://en.wikipedia.org/wiki/Kruskal%27s_algorithm)
[Code 抄写](https://www.geeksforgeeks.org/kruskals-minimum-spanning-tree-algorithm-greedy-algo-2/)

```
struct Edge { 
    int src, dest, weight; 
}; 
struct Graph { 
    int V, E;   
    struct Edge* edge; 
}; 
struct Graph* createGraph(int V, int E) { 
    struct Graph* graph = new Graph; 
    graph->V = V; 
    graph->E = E;   
    graph->edge = new Edge[E]; 
    return graph; 
} 
struct subset { 
    int parent; 
    int rank; 
}; 
int find(struct subset subsets[], int i) { 
    if (subsets[i].parent != i) 
        subsets[i].parent = find(subsets, subsets[i].parent); 
    return subsets[i].parent; 
} 
void Union(struct subset subsets[], int x, int y) { 
    int xroot = find(subsets, x); 
    int yroot = find(subsets, y);   
    if (subsets[xroot].rank < subsets[yroot].rank) 
        subsets[xroot].parent = yroot; 
    else if (subsets[xroot].rank > subsets[yroot].rank) 
        subsets[yroot].parent = xroot;   
    else { 
        subsets[yroot].parent = xroot; 
        subsets[xroot].rank++; 
    } 
} 
int myComp(const void* a, const void* b) { 
    struct Edge* a1 = (struct Edge*)a; 
    struct Edge* b1 = (struct Edge*)b; 
    return a1->weight > b1->weight; 
} 
void KruskalMST(struct Graph* graph) { 
    int V = graph->V; 
    struct Edge result[V];  
    int e = 0; 
    int i = 0; 
    qsort(graph->edge, graph->E, sizeof(graph->edge[0]), myComp);   
    struct subset *subsets = 
        (struct subset*) malloc( V * sizeof(struct subset) ); 
    for (int v = 0; v < V; ++v) { 
        subsets[v].parent = v; 
        subsets[v].rank = 0; 
    }   
    while (e < V - 1) { 
        struct Edge next_edge = graph->edge[i++];   
        int x = find(subsets, next_edge.src); 
        int y = find(subsets, next_edge.dest);   
        if (x != y) { 
            result[e++] = next_edge; 
            Union(subsets, x, y); 
        } 
    }   
    return; 
}
```

## 3. 拓扑排序

**定义**: 对一个有向无环图(Directed Acyclic Graph简称DAG)G进行拓扑排序，是将G中所有顶点排成一个线性序列，使得图中任意一对顶点u和v，若边(u,v)∈E(G)，则u在线性序列中出现在v之前。

1. 计算所有节点的入度
2. 每次选择一个入度为0的顶点u,如果的已经排序的结果中
3. 将u所到达的所有顶点v,入度减1,
4. 重复1,2,直到遍历所有顶点

[百度百科](https://baike.baidu.com/item/%E6%8B%93%E6%89%91%E6%8E%92%E5%BA%8F/5223807?fr=aladdin)
[wikipedia](https://en.wikipedia.org/wiki/Topological_sorting)

```
class Graph { 
    int V;  // 顶点的个数
    list<int> *adj; // 所有顶点的起始指针
};

void topologicalSort(int V, list<int> *adj) { 
    // 计算所有入度
    vector<int> in_degree(V, 0);   
    for (int u=0; u<V; u++) { 
        list<int>::iterator itr; 
        for (itr = adj[u].begin(); itr != adj[u].end(); itr++) {
             in_degree[*itr]++; 
        }
    } 
    // 加入入度为0的点
    queue<int> q; 
    for (int i = 0; i < V; i++) { 
        if (in_degree[i] == 0) q.push(i); 
    }
    int cnt = 0;   
    vector <int> top_order;   
    while (!q.empty()) { 
        int u = q.front(); 
        q.pop(); 
        top_order.push_back(u);   
        // 所有连接点, 入度减去1
        list<int>::iterator itr; 
        for (itr = adj[u].begin(); itr != adj[u].end(); itr++) {
            if (--in_degree[*itr] == 0) q.push(*itr); 
        }
        cnt++; 
    } 
    if (cnt != V) { 
        cout << "There exists a cycle in the graph\n"; 
        return; 
    }   
    for (int i=0; i<top_order.size(); i++) 
        cout << top_order[i] << " "; 
    cout << endl; 
}
```

## 4. 有向图判环

**题目**: 请你判断一个 n 个点，m 条边的有向图是否存在环。参数为两个int数组，start[i]到end[i]有一条有向边.
**解析**: 这是拓扑排序的一种应用.

```
bool isCyclicGraph(vector<int> &start, vector<int> &end) {
    // 找到最大顶点值,构造图,
    int n = 0;
    for (int s : start) {
        n = max(n, s);
    }
    for (int e : end) {
        n = max(n, e);
    }
    // 构造图
    vector<vector<int>> graph(n + 1);
    vector<int> d(n + 1);
    int m = start.size();
    // 计算所有顶点的入度
    for (int i = 0; i < m; i++) {
        graph[start[i]].push_back(end[i]);
        d[end[i]]++;
    }
    queue<int> que;
    // 将所有入度为0的点,加入队列
    for (int i = 1; i <= n; i++) {
        if (graph[i].size() && !d[i]) {
            que.push(i);
        }
    }
    while (!que.empty()) {
        int h = que.front();
        que.pop();
        // 将多有入度为0的点,对应的顶点 入度减去1
        for (int y : graph[h]) {
            d[y]--;
            if (!d[y]) {
                que.push(y);
            }
        }
    }
    // 判断是否所有顶点的入度都是0, 如果是,则没有环,否则就有
    for (int i = 1; i <= n; i++) {
        if (d[i]) {
            return true;
        }
    }
    return false;
}
```

# 参考

1. https://www.cnblogs.com/Ash-ly/p/5920953.html
2. https://www.geeksforgeeks.org/graph-data-structure-and-algorithms/
3. 

---

# 文件：数据结构与算法\greedy.md

---

# 贪心算法

> 是每次只考虑当前最优，目标证明每次是考虑当前最优能够达到局部最优，这就是贪心的思想，一般情况下贪心和排序一起出现，都是先根据条件进行排序，之后基于贪心策略得到最优结果。
> 面试的时候面试官一般不会出贪心算法，如果可能贪心一般都可以使用动态规划解决，面试官很喜欢出动态规划的题目。

## 1. 最大连续子序列

> **题目**: 给定一个整数数组，找到一个具有最大和的子数组，返回其最大和。
> **扩展1**: 给定一个整数数组，找出两个 不重叠 子数组使得它们的和最大。
> **扩展2**: 给定一个整数数组，找出两个不重叠的子数组A和B，使两个子数组和的差的绝对值|SUM(A) - SUM(B)|最大。
> **分析**: 使用这个s表示当前可能满足的最大和，如果s>0,我们认为s对接下来的加操作有帮助，基于s+=nums[i]，if s < 0, 认为s只会对后面造成负影响，两s=nums[i]。
> **扩展问题**: 可以将 数组从每个位置k分开，分别结算[1,i]和[i+1, n)的最值，记录的过程中可以使用数组保存下来的已经计算好的值。

```
int maxSubArray(vector<int> &nums) {
    int s = 0, ans = -1000000;
    for(int i = 0; i < nums.size(); i ++) {
        if(s > 0) s += nums[i];
        else s = nums[i];
        ans = max(s, ans);
    }
    return ans;
}
```

## 2. 删除数字

> **题目**: 给定一个以字符串表示的非负整数，从该数字中移除掉k个数位，让剩余数位组成的数字尽可能小，求可能的最小结果。
> **分析**: 从左到右遍历字符串，找到第一个不满足递增的数字删除，一定会保证当前操作之后剩下的数字最小。

```
string removeKdigits(string &num, int k) {
    int i;
    while(k --) {
        for(i = 0; i < num.size() - 1 && num[i] <= num[i+1]; i ++);
        num.erase(num.begin() + i);
    }
    // remove 0
    auto it = num.begin();
    while(it != num.end() && *it == '0') {
        num.erase(it);
        it = num.begin();
    }
    if(num.size() == 0) num = "0";
    return num;
}
```

## 3. 无重叠区间

> **题目**: 给定一些区间，找到需要移除的最小区间数，以使其余的区间不重叠。
> **分析**: 贪心一般伴随着排序一起出现，我们根据区间的结束使用升序排序，之后进行遍历，如果发现不满足条件，则移除这个不满足的区间。

```
classs Interval {
    int start, end;
    Interval(int start, int end) {
        this->start = start;
        this->end = end;
    }
}
bool cmp(Interval a, Interval b) {
    if(a.end < b.end) return 1;
    else return 0;
}   
int eraseOverlapIntervals(vector<Interval> &intervals) {
    sort(intervals.begin(), intervals.end(), cmp);  
    int cnt = 0;
    Interval tmp = intervals[0];
    for(int i = 1; i < intervals.size(); i ++) {
        if(tmp.end <= intervals[i].start) tmp = intervals[i];
        else {
            cnt ++;
        }
    }
    return cnt;
}
```

## 4. 合并数字

> **题目**: 给出n个数，现在要将这n个数合并成一个数，每次只能选择两个数a,b合并，每次合并需要消耗a+b的能量，输出将这n个数合并成一个数后消耗的最小能量。
> **分析**: 参考哈夫曼树的构造，每一次合并两个最小的数，直到剩下一个数字，因为每次要选择两个最小的，需要用到最小堆来实现，可以使用C++SLT中的优先队列.
> 根据这个题目，请大家**自行补上哈夫曼树**。

```
int mergeNumber(vector<int> &numbers) {
    priority_queue<int, vector<int>, greater<int>> pq;
    for(int i = 0; i < numbers.size(); i ++) {
        pq.push(numbers[i]);
    }
    int cost = 0;
    while(pq.size() > 1) {
        int a = pq.top();
        pq.pop();
        int b = pq.top();
        pq.pop();
        cost += (a + b);
        pq.push(a + b);
    }
    return cost;
}
```

## 5. 最小支撑树

> 题目: 使用kruskal算法，构造最小支撑树。
> 分析: 详见百度百科或者wikipedia.
> 代码: <a href="https://www.geeksforgeeks.org/kruskals-minimum-spanning-tree-algorithm-greedy-algo-2/">kruskal code</a>

```
struct Edge { 
    int src, dest, weight; 
}; 
struct Graph { 
    int V, E;   
    struct Edge* edge; 
}; 
struct Graph* createGraph(int V, int E) { 
    struct Graph* graph = new Graph; 
    graph->V = V; 
    graph->E = E;   
    graph->edge = new Edge[E]; 
    return graph; 
} 
struct subset { 
    int parent; 
    int rank; 
}; 
int find(struct subset subsets[], int i) { 
    if (subsets[i].parent != i) 
        subsets[i].parent = find(subsets, subsets[i].parent); 
    return subsets[i].parent; 
} 
void Union(struct subset subsets[], int x, int y) { 
    int xroot = find(subsets, x); 
    int yroot = find(subsets, y);   
    if (subsets[xroot].rank < subsets[yroot].rank) 
        subsets[xroot].parent = yroot; 
    else if (subsets[xroot].rank > subsets[yroot].rank) 
        subsets[yroot].parent = xroot;   
    else { 
        subsets[yroot].parent = xroot; 
        subsets[xroot].rank++; 
    } 
} 
int myComp(const void* a, const void* b) { 
    struct Edge* a1 = (struct Edge*)a; 
    struct Edge* b1 = (struct Edge*)b; 
    return a1->weight > b1->weight; 
} 
void KruskalMST(struct Graph* graph) { 
    int V = graph->V; 
    struct Edge result[V];  
    int e = 0; 
    int i = 0; 
    qsort(graph->edge, graph->E, sizeof(graph->edge[0]), myComp);   
    struct subset *subsets = 
        (struct subset*) malloc( V * sizeof(struct subset) ); 
    for (int v = 0; v < V; ++v) { 
        subsets[v].parent = v; 
        subsets[v].rank = 0; 
    }   
    while (e < V - 1) { 
        struct Edge next_edge = graph->edge[i++];   
        int x = find(subsets, next_edge.src); 
        int y = find(subsets, next_edge.dest);   
        if (x != y) { 
            result[e++] = next_edge; 
            Union(subsets, x, y); 
        } 
    }   
    return; 
}
```

## 6. 补齐数组

> 题目: 给出一个正整数数组nums和一个整数n，向数组添加patch元素，使得范围[1, n]包含的任何数字都可以由数组中某些元素的总和形成。返回所需的最少补齐数。分析：
> 
> 1. 升序排序，
> 2. 使用r表示目前可以表示的右边界，如果当前值 > r, 超出范围，又因为 [1, n] 区间内的任何数字都可以用 nums 中某几个数字的和来表示，那么只需要有n/2以及 [1, n/2] 区间内任何数字都可以用 nums 中某几个数字的和来表示即可。所有我们将r扩大一倍，继续判断是否满足。
> 3. 直到 r >= n。

```
int minPatches(vector<int> &nums, int n) {
    sort(nums.begin(), nums.end());
    long long r = 1;
    int i = 0;
    int cnt = 0;
    while(r <= n) {
        if(i < nums.size() && nums[i] <= r) r += nums[i++];
        else {
            cnt ++;
            r *= 2;
        }
    }
    return cnt;
}
```

## 7. 买卖股票的最佳时机

> 题目: 假设有一个数组，它的第i个元素是一支给定的股票在第i天的价格。如果你最多只允许完成一次交易(例如,一次买卖股票),设计一个算法来找出最大利润。
> 分析: 先低价买入，再高价卖出，因此从前向后，记录最小值并且更新最有结果，

```
int maxProfit(vector<int> &prices) {
    int minp = prices[0];
    int ans = 0;
    for(int i = 1; i < prices.size(); i ++) {
        ans = max(ans, prices[i] - minp);
        minp = min(minp, prices[i]);
    }
    return ans;
}
```

## 8. 买卖股票的最佳时机II

> **题目**: 假设有一个数组，它的第i个元素是一个给定的股票在第i天的价格。设计一个算法来找到最大的利润。你可以完成尽可能多的交易(多次买卖股票)。然而,你不能同时参与多个交易(你必须在再次购买前出售股票)。
> **分析**: 多次买卖，我们可以尽可能多的买卖股票，如果满足prices[i+1] > price[i]，就进行一次买卖，其实我们知道如果是一个递增序列，(prices[i+1] - prices[i]) + (prices[i] - prices[i-1]) = prices[i+1] - prices[i]，可以保证我们将所有可能的买卖识别出来。

```
int maxProfit(vector<int> &prices) {
    int sum = 0;
    for(int i=1;i<prices.size();i++){
        if(prices[i] > prices[i-1]){
            sum += prices[i] - prices[i-1];
        }
    }
    return sum;
}
```

## 9. 买卖股票的最佳时机含手续费

> **题目**: 给定一个数组，其中第i个元素是一支股票在第i天的价格，以及一个非负数 fee 代表了交易手续费。（只需要在卖出时支付 fee）。你可以进行任意次交易，而每次交易都必须付手续费，而且你不能持有超过1支数量的股票（也就是说，你在买入之前需要先把之前买入的卖出）。返回可以获得的最大利润。**分析**:
> 
> + 我们考虑最朴素的方法，对于每一天，如果当前有股票，考虑出售或者保留，如果没股票，考虑购买或者跳过，进行dfs搜索。每天都有两种操作，时间复杂度为O(2^n).
> + 如何优化呢？我们用动态规划的思想来解决这个问题，考虑每一天同时维护两种状态：拥有股票(own)状态和已经售出股票(sell)状态。用own和sell分别保留这两种状态到目前为止所拥有的最大利润。 对于sell，用前一天own状态转移，比较卖出持有股是否能得到更多的利润，即sell = max(sell , own + price - fee)， 而对于own , 我们考虑是否买新的股票更能赚钱(换言之，更优惠），own=max( own, sell-price).
> + 初始化我们要把sell设为0表示最初是sell状态且没有profit，把own设为负无穷因为最初不存在该状态，我们不希望从这个状态进行转移.
> + 因为我们保存的都是最优状态，所以在买卖股票时候取max能保证最优性不变.
> + 最后直接返回sell即可.
> + 来自(https://www.jiuzhang.com/solution/best-time-to-buy-and-sell-stock-with-transaction-fee/#tag-highlight-lang-cpp)

```
int maxProfit(vector<int> &prices, int fee) {
    int sell = 0, buy = -prices[0];
    for (int price : prices) {
        int sellOld = sell;
        sell = max(sell, buy + price - fee);
        buy = max(buy, sellOld - price);
    }
    return sell;
}
```

## 10. 最后的猫

> **题目**: 给你一个n只猫，每一个猫都有一个初始化的萌系数，当一只猫的萌系数变成0它就会离开你。现在你实在受不了这n只萌猫，想要只留下一只猫，并且使它的萌系数最低。每一个你可以选择任意一只猫A去消耗另外一只猫B的萌系数，这样的话猫B的萌系数就会减去猫A的萌系数，当猫A的萌系数不变。通过多次回合之后，最后剩下的猫的萌系数最小是多少。
> **分析**: 我们的目的是留下一只猫，使它的萌系数最小，从这个角度出发，我们可以选择最小萌系数的猫，去消耗其他的猫，如果其他的猫萌系数变成0，就离开了。例如最小萌系数的猫的系数是a，对于其他的猫，如果b%a == 0，则经过多次消耗之后，b就会离开，如果b%a!=0, 则结果是经过多轮消耗之后变成(b%a, a)，直到一方变成0，我们可以发现这是一个求最大公约的算式。因此，最后的猫萌系数是gcd(h[0],h[0],...,h[n-1]);

```
int gcd(int a, int b) {
    if(a == 0) return b;
    return gcd(b % a, a);
}
int solve(vector<int> &h) {
    if(h.size() == 1) return h[0];
    int ans = gcd(h[0], h[1]);
    for(int i = 2; i < h.size(); i ++) {
        ans = gcd(ans, h[i]);
    }
    return ans;
}
```

---

# 文件：数据结构与算法\linklist.md

---

# linklist

链表也是面试中常问道的题目，链表定义简单很容易考察面试者的水平，比如在数组中很简单的题目转换成链表就有很大的变动。例如链表的插入和归并排序、查找倒数第k个节点等.

## 1.回文链表（234）

请判断一个链表是否为回文链表

```
class Solution(object):
    def isPalindrome(self, head):
        reverse, fast = None, head
        while fast and fast.next:
            fast = fast.next.next
            head.next, reverse, head = reverse, head, head.next
          
        tail = head.next if fast else head

        is_palindrome = True
        while reverse:
            is_palindrome = is_palindrome and reverse.val == tail.val
            reverse.next, head, reverse = head, reverse, reverse.next
            tail = tail.next

        return is_palindrome
```

## 2.求单链表的中间节点

快、慢指针实现

```
def find_middle_node(head):
    slow, fast = head, head
    fast = fast.next if fast else None
    while fast and fast.next:
        slow = slow.next
        fast = fast.next.next
    return slow
```

## 3.删除无序链表中的重复项

给定一个无序的链表，去掉其重复项，并保留原顺序，例如链表1->3->1->5->5->7，去掉重复项后为1->3->5->7

```
class ListNode:
    def __init__(self, x):
        self.val = x
        self.next = None
class Solution:
    def deleteDuplicates(self, head: ListNode) -> ListNode:
        if head is None:
            return head
        outer = head
        while outer:
            inpre = outer
            inner = inpre.next
            while inner:
                if inner.val == outer.val:
                    inner = inner.next
                    inpre.next = inner
                else:
                    inpre = inner
                    inner = inner.next
            outer = outer.next
        return head
```

## 4.给定一个排序链表，删除所有含有重复数字的节点

输入: 1->2->3->3->4->4->5
输出: 1->2->5

```
class ListNode:
    def __init__(self, x):
        self.val = x
        self.next = None

class Solution:
    def deleteDuplicates(self, head):      
        if head is None:
            return head
        new = ListNode(None)
        new.next = head
        head = new
        outpre = head
        outer = outpre.next
        flag = False
        while outer:
            inpre = outer
            inner = inpre.next
            while inner:
                if inner.val == outer.val:
                    flag = True
                    inner = inner.next
                    inpre.next = inner
                else:
                    inpre = inner
                    inner = inner.next
            if flag:
                outer = outer.next              
                outpre.next = outer
                flag = False
            else:
                outpre = outer
                outer = outer.next
        return head.next    

        def deleteDuplicates_2(self, head):  
            if None == head or None == head.next:  
                return head  
  
            new_head = ListNode(-1)  
            new_head.next = head  
            parent = new_head  
            cur = head  
            while None != cur and None != cur.next:   
                if cur.val == cur.next.val:  
                    val = cur.val  
                    while None != cur and val == cur.val:
                        cur = cur.next  
                    parent.next = cur  
                else:  
                    cur = cur.next  
                    parent = parent.next  
  
            return new_head.next
```

## 5.环形链表（141）

~~~
给定一个链表，判断链表中是否有环, 并找到第一个相交的点
思路：设置两个指针slow和fast，一个步长为1，一个步长为2进行遍历。如果有环，则slow和fast总会在某一点相遇。如果没有环，则fast会先为空，或者fast.next为空
~~~

```
class Solution(object):
    def hasCycle(self, head, firstMeetNode):
        fast = slow = head
        while fast and fast.next:
            fast = fast.next.next
            slow = slow.next
            if slow == fast: # 有环
                fast = head
                while fast != slow:
                    fast = fast.next
                    slow = slow.next
                firstMeetNode = fast
                return True
        return False
```

## 6.反转链表（206）

(循环算法，递归算法)  微软

~~~
输入: 1->2->3->4->5->NULL 
输出: 5->4->3->2->1->NULL
~~~

```
class ListNode(object):
    def __init__(self, x):
        self.val = x
        self.next = None

    def __repr__(self):
        if self:
            return "{} -> {}".format(self.val, repr(self.next))
      
class Solution(object):
    '''循环算法'''
    def reverseList(self, head):
        dummy = ListNode(float("-inf"))
        while head:
            dummy.next, head.next, head = head, dummy.next, head.next
        return dummy.next      

class Solution2(object):
    '''递归算法'''
    def reverseList(self, head):
        [begin, end] = self.reverseListRecu(head)
        return begin

    def reverseListRecu(self, head):
        if not head:
            return [None, None]

        [begin, end] = self.reverseListRecu(head.next)

        if end:
            end.next = head
            head.next = None
            return [begin, head]
        else:
            return [head, head]
```

## 7.在双向链表中删除指定元素（微软）

```
class Node(object):
    '''双向节点'''
    def __init__(self, item):
        self.item = item
        self.next = None
        self.prev = None

class DLinkedlist(object):
    def __init__(self):
        self._head = None
  
    def remove(self, item):
        """删除元素"""
        if self.is_empty():
            return
        else:
            cur = self._head
            if cur.item == item:
# 如果首节点的元素即是要删除的元素
                if cur.next == None:
# 如果链表只有这一个节点
                    self._head = None
                else:
# 将第二个节点的prev设置为None
                    cur.next.prev = None
# 将_head指向第二个节点
                    self._head = cur.next
                return
          
            while cur != None:
                if cur.item == item:
# 将cur的前一个节点的next指向cur的后一个节点
                    cur.prev.next = cur.next
# 将cur的后一个节点的prev指向cur的前一个节点
                    cur.next.prev = cur.prev
                    break
                cur = cur.next
```

## 8.两个链表合并为一个升序链表（微软）

```
class Node(object):
    def __init__(self,item,next_=None):
        self.item = item
        self.next = next_
           
class Solution(object):
    def mergeTwoLists(self, l1,l2):   
        head = Node(0)
        cur = head
        while l1 != None and l2 != None:
            if l1.item <= l2.item:
                cur.next = l1
                l1 =  l1.next
            else:
                cur.next = l2
                l2 = l2.next
            cur = cur.next
        if l1 != None:
            cur.next = l1
        if l2 != None:
            cur.next = l2
        return head.next
```

## 9. 倒数第k个节点

> 题目: 找到链表的倒数第k个节点
> 解析: 使用两个指针fast和slow,fast先走k步,之后fast和slow一起走,
> 直到fast到达最后一个节点,slow就是倒数第k个节点

```
class Solution(object):
    '''
    题意：删除链表中倒数第k个结点，尽量只扫描一遍。
    使用两个指针扫描，当第一个指针扫描到第k个结点后，
    第二个指针从表头与第一个指针同时向后移动，
    当第一个指针指向空节点时，另一个指针就指向倒数第k个结点了     
    '''
    def removeNthFromEnd(self, head, k):
        res = ListNode(0)
        res.next = head
        tmp = res
        for i in range(0, k):
            head = head.next
        while head != None:
            head = head.next
            tmp = tmp.next
        tmp.next = tmp.next.next
        return res.next
```

## 10. 等概率返回链表中的一个元素

> 题意: 给你一个单链表,每次等概率随机返回一个元素
> 分析: 这里先不做详细解释,这是一个随机化算法的一种

```
class Solution(object):
    def randNode(self, head):
        res = head
        cur = head.next
        i = 2
        while cur != None:
            x = random.ranint(0,int(2**31))
            j = x % i
            if j == 0:
                res = cur
            cur = cur.next
            i += 1
        return ans
```

---

# 文件：数据结构与算法\search.md

---

# search

bfs 和 dfs的相关的题目

## 1. 全排列

> 题目: 给定一个数字列表，返回其所有可能的排列。

```
// premute(ans, nums, 0)
void permute(vector<vector<int> > &ans, vector<int> &nums, int k){
    if(k==nums.size()-1){
        ans.push_back(nums);
    }
    // 以k开头的所有排列
    for(int i=k;i<nums.size();i++){
        // 以每一个都作为开头，进行遍历
        swap(nums[i],nums[k]);
        permute(ans,nums,k+1);
        // 回溯
        swap(nums[i],nums[k]);
    }
}
```

## 2. 子集

> 题目: 给定一个可能具有重复数字的列表，返回其所有可能的子集。

```
// 调用函数dfs(res, sub, , nums, 0)之前, nums 必须首先排序, 
// sort(nums.begin(), nums.end());
void dfs(vector<vector<int>> &res, vector<int> &sub, vector<int> &nums, int k) {   
        res.push_back(sub);
    for(int i= k; i < nums.size(); i++) {
        // 跳过相同元素, 
        if(i != k && nums[i] == nums[i - 1]) continue; 
        sub.push_back(nums[i]);
        dfs(res, sub, nums, i + 1);
        // 回溯其他可能组合
        sub.pop_back();
    }
}
```

## 3. Word Break Problem

> **题目**: 给一字串s和单词的字典dict,在字串中增加空格来构建一个句子，并且所有单词都来自字典。返回所有有可能的句子。
> **分析**: 利用f[i]记录以i为起点的每个片段的终点j，并且片段要在字典中，然后从0位置开始搜索，每次给当前片段加上空格，然后以当前片段的末尾作为下一次搜索的头部，避免不必要的搜索。

```
vetor<int> f[1000];
vector<string> wordBreak(string &s, unordered_set<string> &wordDict) {
    n = s.length();
    int i, j;
    // 遍历所有可能的(i,j)组合,是否在字典中
    for (i = n - 1; i >= 0; --i) {
        for (j = i + 1; j <= n; ++j) {
            if (wordDict.find(s.substr(i, j - i)) != wordDict.end()) {
                // 大家请思考不加这个条件和加条件有什么区别,
                // if (j == n || f[j].size() > 0) 
                //    f[i].push_back(j);
                f[i].push_back(j);
            }
        }
    }
    dfs(0, s, "");
    return res;
}
void dfs(int p, string s, string &now, vector<int> &res) {
    if(p == s.size()) {
        res.push_back(now);
        return;
    }
    if(p > 0) { // 找到一个单词划分
        now += " ";
    }   
    // 遍历所有以p开头, 以j结尾的划分进行dfs
    for(int i = 0; i < f[p].size(); i ++) {
        dfs(f[p][i], s, now+s.substr(p, f[p][i]-p), res);
    }
}
```

## 4. K-Similar Strings

> **题目**: 如果可以通过将 A 中的两个小写字母精确地交换位置 K 次得到与 B 相等的字符串，我们称字符串 A 和 B 的相似度为 K（K 为非负整数）。
> 给定两个字母异位词 A 和 B ，返回 A 和 B 的相似度 K 的最小值。
> 解析: 这是一个bfs的问题, 每次改变A的一个字符, 和B进行比较,
> 将改变后的A加入到候选队列中,直到所有出现A==B位置,得到此时的次数.

```
struct Node {
    string s;
    int step;
    Node(string _s, int _step):s(_s),step(_step);
    Node(){}
};

int kSimilarity(string &A, string &B) {
    Node start(A, 0);
    queue<Node> q;
    set<string> vis;
    q.push(start);
    int ans = 0;
    while(q.size()) {
        Node str = q.front();
        q.pop();
        if(str.s == B) {
            ans = str.step;
            break;
        }
        int i = 0;
        while(str[i] == B[i]) i ++;
        for(int j = i + 1; j < B.size(); j ++) {
            if(str[j] != B[j] && str[j] == B[i]) {
                string temp = str;
                swap(temp[i], temp[j]);
                if(vis.find(temp) == vis.end()) {
                    q.push(Node(temp, str.step+1));
                    vis.insert(temp);
                }
            }
        }
    }
    return ans;
}
```

## 5. 无向图的联通块

> **题目**: 给一个布尔类型的二维数组, 0 表示海, 1 表示岛。如果两个1是相邻的,那么我们认为他们是同一个岛.我们只考虑 上下左右 相邻. 求出岛屿的个数.
> **解析**: 这就是无向图的联通块问题, 我们遍历所有是1的位置进行dfs(i,j), 并且将所有访问过的位置记录下来,如果当前位置是1,而且没有访问,则次数就加1.

```
void dfs(vector<vector<int>> &grid, int i, int j) {
    if(i < 0 || i >= grid.size()) return;
    if(j < 0 || j >= grid[0].size()) return;
    if(!grid[i][j]) return;
    grid[i][j] = 0;
    // 四个方向搜索
    dfs(grid, i-1, j);
    dfs(grid, i+1, j);
    dfs(grid, i, j-1);
    dfs(grid, i, j+1);
}
int numIslands(vector<vector<int>> &grid) {
    if (grid.empty() || grid[0].empty()) return 0;
    int N = grid.size(), M = grid[0].size();
    int cnt = 0;
    for (int i = 0; i < N; ++i) {
        for (int j = 0; j < M; ++j) {
            if (grid[i][j]) {
                dfs(grid, i, j);
                ++cnt;
            }
        }
    }
    return cnt;
}
```

## 6. k个数的和

> **题目**: 给定n个不同的正整数，整数k（1<= k <= n）以及一个目标数字。
> 在这n个数里面找出K个数，使得这K个数的和等于目标数字，你需要找出所有满足要求的方案。
> **解析**: dfs(i, k, target)
> 每次怕判断是够使用第i个值, 如果使用, dfs(i+1,k-1,target-arr[i])
> 如果不使用, dfs(i+1,k,target),
> if target == 0, 则表示满足要求,存储结果

```
void dfs(vector<int> A, int i, int k, int target, vector<int> &now, vector<vector<int>> &res) {
    if(i > A.size() || target < 0 || k < 0) return;
    if(target == 0 && k==0) {
        res.push_back(now);
        return;
    }
    // user
    now.push_back(A[i]);
    dfs(A, i+1, k-1, target-A[i], now, res);
    now.pop_back();
    // not use i
    dfs(A, i+1, k, target, now, res);

}
vector<vector<int>> kSumII(vector<int> &A, int k, int target) {
    // write your code here
    vector<vector<int>> res;
    vector<int> now;
    dfs(A,0,k,target,now,res);
    return res;
  
}
```

## 7. 单词接龙

> **题目**: 给出两个单词（start和end）和一个字典，找出从start到end的最短转换序列，输出最短序列的长度。
> 变换规则如下：

+ 每次只能改变一个字母。
+ 变换过程中的中间单词必须在字典中出现。(起始单词和结束单词不需要出现在字典中)

> **解析**: 使用bfs进行变换,每一修改一个字符['a'--'z'],判断是否在字典中,并记录当前的步数.

```
int ladderLength(string &start, string &end, unordered_set<string> &dict) {
    int length = 2;  
    if(start == end) return 1;
    queue<string> q;
    q.push(start);
    while(!q.empty()){
        int size = q.size();// 对每一个层以此处理, 这一个的都步数一样
        for(int i=0;i<size;i++){
            string tmp = q.front();
            q.pop();
            // 遍历tmp的所有的字符，进行26个字符的变换
            for(int j=0;j<tmp.size();j++){
                // 要记录老字符，因为最后要恢复
                char oldc = tmp[j];
                for(char c='a';c<='z';c++){
                    if(tmp[j] == c) continue;
                    tmp[j] = c;
                    //验证是否已经满足条件
                    if(tmp == end) return length;
                    // 变换的单词是否在字典中
                    if(dict.find(tmp) != dict.end()){
                        q.push(tmp);
                        dict.erase(tmp); // 防止多次使用
                    }
                }
                // 恢复当前的变化,这个不变，变化下一个,
                tmp[j] = oldc;
            }
        }
        length ++;
    }
    return length;
}
```

## 8. 单词搜索

> **题目**:
> 给出一个二维的字母板和一个单词，寻找字母板网格中是否存在这个单词。单词可以由按顺序的相邻单元的字母组成，其中相邻单元指的是水平或者垂直方向相邻。每个单元中的字母最多只能使用一次。

```
样例
样例 1:

输入：["ABCE","SFCS","ADEE"]，"ABCCED"
输出：true
解释：
[  
     A B C E
     S F C S 
     A D E E
]
(0,0)A->(0,1)B->(0,2)C->(1,2)C->(2,2)E->(2,1)D
样例 2:

输入：["z"]，"z"
输出：true
解释：
[ z ]
(0,0)z
```

```
bool dfs(int i, int j, int k, vector<vector<char>> &board, string word, vector<vector<int>> &vis) {
    if(board[i][j] == word[k]) {
        ++ k;
        if(k == word.size()) {
            return true;
        }
    }
    else return false;
  
    bool flag = false; 
  
    vis[i][j] = 1;
    if(i-1 >=0 && (!vis[i-1][j]) && board[i-1][j] == word[k]) flag = flag | dfs(i-1, j, k, board, word, vis); 
    if(flag) return flag;
    if(i+1 < board.size() && (!vis[i+1][j]) && board[i+1][j] == word[k]) flag = flag | dfs(i+1, j, k, board, word, vis); 
    if(flag) return flag;

    if(j-1 >= 0 && (!vis[i][j-1]) && board[i][j-1] == word[k]) flag = flag | dfs(i, j-1, k, board, word, vis);
    if(flag) return flag;

    if(j+1 <= board[0].size() && (!vis[i][j+1]) && board[i][j+1] == word[k]) flag = flag | dfs(i, j+1, k, board, word, vis);
    // 下次使用标记
    vis[i][j] = 0;
    return flag;
}  
bool exist(vector<vector<char>> &board, string &word) {
    if(board.empty() || board[0].size() == 0) return false;
    bool res = false;
    vector<vector<int>> vis(board.size(), vector<int>(board[0].size(), 0));
  
    for(int i = 0; i < board.size(); i ++) {
        for(int j = 0; j < board[i].size(); j ++) {
            if(word[0] == board[i][j] && dfs(i,j,0,board,word, vis)){
                return true;                  
            }
        }
    }
    return res;   
}
```

## 9. 分割字符串

> **题目**: 给一个字符串,你可以选择在一个字符或两个相邻字符之后拆分字符串,使字符串由仅一个字符或两个字符组成,输出所有可能的结果.
> **解析**： dfs(s) = dfs(s-1) + dfs(s-2)

```
void dfs(int i, string s, vector<string> &now, vector<vector<string>> &res) {
    if(i == s.size()) {
        res.push_back(now);
        return;
    }
    if(s.size() - i == 1) {
        now.push_back(s.substr(i, 1));
        dfs(i+1,s,now,res);
        now.pop_back();
        return;
    }
    if(s.size() - i >= 2) {
        now.push_back(s.substr(i, 1));
        dfs(i+1,s,now,res);  
        now.pop_back();
        now.push_back(s.substr(i, 2));
        dfs(i+2,s,now,res);
        now.pop_back();
    }  
} 
vector<vector<string>> splitString(string& s) {
    vector<string> now; 
    vector<vector<string>> res;
    dfs(0,s,now,res);  
    return res;
}
```

## 10. 划分回文串

> **题目**: 给定一个字符串S，将S切分成每一个子串都是回文串，返回所有可能的结果.

```
Input  : s = "bcc"
Output : [["b", "c", "c"], ["b", "cc"]]
```

```
bool checkPalindrome(string str) { 
    int len = str.length(); 
    len--; 
    for (int i=0; i<len; i++) { 
        if (str[i] != str[len]) return false; 
        len--; 
    } 
    return true; 
} 
void addStrings(vector<vector<string> > &v, string &s, 
                vector<string> &temp, int index) { 
    int len = s.length(); 
    string str; 
    vector<string> current = temp; 
    if (index == 0) temp.clear(); 
    for (int i = index; i < len; ++i) { 
        str = str + s[i]; 
        if (checkPalindrome(str)) { 
            temp.push_back(str); 
            if (i+1 < len) 
                addStrings(v,s,temp,i+1); 
            else
                v.push_back(temp); 
            temp = current; 
        } 
    } 
    return; 
}
```

---

# 文件：数据结构与算法\sort.md

---

# 排序（sort）

> 排序的目的是让一组无序的对象变成有序（升序、降序），排序在面试中很容易被问道。排序之所以这么重要是因为排序是解决大部分问题的第一步，一些看似复杂的问题当数据有序的时候就变的简单，例如查找问题，如果数组有序可以使用搞笑的折半查找。

需要提出，这篇文章并不介绍排序，什么插入、冒泡、希尔等算法，我们都不会介绍，我们的目的是给出最常见的关于排序的面试题目，俗称押题，当然希望每个人都能研究每一个题目，在面试过程中遇到排序问题，都可以解决。

## 1. 快速排序

> 题目: 这是面试中最常见的问题，手写快排，面试官主要是考查候选人的算法基本工。
> 公司: 爱奇艺，某金融公司

```
template<class T>
static bool cmp(const T a, const T b) {
    return a < b;
}

template<class T>
int Poivt(T list[], int start,int end, bool (*cmp)(T, T)=cmp) {
    int t = randint(start, end);
    swap(list[t],list[start]);
    int p,i,j;
    i = start+1;
    j = end;
    p = start;
    while(1) {
        while(i<end && cmp(list[i],list[p])) ++i;
        while(j>start && !cmp(list[j],list[p])) --j;
        if(j<=i) break;
        else{
            swap(list[i],list[j]);
            ++i;
            --j;
        }
    }
    swap(list[j],list[p]);
    return j;
}

// qsort
template<class T>
void QuickSort(T list[], int start,int end, bool (*cmp)(T, T)=cmp) {
    if(start>=end) return;
    int p = Poivt(list,start,end,cmp);
    QuickSort(list,start,p-1,cmp);
    QuickSort(list,p+1,end,cmp);
}
```

## 2. 堆排序

> 题目: 手写堆排序
> 公司: 阿里

```
// 第一步建立最大堆， 下标从1开始 A[1..n]
void BuildMaxHeap(int *A, int n, int &heapsize){
    heapsize=n; //全局变量，表示最大堆的大小
    for(int i = n/2; i > 1; i --){      
        MaxheapFY(A, i);
    }
}

// heapsort
void HeapSort(int *A,int n){
    BuildMaxHeap(A,n);//建立最大堆
    for(int i = n;i >= 1;i --){
        swap(A[0],A[i]);      
        heapsize --;
        MaxheapFY(A, 1);
    }
}

// 维护位置i最大堆的性质
void MaxheapFY(int *A, int i){
    int l,r,now;
    l = i * 2;
    r = i * 2 + 1;
    now = i;
    if(l <= heapsize && A[l] > A[now]) {
        now = l;
    }
    if(r <= heapsize && A[r] > A[now]){
        now = r;
    }
    if(now != i){
        swap(A[i], A[now]);
        MaxheapFY(A, now);  
    }
}
```

## 3. 归并排序

> 题目: 手写归并排序

```
template<class T>
void Merge(T list[], int start, int mid, int end, bool (*cmp)(T, T)=cmp) {
    T *temp = new T[end-start+1];
    int i=start,j=mid+1,k=0;
    while(i<=mid && j<=end) {
        if(cmp(list[i],list[j])) temp[k++] = list[i++];
        else temp[k++] = list[j++];
    }
    while(i<=mid) {
        temp[k++] = list[i++];
    }
    while(j<=end) {
        temp[k++] = list[j++];
    }
    // copy 
    for(i=start;i<=end;i++){
        list[i] = temp[i-start];
    }
    delete [] temp;
}

// MergeSortUtil
template<class T>
void MergeSortUtil(T list[], int start,int end, bool (*cmp)(T, T)=cmp) {
    if(start>=end) return;
    int mid = (start+end) / 2;
    MergeSortUtil(list,start,mid,cmp);
    MergeSortUtil(list,mid+1,end,cmp);
  
    Merge(list,start,mid,end,cmp);
}
```

## 4. 实现多路归并排序

> 题目: 实现常用的多路归并排序(使用最大堆，或者优先队列)
> 公司: 百度，360

```
// vec中每一个vector都是有序的
vector<int> MultMerge(vector<vector<int> > vec, vector<int> &result) {
	int n = vec.size();
	priority_queue<int, vector<int>, greater<int> > q;
	vector<vector<int>::iterator> vec_it;
	for(int i = 0; i < n; i ++) {
		vector<int>::iterator it = vec[i].begin();
		vec_it.push_back(it);
	}
	for(int i = 0; i < n; i ++) {
		if(q.size() < k && vec_it[i] != vec[i].end()) {
			q.push(*(vec_it[i]));
		}
	}
	while(q.size()) {
		int cand = q.top();
		q.pop();
		result.push_back(cand);
		int index = 0;
		for(int i = 0; i < n; i ++) {
			if(vec_it[i] != vec[i].end() && cand == *(vec_it[i])) {
				index = i;
				vec_it[index] ++;
				break;
			}
		}
		if(vec_it[index] != vec[index].end()) {
			q.push(*(vec_it[index]));
		}

	}
	return result;
}
```

## 5. 单链表插入排序

> 题目: 单链表的插入排序（升序）。
> 公司: 百度

```
struct Node {
    int data;
    struct Node * next;
};

void InsertLinked(Node** sorted, Node* tmp) {
    Node* cur;
    // 当前插入节点是最小的值
    if(*sorted == NULL || tmp->data <= (*sorted)->data) {
        tmp->next = *sorted;
        *sorted = tmp;
    }
    else { // 找到插入的位置
        cur = *sorted;
        while(cur->next != NULL && tmp->data > cur->next->data) {
            cur = cur->next;
        }
        tmp->next = cur->next;
        cur->next = tmp;
    }
}

void InsertSort(Node** head) {
    // 有序链表
    Node *sorted = NULL;
    Node * cur = *head;

    while(cur != NULL) {
        Node *next = cur->next;

        // 将cur插入到sorted中，这是一个有序的链表
        InsertLinked(&sorted, cur);

        cur = next;
    }
    *head = sorted;
}
```

## 6. 单链表归并排序

> 题目: 单链表的归并排序。
> 公司: 百度

```
void MergeSort(Node **head_ref) {
    Node *head  = *head_ref;
    Node *left;
    Node *right;

    // 判断是否是null
    if(head == NULL || head->next == NULL) {
        return;
    }

    // 链表分成两个部分，left 和 right
    split(head, &left, &right);
  
    MergeSort(left);
    MergeSort(right);
  
    *head_ref = Merge(left, right);
}

// 左右各一半，
void split(Node *head, Node **left, Node **right) {
    //1. 先计算长度n，分别选择前一半和后一半。
    //2. 使用快慢指针，各取一半
    int n = 0;
    Node *cur = head;
    while(cur != NULL) {
        n ++;
    }
    *left = head;

    int k = n / 2;
    cur = head;
    Node *p = NULL;
    while(k--) {
        p = cur;
        cur = cur->next;
    }
    p->next = NULL;

    *right = cur;
}

Node* Merge(Node *left, Node *right) {
    // merge right to left
    Node *head = NULL;
    head->data = -1;
    Node *p = head;  

    while(left != NULL && right != NULL) {
        if(left->data <= right->data) {
            p->next = left;
            left = left->next;
        }
        else {
            p->next = right;
            right = right->next;
        }
        p = p->next;
    }

    if(left != NULL) {
        p->next = left;
    }
    if(right != NULL) {
        p->next = right;
    }
  
    return head->next;
}
```

---

# 文件：数据结构与算法\string.md

---

# 1.最长公共前缀

编写一个函数来查找字符串数组中的最长公共前缀。

如果不存在公共前缀，返回空字符串 ""。

示例 1:

输入: ["flower","flow","flight"]
输出: "fl"
示例 2:

输入: ["dog","racecar","car"]
输出: ""
解释: 输入不存在公共前缀。
说明:

所有输入只包含小写字母 a-z 。

**思路**

首先，我们将描述一种查找一组字符串的最长公共前缀 $LCP(S_1 \ldots S_n)LCP(S_1 …S_n )$ 的简单方法。 我们将会用到这样的结论：
$LCP(S_1…S_n)=LCP(LCP(LCP(S_1,S_2),S_3 ),…S_n )$
算法

为了运用这种思想，算法要依次遍历字符串 $[S_1 \ldots S_n]$，当遍历到第 i个字符串的时候，找到最长公共前缀 $LCP(S_1,……S_i)$，当$LCP(S_1,……，S_i)$是一个空串的时候，算法就结束了。否则，在执行了n次遍历之后，算法就会返回最终答案的$LCP(S_1……S_n)$
![图片](https://uploader.shimo.im/f/aNBix4PL8rMkuaHL!thumbnail)

```
class Solution {
public:
    string longestCommonPrefix(vector<string>& strs) {
       if(strs.size() ==0)
        return "";
       string temp=strs[0];
       for(int ii=1;ii<strs.size();ii++){
            while(strs[ii].find(temp) != 0){
                temp=temp.substr(0,temp.size()-1);
                if(temp.size() == 0)
                    return "";
            }
       }
       return temp;
    }
};
```

# 2.有效的括号

给定一个只包括 '('，')'，'{'，'}'，'['，']' 的字符串，判断字符串是否有效。

有效字符串需满足：

左括号必须用相同类型的右括号闭合。
左括号必须以正确的顺序闭合。
注意空字符串可被认为是有效字符串。

示例 1:

输入: "()"
输出: true
示例 2:

输入: "()[]{}"
输出: true
示例 3:

输入: "(]"
输出: false
示例 4:

输入: "([)]"
输出: false
示例 5:

输入: "{[]}"
输出: true

```
class Solution {  
public:  
    bool isValid(string s) {  
        stack<char> paren;
        for(char& c : s){
        case '(':
        case '[':
        case '{': paren.push(c);break;
        case ')':
            if(paren.empty() || paren.top()!= '(')
                return false;
            else
                paren.pop();
            break;
        case '}':
            if(paren.empty() || paren.top()!= '{')
                return false;
            else
                paren.pop();
            break;
        case ']':
            if(paren.empty() || paren.top()!= '[')
                return false;
            else
                paren.pop();
        default:
            pass
          
        }
        return paren.empty();
    }  
};
```

# 3.验证回文串

 给定一个字符串，验证它是否是回文串，只考虑字母和数字字符，可以忽略字母的大小写。

说明：本题中，我们将空字符串定义为有效的回文串。

示例 1:

输入: "A man, a plan, a canal: Panama"
输出: true
示例 2:

输入: "race a car"
输出: false

```
class Solution {
public:
    bool is_letter_number(char c){
        if( (c>='a' && c<='z') || (c>='A' && c<='Z') ||(c>='0' && c<='9') )
            return true;
        else
            return false;
    }
    bool isPalindrome(string s) {
        int right=0,left=s.size()-1;
        while(right < left){
            while(right<left && !is_letter_number(s[right]))
                right++;
            while(right<left && !is_letter_number(s[left]))
                left--;
            //cout<<s[right]<<s[left]<<endl;
            if(tolower(s[left]) != tolower(s[right]))
                return false;

            left--;
            right++;
        }
        return true;
    }
};
```

# 4.反转字符串中的单词III

给定一个字符串，你需要反转字符串中每个单词的字符顺序，同时仍保留空格和单词的初始顺序。

示例 1:

输入: "Let's take LeetCode contest"
输出: "s'teL ekat edoCteeL tsetnoc"
注意：在字符串中，每个单词由单个空格分隔，并且字符串中不会有任何额外的空格。

```
class Solution {
public:
    string reverseWords(string s) {
        int start=0,endlv=0;
        string res="";
        for(int ii=0;ii<s.size();ii++){
            if(s[ii] == ' ' ){
                endlv=ii-start;
                res+=str_reverse(s.substr(start,endlv));
                res+=" ";
                start=ii+1;
            }
            if(ii == s.size()-1){
                endlv=ii+1-start;
                res+=str_reverse(s.substr(start,endlv));
            }
        }
        return res;
    }
    string str_reverse(string s){
        int right=0,left=s.size()-1;
        while(right<left){
            char temp=s[right];
            s[right++]=s[left];
            s[left--]=temp;
        }
        return s;
    }
    void rev(string &s, int i, int j) {
        while(i < j) {
            swap(s[i], s[j]);
            i++, j--;
        }
    }
    string reverseWords(string &s) {
        int i = 0;
        for(int j=0;j<s.size();j++){
            if(s[j] == ' ') {
                rev(s, i, j-1);
                i = j + 1;
            }
        }
        rev(s, i, s.size()-1);

        rev(s, 0, s.size()-1);
        return s;
    }
};
```

# 5.无重复字符的最长子串

给定一个字符串，请你找出其中不含有重复字符的 最长子串 的长度。

示例 1:

输入: "abcabcbb"
输出: 3
解释: 因为无重复字符的最长子串是 "abc"，所以其长度为 3。
示例 2:

输入: "bbbbb"
输出: 1
解释: 因为无重复字符的最长子串是 "b"，所以其长度为 1。
示例 3:

输入: "pwwkew"
输出: 3
解释: 因为无重复字符的最长子串是 "wke"，所以其长度为 3。
请注意，你的答案必须是 子串 的长度，"pwke" 是一个子序列，不是子串。

```
class Solution {
public:
    int lengthOfLongestSubstring(string s) {
        int  size,i=0,j,k,max=0;
        size = s.size();
        for(j = 0;j<size;j++){
            for(k = i;k<j;k++)
                if(s[k]==s[j]){
                    i = k+1;
                    break;
                }
            if(j-i+1 > max)
                max = j-i+1;
        }
        return max;
    }
   
    int Judge(int* cnt) {
        for(int i = 0; i < 256; i ++) {
            if(cnt[i] >= 2) return false;
        }
        return true;
    }
    int lengthOfLongestSubstring2(string s) {
        int cnt[256] = {0};
        int start = 0, end = 0, ans = 0;
        while(start <= end){
            if(judge(cnt)) {
                cnt[s[end]] ++;
                end ++;
            }
            else {
                cnt[s[start]]--;
                start ++;
            }
            ans = max(ans, end-start+1);
        }
        return ans;
    }
};
```

# 6. 最长回文子串

给定一个字符串 s，找到 s 中最长的回文子串。你可以假设 s 的最大长度为 1000。

示例 1：

输入: "babad"
输出: "bab"
注意: "aba" 也是一个有效答案。
示例 2：

输入: "cbbd"
输出: "bb"

```
class Solution {
public:
string longestPalindrome(string s) {
    int len = s.size(),max = 0;
    int r_lo = 0;
    for(int i=1;i<len;++i){
        int lo = i,hi =i;
        while(lo>=0 && hi<=len &&s[lo]==s[hi]){//从左到右遍历aba型的回文
            if(max<hi-lo){//记录最大回文串长度并记录该串的秩
                max = hi-lo;
                r_lo = lo;
            }
            lo--;
            hi++;
        }
        lo = i-1;hi = i;
        while(lo>=0 && hi<=len &&s[lo]==s[hi]){//遍历abba型的回文串，其余同上
            if(max<hi-lo){
                max = hi-lo;
                r_lo = lo;
            }
            lo--;
            hi++;
        }
    }
    string s1(s,r_lo,max+1);//构造输出回文串
    return s1;
    }
};
```

# 7.括号生成

给出 n 代表生成括号的对数，请你写出一个函数，使其能够生成所有可能的并且有效的括号组合。

例如，给出 n = 3，生成结果为：

[
"((()))",
"(()())",
"(())()",
"()(())",
"()()()"
]

```
class Solution {
    public List<String> generateParenthesis(int n) {
        List<String> res = new ArrayList<String>();
        generate(res, "", 0, 0, n);
      
        return res;
    }
        //count1统计“(”的个数，count2统计“)”的个数
    public void generate(List<String> res , String ans, int count1, int count2, int n){
      
        if(count1 > n || count2 > n) return;
      
        if(count1 == n && count2 == n)  res.add(ans);
 
      
        if(count1 >= count2){
            String ans1 = new String(ans);
            generate(res, ans+"(", count1+1, count2, n);
            generate(res, ans1+")", count1, count2+1, n);
          
        }
    }

}
```

# 8.压缩字符串

给定一组字符，使用原地算法将其压缩。

压缩后的长度必须始终小于或等于原数组长度。

数组的每个元素应该是长度为1 的字符（不是 int 整数类型）。

在完成原地修改输入数组后，返回数组的新长度。

进阶：
你能否仅使用O(1) 空间解决问题？

示例 1：

输入：
["a","a","b","b","c","c","c"]

输出：
返回6，输入数组的前6个字符应该是：["a","2","b","2","c","3"]

说明：
"aa"被"a2"替代。"bb"被"b2"替代。"ccc"被"c3"替代。
示例 2：

输入：
["a"]

输出：
返回1，输入数组的前1个字符应该是：["a"]

说明：
没有任何字符串被替代。
示例 3：

输入：
["a","b","b","b","b","b","b","b","b","b","b","b","b"]

输出：
返回4，输入数组的前4个字符应该是：["a","b","1","2"]。

说明：
由于字符"a"不重复，所以不会被压缩。"bbbbbbbbbbbb"被“b12”替代。
注意每个数字在数组中都有它自己的位置。
注意：

所有字符都有一个ASCII值在[35, 126]区间内。
1 <= len(chars) <= 1000。

```
class Solution {
public:
    int compress(vector<char>& chars) {
        int i;
        int len=0;
      
       for( i=0;i<chars.size();i++){
           int cnt=1;
            while(i+1<chars.size()&&chars[i]==chars[i+1]){
                cnt++;
                i++;
            }
           chars[len++]=chars[i];
           if(cnt==1)continue;
           for(char ch:to_string(cnt)){
               chars[len++]=ch;
           }
         
        }
        return len;
    }
};
```

# 9.字符串中的第一个唯一字符

给定一个字符串，找到它的第一个不重复的字符，并返回它的索引。如果不存在，则返回 -1。

案例:

s = "leetcode"
返回 0.

s = "loveleetcode",
返回 2.

```
import java.util.HashMap;

class Solution {
    public int firstUniqChar(String s) {
        HashMap<Character, Integer> map = new HashMap<>();
        //第一次遍历哈希表，将数组元素和对应的频次存入哈希表
        for(int i = 0; i < s.length(); i++){
            if(!map.containsKey(s.charAt(i))){
                map.put(s.charAt(i), 1);
            }else{
                map.put(s.charAt(i), map.get(s.charAt(i)) + 1);
            }
        }
        //第二次遍历数组，依次看数组中每一个元素的频次，如果为1则返回
        for(int i = 0; i < s.length(); i++){
            if(map.get(s.charAt(i)) == 1){
                return i;
            }
        }
        return -1;
    }
}

// O(n+256*2)
struct TNode {
	int k;
	int index;
	TNode() {}
	TNode(int _k, int _index):k(_k), index(_index) {}
};

char first_char_4(string s) {
	if(s.size() == 0) return ' ';
	if(s.size() == 1) return s[0];

	TNode cnt[256];
	for(int i = 0; i < 256; i++) {
		cnt[i].k = 0;
		cnt[i].index = -1;
	}

	for(int i = 0; i < s.size(); i++) {
		if(cnt[s[i]].index == -1) {
			cnt[s[i]].index = i;
		}
		cnt[s[i]].k ++;
	}

	int t_index = s.size() + 1;
	int ans = (char) ' ';
	for(int i = 0; i < 256; i++) {
		if(cnt[i].k == 1 && cnt[i].index < t_index) {
			t_index = cnt[i].index;
			ans = i;
		}
	}
	return (char) ans;
}
```

# 10.字符串相乘

给定两个以字符串形式表示的非负整数 num1 和 num2，返回 num1 和 num2 的乘积，它们的乘积也表示为字符串形式。

示例 1:

输入: num1 = "2", num2 = "3"
输出: "6"
示例 2:

输入: num1 = "123", num2 = "456"
输出: "56088"
说明：

num1 和 num2 的长度小于110。
num1 和 num2 只包含数字 0-9。
num1 和 num2 均不以零开头，除非是数字 0 本身。
不能使用任何标准库的大数类型（比如 BigInteger）或直接将输入转换为整数来处理。

```
class Solution {
    public String multiply(String num1, String num2) {
        /**
        num1的第i位(高位从0开始)和num2的第j位相乘的结果在乘积中的位置是[i+j, i+j+1]
        例: 123 * 45,  123的第1位 2 和45的第0位 4 乘积 08 存放在结果的第[1, 2]位中
          index:    0 1 2 3 4  
            
                        1 2 3
                    *     4 5
                    ---------
                          1 5
                        1 0
                      0 5
                    ---------
                      0 6 1 5
                        1 2
                      0 8
                    0 4
                    ---------
                    0 5 5 3 5
        这样我们就可以单独都对每一位进行相乘计算把结果存入相应的index中      
        **/
      
        int n1 = num1.length()-1;
        int n2 = num2.length()-1;
        if(n1 < 0 || n2 < 0) return "";
        int[] mul = new int[n1+n2+2];
      
        for(int i = n1; i >= 0; --i) {
            for(int j = n2; j >= 0; --j) {
                int bitmul = (num1.charAt(i)-'0') * (num2.charAt(j)-'0');    
                bitmul += mul[i+j+1]; // 先加低位判断是否有新的进位
              
                mul[i+j] += bitmul / 10;
                mul[i+j+1] = bitmul % 10;
            }
        }
      
        StringBuilder sb = new StringBuilder();
        int i = 0;
        // 去掉前导0
        while(i < mul.length-1 && mul[i] == 0) 
            i++;
        for(; i < mul.length; ++i)
            sb.append(mul[i]);
        return sb.toString();
    }
}
```

---

# 文件：模拟面试\readme.md

---

---

# 文件：计算机基础\操作系统.md

---

# 操作系统

## 知识体系

## Questions

### 1.进程和线程的区别

- **进程**是系统进行资源分配和调度的基本单位；
- **线程**是CPU调度和分派的基本单位。
  - 每个进程都有独立的代码和数据空间（程序上下文），程序之间的切换会有较大的开销；线程可以看做轻量级的进程，同一类线程共享代码和数据空间，每个线程都有自己独立的运行栈和程序计数器，线程之间切换的开销小；
  - 一个进程至少有一个线程，线程依赖于进程而存在；
  - 每个独立的进程有程序运行的入口、顺序执行序列和程序出口。但是线程不能独立执行，必须依存在应用程序中，由应用程序提供多个线程执行控制，两者均可并发执行；
  - 多线程程序只要有一个线程崩溃，整个程序就崩溃了，但多进程程序中一个进程崩溃并不会对其它进程造成影响，因为进程有自己的独立地址空间，因此多进程更加健壮。

### 2.协程

- **协程**：协程是一种用户态的轻量级线程，协程的调度完全由用户控制。协程拥有自己的寄存器上下文和栈。协程调度切换时，将寄存器上下文和栈保存到其他地方，在切回来的时候，恢复先前保存的寄存器上下文和栈，直接操作栈则基本没有内核切换的开销，可以不加锁的访问全局变量，所以上下文的切换非常快。

### 3.进程的状态

#### 三态模型

* **运行**：当一个进程在处理机上运行时，则称该进程处于运行状态。处于此状态的进程的数目小于等于处理器的数目，对于单处理机系统，处于运行状态的进程只有一个。在没有其他进程可以执行时（如所有进程都在阻塞状态），通常会自动执行系统的空闲进程。
* **就绪**：当一个进程获得了除处理机以外的一切所需资源，一旦得到处理机即可运行，则称此进程处于就绪状态。就绪进程可以按多个优先级来划分队列。例如，当一个进程由于时间片用完而进入就绪状态时，排入低优先级队列；当进程由I／O操作完成而进入就绪状态时，排入高优先级队列。
* **阻塞**：一个进程正在等待某一事件发生（例如请求I/O而等待I/O完成等）而暂时停止运行，这时即使把处理机分配给进程也无法运行，故称该进程处于阻塞状态。

![三态模型](https://i.loli.net/2021/03/25/fjOoY642rznWUGE.png)

#### 五态模型

* **新建**：对应于进程被创建时的状态，尚未进入就绪队列。
* **终止**：进程完成任务到达正常结束点，或出现无法克服的错误而异常终止，或被操作系统及有终止权的进程所终止时所处的状态。

![](https://i.loli.net/2021/03/25/B7Ee5j2XCrObMhw.png)

### 4.进程间通信方式

* **匿名管道**：管道是一种半双工的通信方式，数据只能单向流动，而且只能在具有亲缘关系的进程间使用。进程的亲缘关系通常是指父子进程关系。
* **高级管道**：将另一个程序当做一个新的进程在当前程序进程中启动，则它算是当前程序的子进程。
* **有名管道**：有名管道也是半双工的通信方式，但是它允许无亲缘关系进程间的通信。
* **消息队列**：消息队列是由消息的链表，存放在内核中并由消息队列标识符标识。消息队列克服了信号传递信息少、管道只能承载无格式字节流以及缓冲区大小受限等缺点。
* **信号量**：信号量是一个计数器，可以用来控制多个进程对共享资源的访问。它常作为一种锁机制，防止某进程正在访问共享资源时，其他进程也访问该资源。因此，主要作为进程间以及同一进程内不同线程之间的同步手段。
* **信号**： 信号是一种比较复杂的通信方式，用于通知接收进程某个事件已经发生。
* **共享内存**：共享内存就是映射一段能被其他进程所访问的内存，这段共享内存由一个进程创建，但多个进程都可以访问。共享内存是最快的 IPC 方式，它是针对其他进程间通信方式运行效率低而专门设计的。它往往与其他通信机制，如信号两，配合使用，来实现进程间的同步和通信。
* **套接字**：套接口也是一种进程间通信机制，与其他通信机制不同的是，它可用于不同机器间的进程通信。

### 5.僵尸进程和孤儿进程

* **僵尸进程**：一个进程使用fork创建子进程，如果子进程退出，而父进程并没有调用wait或waitpid获取子进程的状态信息，那么子进程的进程描述符仍然保存在系统中。这种进程称之为僵尸进程。
* **孤儿进程**：一个父进程退出，而它的一个或多个子进程还在运行，那么那些子进程将成为孤儿进程。孤儿进程将被init进程(进程号为1)所收养，并由init进程对它们完成状态收集工作。

### 6.死锁

* **死锁**：死锁是指两个或两个以上的进程在执行过程中，由于竞争资源或者由于彼此通信而造成的一种阻塞的现象，若无外力作用，它们都将无法推进下去。

#### 死锁产生的必要条件

* **互斥条件**：一个资源每次只能被一个进程使用；
* **请求与保持条件**：一个进程因请求资源而阻塞时，对已获得的资源保持不放；
* **不剥夺条件**：进程已获得的资源，在未使用完之前，不能强行剥夺；
* **循环等待条件**：若干进程之间形成一种头尾相接的循环等待资源关系。

#### 死锁预防

* 破坏互斥条件：允许某些资源同时被多个进程访问，但是有些资源本身并不具有这种属性；
* 破坏请求与保持条件：
  * 实行资源预先分配策略（当一个进程开始运行之前，必须一次性向系统申请它所需要的全部资源，否则不运行）；
  * 只允许进程在没有占用资源的时候才能申请资源（申请资源前先释放占有的资源）；
* 破坏不剥夺条件：允许进程强行抢占被其它进程占有的资源，这样做会降低系统性能；
* 破坏循环等待条件：将系统中的所有资源统一编号，进程可在任何时刻提出资源申请，但所有申请必须按照资源的编号顺序（升序）提出。

#### 死锁避免

> 银行家算法
> 
> 参考： [ 银行家算法](https://zh.wikipedia.org/wiki/%E9%93%B6%E8%A1%8C%E5%AE%B6%E7%AE%97%E6%B3%95)

### 7.页面置换算法

* **最佳置换算法**（OPT）：选择以后永不使用的或者是在最长时间内不再被访问的页面；
* **先进先出置换算法**（FIFO）：优先淘汰最早进入内存的页面，亦即在内存中驻留时间最久的页面；
* **最近最久未使用置换算法**（LRU）：置换出未使用时间最长的页面；
* **第二次机会算法**（SCR）：按FIFO选择某一页面，若其访问位为1，给第二次机会，并将访问位置0；
* **时钟算法**（CLOCK）：SCR中需要将页面在链表中移动（第二次机会的时候要将这个页面从链表头移到链表尾），时钟算法使用环形链表，再使用一个指针指向最老的页面，避免了移动页面的开销。
* 注：[LRU算法题](https://leetcode-cn.com/problems/lru-cache/)

### 8.分页和分段的区别

* 段是信息的逻辑单位，它是根据用户的需要划分的，因此段对用户是可见的 ；页是信息的物理单位，是为了管理主存的方便而划分的，对用户是透明的；
* 段的大小不固定，由它所完成的功能决定；页的大小固定，由系统决定；
* 段向用户提供二维地址空间；页向用户提供的是一维地址空间；
* 段是信息的逻辑单位，便于存储保护和信息的共享，页的保护和共享受到限制。

### 9.硬中断和软中断

```

```

**硬中断**是由硬件产生的，比如，像磁盘，网卡，键盘，时钟等。每个设备或设备集都有它自己的IRQ（中断请求）。

```
处理中断的驱动是需要运行在CPU上的，因此，当中断产生的时候，CPU会中断当前正在运行的任务，来处理中断。在有多核心的系统上，一个中断通常只能中断一颗CPU（也有一种特殊的情况，就是在大型主机上是有硬件通道的，它可以在没有主CPU的支持下，可以同时处理多个中断）。
```

```

```

**硬中断**可以直接中断CPU。它会引起内核中相关的代码被触发。对于那些需要花费一些时间去处理的进程，中断代码本身也可以被其他的硬中断中断。

```

```

**软中断**的处理非常像硬中断。然而，它们仅仅是由当前正在运行的进程所产生的。通常，软中断是一些对I/O的请求。这些请求会调用内核中可以调度I/O发生的程序。对于某些设备，I/O请求需要被立即处理，而磁盘I/O请求通常可以排队并且可以稍后处理。根据I/O模型的不同，进程或许会被挂起直到I/O完成，此时内核调度器就会选择另一个进程去运行。I/O可以在进程之间产生并且调度过程通常和磁盘I/O的方式是相同。

```

```

**软中断**仅与**内核**相联系。而内核主要负责对需要运行的任何其他的进程进行调度。一些内核允许设备驱动的一些部分存在于用户空间，并且当需要的时候内核也会调度这个进程去运行。

```

```

**软中断**并不会直接中断CPU。也只有当前正在运行的代码（或进程）才会产生软中断。这种中断是一种需要内核为正在运行的进程去做一些事情（通常为I/O）的请求。有一个特殊的软中断是Yield调用，它的作用是请求内核调度器去查看是否有一些其他的进程可以运行。

### 10.IO模型

* **阻塞式 I/O**：应用进程被阻塞，直到数据从内核缓冲区复制到应用进程缓冲区中才返回；
* **非阻塞式 I/O**：应用进程可以继续执行，但是需要不断地执行系统调用来获知 I/O 是否完成，这种方式称为轮询；
* **I/O 复用**：单个进程具有处理多个 I/O 事件的能力；
  
  * **select**：将文件描述符放入一个集合中，调用select时，将这个集合从用户空间拷贝到内核空间（缺点1：每次都要复制，**开销大**），由内核根据就绪状态修改该集合的内容。（缺点2）**集合大小有限制**，32位机默认是1024（64位：2048）；采用水平触发机制。select函数返回后，需要通过遍历这个集合，找到就绪的文件描述符（缺点3：**轮询的方式效率较低**），当文件描述符的数量增加时，效率会线性下降；
    
    默认单个进程打开的FD有限制是1024个，可修改宏定义，但是效率仍然慢。
  * **poll**：基本原理与select一致，也是轮询+遍历；唯一的区别就是**poll**采用链表的方式存储，没有最大文件描述符限制。
  * **epoll**：通过内核和用户空间共享内存，避免了不断复制的问题；支持的同时连接数上限很高（1G左右的内存支持10W左右的连接数）；文件描述符就绪时，采用回调机制，避免了轮询（回调函数将就绪的描述符添加到一个链表中，执行epoll_wait时，返回这个链表）；支持水平触发和边缘触发，采用边缘触发机制时，只有活跃的描述符才会触发回调函数。
* **信号驱动式 I/O**：内核在数据到达时向应用进程发送 SIGIO 信号；
* 异步 I/O：内核完成所有操作后向应用进程发送信号。

## 参考链接

* https://github.com/wolverinn/Waking-Up#2-%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F
* http://www.linuxidc.com/Linux/2014-03/98013.htm
* https://blog.csdn.net/violet_echo_0908/article/details/51201278
* https://www.cnblogs.com/wlwl/p/10293057.html

---

# 文件：计算机基础\数据库.md

---

# 数据库

## 1. 事务四大特性

1. 原子性，要么执行，要么不执行
2. 隔离性，所有操作全部执行完以前其它会话不能看到过程
3. 一致性，事务前后，数据总额一致
4. 持久性，一旦事务提交，对数据的改变就是永久的

## 2. 数据库模型编辑

1. 对象模型
2. 层次模型（轻量级数据访问协议）
3. 网状模型（大型数据储存）
4. 关系模型
5. 面向对象模型
6. 半结构化模型
7. 平面模型（表格模型，一般在形式上是一个二维数组。如表格模型数据Excel)

## 3. 数据库三范式

第一范式：列不可再分
第二范式：行可以唯一区分，主键约束
第三范式：表的非主属性不能依赖与其他表的非主属性 外键约束 且三大范式是一级一级依赖的，第二范式建立在第一范式上，第三范式建立第一第二范式上

## 4. 关系型数据库和非关系型数据库

+ **关系数据库**，是建立在关系模型基础上的数据库，借助于集合代数等数学概念和方法来处理数据库中的数据。数据库事务必须具备ACID特性，ACID分别是Atomic原子性，Consistency一致性，Isolation隔离性，Durability持久性。代表数据库：Oracle、Mysql、DB2等。
+ **关系型数据库的优点**

1. 容易理解：二维表结构表示逻辑世界的相对的概念，直观明了
2. 使用方便：使用SQl语句查询想要的一切
3. 易于维护：遵循数据库的设计原则，降低数据的冗余性

+ **非关系型数据库**，又被称为NoSQL（Not Only SQL)，意为不仅仅是SQL，主要是指非关系型、分布式、不提供ACID (数据库事务处理的四个基本要素)的数据库设计模式。对NoSQL 最普遍的定义是“非关联型的”，强调Key-Value 存储和文档数据库的优点。代表数据库：MongoDB、Redis等
+ **非关系型数据库的有点**

1. 将所有数据看做key-value对，根据key值得到想要的一切数据
2. 适用于SNS(Social Networking Services)中，例如facebook，微博。系统的升级，功能的增加，往往意味着数据结构巨大变动，key值不变，在value中加入需要的字段，一般value的格式是json或者文本等。

**待续...**

# 参考

1. https://blog.csdn.net/qq_22222499/article/details/79060495#8BB_54
2. https://baike.baidu.com/item/%E6%95%B0%E6%8D%AE%E5%BA%93/103728?fr=aladdin
3. https://www.2cto.com/database/201710/688377.html

---

# 文件：计算机基础\计算机网络.md

---

# 计算机网络

## 知识体系

![](https://i.loli.net/2021/03/10/r4kCcHFAiOoDmd3.gif)

## Questions

### 1.计算机网络分层的优点和缺点

* 优点
  * 各层之间是独立的；
  * 灵活性好；
  * 结构上可分割开；
  * 易于实现和维护；
  * 能促进标准化工作。
* 缺点：
  * 降低效率；
  * 有些功能会在不同的层次中重复出现，因而产生了额外开销。

### 2.计算机体系结构

![](https://i.loli.net/2021/03/12/hpfczavIN871dL6.png)

### 3.从输入网址到获得页面的过程

1. 浏览器解析URL，查询 DNS，检查域名是否在缓存中（浏览器自身的DNS缓存、操作系统的DNS缓存、本地的hosts文件和向本地DNS服务器进行查询）。如果 DNS 服务器和主机在同一个子网内，系统会按照 ARP 过程对 DNS 服务器进行 ARP查询，如果在不同的子网，系统会按照 ARP 过程对默认网关进行查询；
2. 浏览器获得域名对应的IP地址以后，向服务器请求建立链接，发起三次握手；
3. TCP/IP链接建立起来后，浏览器向服务器发送HTTP请求；
4. 服务器接收到这个请求，并根据路径参数映射到特定的请求处理器进行处理，并将处理结果及相应的视图返回给浏览器；
5. 浏览器对HTML、CSS、JS进行解析，构建DOM树，渲染视图；
6. 浏览器根据其请求到的资源、数据渲染页面，最终向用户呈现一个完整的页面。

[超详细版本1](https://github.com/skyline75489/what-happens-when-zh_CN)

[超详细版本2](https://segmentfault.com/a/1190000006879700)

### 4.三次握手

* TCP 建立连接的过程叫做**握手**。
* 握手需要在客户和服务器之间交换三个TCP报文段，称之为三报文握手**;**
* 采用三报文握手主要是为了防止已失效的连接请求报文段突然又传送到了，因而产生错误。

#### 为什么需要三次握手

> ```
> TCP 建立连接时通过三次握手可以有效地避免历史错误连接的建立，减少通信双方不必要的资源消耗，三次握手能够帮助通信双方获取初始化序列号，它们能够保证数据包传输的不重不丢，还能保证它们的传输顺序，不会因为网络传输的问题发生混乱。
> ```
> 
> * 两次握手：无法避免历史错误连接的初始化，浪费接收方的资源；
> * 四次握手：TCP 协议的设计可以让我们同时传递 `ACK` 和 `SYN` 两个控制信息，减少了通信次数，所以不需要使用更多的通信次数传输相同的信息。

![image-20210226164259010.png](https://i.loli.net/2021/03/10/ZGXRKOaj7Wxzs2p.png)

![](https://i.loli.net/2021/03/10/Woin6Xqkbj259Pf.png)

* 第一次握手

![](https://i.loli.net/2021/03/10/Ln1E65xVDuwcUHS.png)

* 第二次握手

![](https://i.loli.net/2021/03/10/8xhdkOwJaWEBRUu.png)

* 第三次握手

![](https://i.loli.net/2021/03/10/a7jyZsrhOKPiVL3.png)

![](https://i.loli.net/2021/03/10/s62ZLCtQXVgSPHk.png)

![](https://i.loli.net/2021/03/10/F3iblcJvD8YTPQI.png)

### 5.四次挥手

#### 为什么需要四次挥手

> ```
> 因为当服务端收到客户端的SYN连接请求报文后，可以直接发送SYN+ACK报文。其中ACK报文是用来应答的，SYN报文是用来同步的。但是关闭连接时，当服务端收到FIN报文时，很可能并不会立即关闭SOCKET，所以只能先回复一个ACK报文，告诉客户端，"你发的FIN报文我收到了"。只有等到我服务端所有的报文都发送完了，我才能发送FIN报文，因此不能一起发送。故需要四次挥手。
> ```

* 第一次挥手

![](https://i.loli.net/2021/03/10/dM1rBt5fqblzHcL.png)

* 第二次挥手

![](https://i.loli.net/2021/03/10/WMlSqjukUtpbZzr.png)

* 第三次挥手

![](https://i.loli.net/2021/03/10/nr6Vv3AeJHRNIGX.png)

* 第四次挥手

![](https://i.loli.net/2021/03/10/m7ht9eKsI4ZzWQT.png)

![](https://i.loli.net/2021/03/10/hDAObXlavYiRPC8.png)

![](https://i.loli.net/2021/03/10/62ceNEwKJf1TsGV.png)

### 6.TCP和UDP的区别

**TCP**：

* 面向连接的协议，提供面向连接服务；
* 面向报文，其传送的运输协议数据单元TPDU是 TCP报文；
* 支持点对点单播，不支持多播、广播；
* 可靠传输，使用流量控制和拥塞控制；
* 复杂，用于大多数应用。如：万维网、电子邮件、文件传输等。

**UDP**：

* 无连接的协议，提供无连接服务；
* 面向字节流，其传送的运输协议数据单元TPDU是 UDP报文或用户数据报；
* 支持单播、多播、广播；
* 不可靠传输，不使用流量控制和拥塞控制；
* 简单，适用于很多应用。如：多媒体应用等。

### 7.TCP如何保证可靠传输

* 应用数据被分割成 TCP 认为最适合发送的数据块；
* TCP 给发送的每一个包进行编号，接收方对数据包进行排序，把有序数据传送给应用层；
* **校验和：**TCP 将保持它首部和数据的检验和。这是一个端到端的检验和，目的是检测数据在传输过程中的任何变化。如果收到段的检验和有差错，TCP 将丢弃这个报文段和不确认收到此报文段；
* TCP 的接收端会丢弃重复的数据；
* **流量控制：**TCP 连接的每一方都有固定大小的缓冲空间，TCP的接收端只允许发送端发送接收端缓冲区能接纳的数据。当接收方来不及处理发送方的数据，能提示发送方降低发送的速率，防止包丢失。TCP 使用的流量控制协议是可变大小的滑动窗口协议；（TCP 利用滑动窗口实现流量控制）
* **拥塞控制：**当网络拥塞时，减少数据的发送；
* **ARQ协议：**也是为了实现可靠传输的，它的基本原理就是每发完一个分组就停止发送，等待对方确认。在收到确认后再发下一个分组；
* **超时重传：**当 TCP 发出一个段后，它启动一个定时器，等待目的端确认收到这个报文段。如果不能及时收到一个确认，将重发这个报文段。

### 8.TCP流量控制

```
TCP使用滑动窗口协议（连续ARQ协议）实现流量控制。滑动窗口协议既保证了分组无差错、有序接收，也实现了流量控制。主要的方式就是接收方返回的 ACK 中会包含自己的接收窗口的大小，并且利用大小来控制发送方的数据发送。发送窗口的大小不能超过接受窗口的大小，只有当发送方发送并收到确认之后，才能将发送窗口右移。发送窗口的上限为接受窗口和拥塞窗口中的较小值。接受窗口表明了接收方的接收能力，拥塞窗口表明了网络的传送能力。
```

### 9.TCP拥塞控制

* 慢开始
  
  * 刚开始发送数据时，先把拥塞窗口cwnd（congestion window）设置为一个最大报文段MSS的数值，每收到一个新的确认报文之后，可以把拥塞窗口增加最多一个 SMSS 的数值。这样每经过一个传输轮次（或者说是每经过一个往返时间RTT），拥塞窗口的大小就会加倍。
    
    ![](https://i.loli.net/2021/03/12/lpe2o7CVsbJaNj1.png)
* 拥塞避免
  
  * 当拥塞窗口的大小达到慢开始门限(slow start threshold)时，开始执行拥塞避免算法，让拥塞窗口cwnd缓慢地增大，避免出现拥塞，每经过一个传输轮次，拥塞窗口加一，使其按线性规律缓慢增大。
  
  ![](https://i.loli.net/2021/03/12/HgaMwBY7y6vskiZ.png)
* 快重传
  
  * 发送方只要一连收到三个重复确认，就知道接收方确实没有收到报文段，因而应当立即进行重传（即快重传），这样就不会出现超时，发送方也不就会误认为出现了网络拥塞。使用快重传可以使整个网络的吞吐量提高约20%。
  
  ![](https://i.loli.net/2021/03/12/iOW3JSVly2BaEQt.png)
* 快恢复
  
  * 当发送端收到连续三个重复的确认时，由于发送方现在认为网络很可能没有发生拥塞，因此现在不执行慢开始算法，而是执行快恢复算法FR (Fast Recovery)算法：
    
    1.慢开始门限ssthresh=当前拥塞窗口cwnd / 2 ；
    
    2.新拥塞窗口cwnd=慢开始门限ssthresh；
    
    3.开始执行拥塞避免算法，使拥塞窗口缓慢地线性增大。

### 10.Session与Cookie

**Cookie**：

```
Cookie是服务器发送到用户浏览器并保存在本地的一小块数据，它会在浏览器下次向同一服务器再发起请求时被携带并发送到服务器上。通常，它用于告知服务端两个请求是否来自同一浏览器，如保持用户的登录状态。Cookie 使基于无状态的 HTTP 协议记录稳定的状态信息成为了可能。
```

**Session**：

```
Session 代表着服务器和客户端一次会话的过程。Session 对象存储特定用户会话所需的属性及配置信息。这样，当用户在应用程序的 Web 页之间跳转时，存储在 Session 对象中的变量将不会丢失，而是在整个用户会话中一直存在下去。当客户端关闭会话，或者 Session 超时失效时会话结束。
```

**区别**：

* 作用范围不同，Cookie 保存在客户端（浏览器），Session 保存在服务器端；
* 存取方式不同，Cookie 只能保存 ASCII，Session 可以存任意数据类型；
* 有效期不同，Cookie 可设置为长时间保持，Session 一般失效时间较短，客户端关闭或者 Session 超时都会失效；
* 隐私策略不同，Cookie 存储在客户端，比较容易遭到不法获取，Session 存储在服务端，安全性相对 Cookie 要好一些；
* 存储大小不同， 单个 Cookie 保存的数据不能超过 4K，Session 可存储数据远高于 Cookie。

### 11.HTTP状态码

* 2xx状态码：操作成功。200 OK；
* 3xx状态码：重定向。301 永久重定向；302暂时重定向；
* 4xx状态码：客户端错误。400 Bad Request；401 Unauthorized；403 Forbidden；404 Not Found；
* 5xx状态码：服务端错误。500服务器内部错误；501服务不可用；

### 12.HTTP报文

HTTP报文有请求报文和响应报文两种，请求报文：从客户向服务器发送请求报文；响应报文：从服务端到客户的回答。HTTP报文都由**开始行、首部行、实体主体**三部分组成。

* **开始行**
  * 请求报文
    * 由 **方法**、**[空格]**、**URL**、**[空格]**、**HTTP版本** 组成。
  * 响应报文
    * 由 **HTTP 版本**、**[空格]**、**状态码**组成。
* **首部行**
  * 是用来说明浏览器、服务器或报文主体的一些信息。
  * 可以有好几行，也可以不使用
  * 每个首部行都是由 **首部字段名**、**[空格]** 和 **值** 组成
  * 每个首部行在结束地方都有 `CRLF`（『回车』和『换行』符）
* **实体主体**
  * 在请求报文中，一般是 post/put 提交的表单信息。与首部行之间有 `CRLF` 即空行。

### 13.HTTP与HTTPS的区别

* HTTP
  * 互联网上应用最为广泛的一种网络协议，是一个客户端和服务器端请求和应答的标准（TCP），用于从WWW服务器传输超文本到本地浏览器的传输协议，它可以使浏览器更加高效，使网络传输减少。
* HTTPS
  * 以安全为目标的HTTP通道，简单讲是HTTP的安全版，即HTTP下加入SSL层，HTTPS的安全基础是SSL，因此加密的详细内容就需要SSL。
  * 工作原理：
    * 客户使用HTTPS的URL访问Web服务器，要求与Web服务器建立SSL连接。
    * Web服务器收到客户端请求后，会将网站的证书信息（证书中包含公钥）传送一份给客户端。
    * 客户端的浏览器与Web服务器开始协商SSL连接的安全等级，也就是信息加密的等级。
    * 客户端的浏览器根据双方同意的安全等级，建立会话密钥，然后利用网站的公钥将会话密钥加密，并传送给网站。
    * Web服务器利用自己的私钥解密出会话密钥。
    * Web服务器利用会话密钥加密与客户端之间的通信。

![](https://i.loli.net/2021/03/12/iDyBOY49orGjt8k.gif)

* 区别：
  * HTTPS协议需要到ca申请证书，一般免费证书较少，因而需要一定费用；
  * HTTP是超文本传输协议，信息是明文传输，HTTPS则是具有安全性的SSL加密传输协议；
  * HTTP和HTTPS使用的是完全不同的连接方式，用的端口也不一样，前者是80，后者是443；
  * HTTP的连接很简单，是无状态的；HTTPS协议是由SSL+HTTP协议构建的可进行加密传输、身份认证的网络协议，比HTTP协议安全。

## 面试真题

### 1.DNS的具体过程

1. 检查浏览器缓存中是否缓存过该域名对应的IP地址；
2. 如果在浏览器缓存中没有找到IP，那么将继续查找本机系统是否缓存过IP；
3. 向本地域名解析服务系统发起域名解析的请求；
4. 向根域名解析服务器发起域名解析请求；
5. 根域名服务器返回gTLD（通用顶级域名）域名解析服务器地址；
6. 向gTLD服务器发起解析请求；
7. gTLD服务器接收请求并返回Name Server服务器；
8. Name Server服务器返回IP地址给本地服务器；
9. 本地域名服务器缓存解析结果；
10. 返回解析结果给用户。

### 2.ARP协议的工作原理和流程，路由器是如何转发的？(路由表的工作原理)

**工作流程**

* 当主机A欲向本局域网上的某个主机 B 发送 IP 数据报时，就先在其 ARP高速缓存中查看有无主机B的IP地址；
  * 如有，就可查出其对应的硬件地址，再将此硬件地址写入MAC帧，然后通过局域网将该MAC帧发往此硬件地址；
  * 如没有，ARP进程在本局域网上广播发送一个ARP请求分组。收到ARP响应分组后，将得到的IP地址到硬件地址的映射写入ARP高速缓存。

**路由器工作原理**

1. 一个帧到达路由器后，首先检查帧中目标MAC地址是否是本接口的MAC，如不是则丢弃，如是则解封装并将IP包移动到路由器内部；
2. 将IP包中目标IP与路由表进行匹配，如匹配路由表不成功，则丢弃，返回ICMP（互联网控制消息协议）错误消息；若成功，则将数据路由到相应的出口，再封装帧头帧尾；
3. 检查ARP缓存中是否有下一跳的MAC，如有则重新封装出去；如没有，则发送ARP请求广播报文获取下一跳的MAC，并记录到ARP缓存表中，再重新封装数据帧发送出去。

### 3.IPv4和IPv6的区别

* IPv4协议具有32位（4字节）地址长度；IPv6协议具有128位（16字节）地址长度；
* IPv4地址是以小数表示的二进制数。 IPv6地址是以十六进制表示的二进制数；
* IPv4协议的数据包需要576个字节，碎片可选 ；IPv6协议的数据包需要1280个字节，不会碎片；
* IPv4根据提供的 IP 选项，有 20-60 个字节的可变长度；IPv6有40 个字节的固定长度。没有 IP 报头选项。通常，IPv6 报头比 IPv4 报头简单。

### 4.对称加密和非对称加密在HTTPS的应用

**对称加密算法**：发送方和接收方需要持有同一把密钥，发送消息和接收消息均使用该密钥。相对于非对称加密，对称加密具有更高的加解密速度，但双方都需要事先知道密钥，密钥在传输过程中可能会被窃取，因此安全性没有非对称加密高。

**非对称加密算法**：接收方在发送消息前需要事先生成公钥和私钥，然后将公钥发送给发送方。发送放收到公钥后，将待发送数据用公钥加密，发送给接收方。接收到收到数据后，用私钥解密。在这个过程中，公钥负责加密，私钥负责解密，数据在传输过程中即使被截获，攻击者由于没有私钥，因此也无法破解。非对称加密算法的加解密速度低于对称加密算法，但是安全性更高。

HTTPS中同时使用了**对称加密算法**和**非对称加密算法**。对于一个完整的HTTPS请求过程：

1.首先，浏览器请求一个URL，找到服务器，向服务器发起一个请求。服务器将自己的证书(包含服务器公钥S_PuKey)、对称加密算法种类及其他相关信息返回客户端；

2.浏览器检查CA证书是不是由可以信赖的CA机构颁发的，确认证书有效和此证书是此网站的。如果不是，给客户端发一个警告，询问是否继续访问；

3.如果是，客户端使用公钥加密了一个随机对称密钥，包括加密的URL一起发送到服务器；

4.服务器用自己的私匙解密你发送的钥匙，然后用这把对称加密的钥匙给你请求的URL链接解密；

5.服务器用你发的对称钥匙给你请求的网页加密，你也有相同的钥匙就可以解密发回来的网页了。

![](https://i.loli.net/2021/05/06/oKemVBJXkITOYGE.png)

## 参考链接

* http://yx.51zhy.cn/net.jsp
* https://github.com/wolverinn/Waking-Up#1-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C
* https://draveness.me/whys-the-design-tcp-three-way-handshake/
* https://segmentfault.com/a/1190000020610336
* https://www.cnblogs.com/ityouknow/p/10856177.htmls
* https://www.jianshu.com/p/a2c4ede32d11
* https://zhuanlan.zhihu.com/p/34732244
* https://www.cnblogs.com/panpanwelcome/p/12583965.html
* [双非渣本后端三月逆袭字节](https://www.nowcoder.com/discuss/611627?type=2&channel=1009&source_id=discuss_center_discuss_hot_nctrack)
* [腾讯csig后台开发实习一面](https://www.nowcoder.com/discuss/611429?type=2&channel=1009&source_id=discuss_center_discuss_hot_nctrack)
* [阿里Java实习生一面](https://www.nowcoder.com/discuss/627184?type=post&order=time&pos=&page=0&channel=-1&source_id=search_post_nctrack)
* [字节跳动教育prek客户端日常实习-一二三面面经](https://www.nowcoder.com/discuss/653809?type=2&channel=1009&source_id=discuss_center_discuss_hot_nctrack)

